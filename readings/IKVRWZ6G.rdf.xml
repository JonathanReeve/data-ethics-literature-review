<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:res="http://purl.org/vocab/resourcelist/schema#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:bibo="http://purl.org/ontology/bibo/"
 xmlns:foaf="http://xmlns.com/foaf/0.1/">
    <z:UserItem rdf:about="IKVRWZ6G">
        <res:resource rdf:resource="https://www.pnas.org/content/113/48/13538"/>
        <z:repository>www.pnas.org</z:repository>
        <z:accessDate>2021-05-09 03:44:47</z:accessDate>
    </z:UserItem>
    <bibo:AcademicArticle rdf:about="https://www.pnas.org/content/113/48/13538">
        <z:extra>PMID: 27911762</z:extra>
        <dcterms:title>Opinion: The dangers of faulty, biased, or malicious algorithms requires independent oversight</dcterms:title>
        <dcterms:abstract>The recent crash of a driverless car sends a clear warning about how algorithms can be deadly (1). Similarly, there are clear dangers in vital national services, such as communications, financial trading, healthcare, and transportation. These services depend on sophisticated algorithms, some relying on unpredictable artificial intelligence techniques, such as deep learning, that are increasingly embedded in complex software systems (2⇓–4). As search algorithms, high-speed trading, medical devices, and autonomous aircraft become more widely implemented, stronger checks become necessary to prevent failures (5, 6).



Proper independent oversight and investigation of flawed algorithms can help anticipate and improve quality, hence avoiding failures that lead to disaster. Image courtesy of Shutterstock/joloei.



What might help are traditional forms of independent oversight that use knowledgeable people who have powerful tools to anticipate, monitor, and retrospectively review operations of vital national services. The three forms of independent oversight that have been used in the past by industry and governments—planning oversight, continuous monitoring by knowledgeable review boards using advanced software, and a retrospective analysis of disasters—provide guidance for responsible technology leaders and concerned policy makers (7). Considering all three forms of oversight could lead to policies that prevent inadequate designs, biased outcomes, or criminal actions.

There is a long history of analyses of how poor design, unintentional bias, and malicious interventions can cause algorithms to trigger huge financial losses, promote unfair decisions, violate laws, and even cause deaths (8). Helen Nissenbaum, a scholar who focuses on the impact of technology on culture and society, identified the sources of bugs and biases in software, complaining about the “systematic erosion … 

[↵][1]1Email: ben{at}cs.umd.edu.

 [1]: #xref-corresp-1-1</dcterms:abstract>
        <dcterms:language>en</dcterms:language>
        <bibo:uri>https://www.pnas.org/content/113/48/13538</bibo:uri>
        <bibo:pages>13538-13540</bibo:pages>
        <bibo:doi>10.1073/pnas.1618211113</bibo:doi>
        <bibo:shortTitle>Opinion</bibo:shortTitle>
        <dcterms:creator rdf:nodeID="n8378"/>
        <bibo:authorList>
           <rdf:Seq><rdf:li rdf:nodeID="n8378"/></rdf:Seq>
        </bibo:authorList>
        <dcterms:isPartOf>
            <bibo:Issue>
                <dcterms:date>2016/11/29</dcterms:date>
                <bibo:volume>113</bibo:volume>
                <bibo:issue>48</bibo:issue>
                <dcterms:isPartOf>
                    <bibo:Journal>
                        <dcterms:title>Proceedings of the National Academy of Sciences</dcterms:title>
                        <bibo:shortTitle>PNAS</bibo:shortTitle>
                        <bibo:issn>0027-8424, 1091-6490</bibo:issn>
                    </bibo:Journal>
                </dcterms:isPartOf>
            </bibo:Issue>
        </dcterms:isPartOf>
    </bibo:AcademicArticle>
    <foaf:Person rdf:nodeID="n8378">
        <foaf:givenName>Ben</foaf:givenName>
        <foaf:surname>Shneiderman</foaf:surname>
    </foaf:Person>
</rdf:RDF>
