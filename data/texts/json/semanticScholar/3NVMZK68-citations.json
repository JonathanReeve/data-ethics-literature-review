{"offset": 0, "next": 100, "data": [{"contexts": ["This model has roots in the early days of computing and it is commonly referred as the Von Neumann architecture (Goldstine 1993)."], "isInfluential": false, "intents": ["methodology"], "citingPaper": {"paperId": "0fd3c17f655b2e6a6833e60a7564ce86819151d9", "externalIds": {"MAG": "3161099457", "DOI": "10.53136/979125994120639"}, "url": "https://www.semanticscholar.org/paper/0fd3c17f655b2e6a6833e60a7564ce86819151d9", "title": "Prototyping Spatial Interactions", "abstract": "A growing field in the interaction design practice is the domain of connected and interactive spaces. The idea of ubiquitous computing, that was originated in the late 1980s and the beginning of the 1990s, is now seeking maturity, and technology trajectories have helped materialize the basic prerequisites for the proliferation of spatial computing. This paper illustrates how the design process for interactive environments can benefit from an emerging set of methodologies and tools, that is the physical counterpart for the successful creation of human-centered interactive experiences in space. When creating interactive spaces, a designer needs to take into consideration new dimensions, like the flow of users within the space, the spatial context, and a new set of sensors and actuators that go well beyond screens, keyboards, mice, or controllers. Unlike other interaction design domains, there is not a defined device, like a smartphone or personal computer, but instead, the technologies of spatial computing are a collection of distributed devices, sensors, and actuators. Due to this configuration the implementation and prototyping of interactive environments requires new tools to deal with the complexity of non-standard components to ease the design process and allow for sketching in hardware and software at scale. With ad hoc software and hardware designed with the purpose of standardizing and making accessible these components, a design practitioner can implement prototypes of interactive and connected spaces to gather inspirations, insights, and validation before investing in creating the complete experience. This paper explains how the process of designing with such tools will transform the work and the outcome of the interaction designer that explores the forefront domains of connected environments, ubiquitous media, and spatial computing. Pierluigi Dalla Rosa http://www.pierdr.com/ 90 | Prototyping Spatial Interactions doi:10.53136/979125994120639 DigitCult | Scientific Journal on Digital Cultures Introduction Everyone that got in contact with digital technologies experienced the fruits of interaction design. From remote villages to tier-one cities, from smartphones to elevators, digital technologies spread into everyday objects. Personal digital devices, first computers than smartphones, became important and indispensable companions to contemporary life, but are just one instantiation of the digital. Nicholas Negroponte predicted the spread of computation outside computers already in 1998 when he stated that \"computers, as we know them today, will be boring, and disappear into things that are first and foremost something else: smart nails, self-cleaning shirts, driverless cars, therapeutic Barbie dolls, intelligent doorknobs [...] Computers will be a sweeping yet invisible part of our everyday lives: We\u2019ll live in them, wear them, even eat them\". This trajectory is materializing with rising of embedded computing and connected objects. For example, large consumer electronic manufacturers, today, have in their product lines home assistants, like the Apple Home Pod or Google Home. These are new media touch-points, that blend the digital world with new forms of interactions in spaces. The fact that there are numerous players in the field of connected appliances is an indicator of the maturity of a set of technologies that are enabling computation to spread outside traditional devices like computers and smartphones and become part of objects, like the iRobot Roomba, an autonomous vacuum cleaner, or Philips Hue, a smart lighting system where light bulbs with a micro-controller and wireless connectivity can be programmed from anywhere. The spread of low-cost networked micro-controllers allows for new possibilities not just in single connected objects but in creating spaces that are characterized by distributed touchpoints, enabling multi-user interaction in physical space. There are many examples of these if we take into consideration interactive museum exhibitions, interactive experiential marketing campaigns, escape rooms, or alternate reality games. More and more of these experiences pioneered in public space are now permeating people's homes as described in the article of December 2020 in the Scientific American where spatial computing is quoted as one of the top emerging technologies and is quoed to be at the heart of the ongoing convergence of the physical and digital worlds (Lathan 2020). Computing Systems and Spatial Interactions Every computing system, in the most essential form, has three components: input, processing, output. The Analytical Engine by Charles Babbage, designed in 1837, already manifested these components; in the description of Babbage's computer the input, consisting of programs and data, was to be provided to the machine via punched cards [...]. For output, the machine would have a printer, a curve plotter, and a bell (Collier 1970). Everyone that uses a computer today is using a system that conforms to this model, having inputs like the keyboard and mouse, and outputs like a monitor or a printer, while the processing happens inside the computing unit. This model has roots in the early days of computing and it is commonly referred as the Von Neumann architecture (Goldstine 1993). It is possible to apply the label spatial computing to those instances of digital systems that don\u2019t have a predefined set of inputs and outputs and break the paradigm of personal digital devices. Aiming to define a growing domain that has divergent characteristics in terms of the user interface but shared digital architecture compared to current digital systems. The wider consequences of the reach, memorability, and engagement of spatial computing systems are a long way off the scope of this paper, so within it, this classification will be used mainly for technical utility. When designing spatial computing systems without a predefined set of inputs and outputs the focus is on understanding the affordances, mental model, and general usability of different touch-points that populate new devices or interactive spaces; the challenge is also an opportunity to redefine new forms of inputs and outputs that suit the problem, the destination and the purpose of the new digital structure. doi:10.53136/979125994120639 Pierluigi Dalla Rosa | 91 DigitCult | Scientific Journal on Digital Cultures A Prototyping Tool for Interactive Spaces Interaction designers sketch with prototyping tools to get inspiration and test their ideas, before delegating engineering teams the full implementing of a final solution. When designing interactive spaces, the ideal process follows a similar path, with simulation and prototyping ahead of implementation, but the different nature of interactive spaces, compared with digital products, make it more time consuming and with a higher degree of complexity. Prototyping tools can help the designer to simulate the experience, or parts of it, so that some test-users can be immersed in the simulated experience and provide early feedback and inspiration to the design team. The first pioneers to create experimental tools for interactive spaces have been John Underkoffler and Hiroshi Ishii, that started expanding the role of computation with projection mapping; they used a projector to display a computer image that is mapped to real, physical features. One example is their project Urp, developed at MIT, and it is an urban planning tool that shows information about shadows, proximity, reflections, wind flow and visual space using small architectural models with a projection layered on the topographical surfaces. A movement of designers just after the web-bubble, started hacking keyboards to create physical button that can help create physical interactions. The design consultancy Tellart LLC, pioneered this methodology in RISD, where they taught industrial design students how to use standard electronics to sketch in hardware. The students hacked keyboards to create physical actuation in order to open and close a circuit, triggering keypresses on a personal computer, or connected RFID readers as inputs for a webpage (Noble, 2012). Today there are different electronic components that simulate a keyboard input. The makeymakey, iPac boards, Bare Conductive touch-boards or Playtronica boards. These devices allow to simulate keypresses; these keypresses can be used to trigger a slideshow software, like PowerPoint or Keynote, or to play or sequence media, like video or audio. The designer, utilizing these boards, can focus on the creation of physical triggers for an interactive space. The physical space is rich of triggers like a person sitting on a chair, a door opening, a person touching a metal object, a loud sound, a change in light intensity, a gas detected in the air. In this paper physical triggers will be simply referred as buttons. Buttons and projections are great first steps to prototype interactive spaces, because their purpose is to expand the reach of a single computing unit, both enhancing the inputs of a personal computer with distributed physical buttons, and in outputs, using projections to expand the reach of the screen in space. Of course, this is just the first step towards the creation of engaging interactions in space. Most of the time designers want to enable more complex sensing capabilities and more actuations beyond projections. In fact, actuation expands beyond pixels using motors, lights, relays, etc. The complexity of prototyping these experiences is proportional to the expensive palette of sensors and actuators. A simple approach in creating an interactive space, like the one described above, will be to consider every element of the system as just a set of inputs of sensors and actuators, while the central logic is confined in just one single place. If recalling the above example, it is possible to imagine that at the beginning of the project the designer wants to test their initial", "venue": "", "year": 2021, "referenceCount": 13, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2515261", "name": "Pierluigi Dalla Rosa"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2b8440a784b141a0ec4b7971ee41b4bc3eec10dc", "externalIds": {"MAG": "3129630401", "DOI": "10.1080/03080188.2020.1831227"}, "url": "https://www.semanticscholar.org/paper/2b8440a784b141a0ec4b7971ee41b4bc3eec10dc", "title": "As perceived, not as known: digital enquiry and the art of intelligence", "abstract": "\u1f10\u1f70\u03bd \u03bc\u1f74 \u1f14\u03bb\u03c0\u03b7\u03c4\u03b1\u03b9 \u1f00\u03bd\u03ad\u03bb\u03c0\u03b9\u03c3\u03c4\u03bf\u03bd \u03bf\u1f50\u03ba \u1f10\u03be\u03f5\u03c5\u03c1\u03ae\u03c3\u03f5\u03b9, \u1f00\u03bd\u03f5\u03be\u03f5\u03c1\u03f5\u03cd\u03bd\u03b7\u03c4\u03bf\u03bd \u1f10\u1f78\u03bd \u03ba\u03b1\u1f76 \u1f04\u03c0\u03bf\u03c1\u03bf\u03bd. (\u2018If you do not expect the unexpected, you will not find it; for it is hard to be sought out and difficult\u2019) \u2003Heraclitus (DK 1...", "venue": "", "year": 2021, "referenceCount": 501, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "145918407", "name": "W. McCarty"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "314ff8d5ce6ba7b6d0e58342f62f316f679e19bf", "externalIds": {"DOI": "10.1145/3477339.3477350"}, "url": "https://www.semanticscholar.org/paper/314ff8d5ce6ba7b6d0e58342f62f316f679e19bf", "title": "Bibliography", "abstract": null, "venue": "Software", "year": 2021, "referenceCount": 533, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2113716897", "name": "L. Johnson"}, {"authorId": "2072953076", "name": "J. H. Palmer"}, {"authorId": "12883317", "name": "E. Pugh"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "319c65bbb3605754b65071de6cb8e96e06c83c25", "externalIds": {"MAG": "3185245581", "DOI": "10.1002/AISY.202100054"}, "url": "https://www.semanticscholar.org/paper/319c65bbb3605754b65071de6cb8e96e06c83c25", "title": "A Marr's Three\u2010Level Analytical Framework for Neuromorphic Electronic Systems", "abstract": null, "venue": "Advanced Intelligent Systems", "year": 2021, "referenceCount": 929, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2130035807", "name": "Yunpeng Guo"}, {"authorId": null, "name": "Xiaolong Zou"}, {"authorId": null, "name": "Yifan Hu"}, {"authorId": "2134788826", "name": "Yifei Yang"}, {"authorId": "2135829008", "name": "Xinxin Wang"}, {"authorId": null, "name": "Yuhan He"}, {"authorId": "2128278588", "name": "Ruikai Kong"}, {"authorId": "2844219", "name": "Yuzheng Guo"}, {"authorId": null, "name": "Guoqi Li"}, {"authorId": null, "name": "Wei Zhang"}, {"authorId": null, "name": "Si Wu"}, {"authorId": null, "name": "Huanglong Li"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3cf0f675e22c41b5e93737939b84560f755bc303", "externalIds": {"MAG": "3183456912", "DOI": "10.1007/S40614-021-00302-1", "PubMed": "34632279"}, "url": "https://www.semanticscholar.org/paper/3cf0f675e22c41b5e93737939b84560f755bc303", "title": "Ode to Zig (and the Bard): In Support of an Incomplete Logical-Empirical Model of Direct Instruction.", "abstract": "In this article, I offer my perspective on several elements of Engelmann's Direct Instruction. I hypothesize Engelmann's thinking about the schooling environment that arguably provoked his theoretical, philosophical, and conceptual insights into the design of Direct Instruction. I also examine the research on Direct Instruction as a national educational model, but only as an extension of Engelmann's commitment to falsifying his own thinking. In addition, I survey the research on the design of instruction to highlight how greatly different disciplines can find common ground around \"faultless communications.\" Along the way, I offer examples and descriptive analyses of selected design of instruction elements of Direct Instruction. Finally, I conclude with a brief ode to Engelmann.", "venue": "Perspectives on behavior science", "year": 2021, "referenceCount": 54, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "4142048", "name": "Edward J. Kame\u2019enui"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4f07bd31a92285798828302321a646edbafbaf37", "externalIds": {"MAG": "3187004658", "DOI": "10.1007/978-3-030-70373-8_4"}, "url": "https://www.semanticscholar.org/paper/4f07bd31a92285798828302321a646edbafbaf37", "title": "Sharing Information (or Not) for Computer Development", "abstract": null, "venue": "History of Computing", "year": 2021, "referenceCount": 32, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "49732434", "name": "B. Longo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5c7c3c9a2c87c55935885b24625814bb37adf9cd", "externalIds": {"MAG": "3184743799", "DOI": "10.1007/978-3-030-70373-8_8"}, "url": "https://www.semanticscholar.org/paper/5c7c3c9a2c87c55935885b24625814bb37adf9cd", "title": "Establishing the Field of Computer Science", "abstract": null, "venue": "History of Computing", "year": 2021, "referenceCount": 51, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "49732434", "name": "B. Longo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "6663a15bd49db9b994377b8d8780abb2b59828a3", "externalIds": {"DOI": "10.1007/978-1-4842-6306-8"}, "url": "https://www.semanticscholar.org/paper/6663a15bd49db9b994377b8d8780abb2b59828a3", "title": "C++20 for Lazy Programmers: Quick, Easy, and Fun C++ for Beginners", "abstract": null, "venue": "", "year": 2021, "referenceCount": 78, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "40982698", "name": "Will Briggs"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7b5c533b8671d88718b399e1eeacd1851e5e0a5f", "externalIds": {"DOI": "10.1201/9781315162539-11"}, "url": "https://www.semanticscholar.org/paper/7b5c533b8671d88718b399e1eeacd1851e5e0a5f", "title": "Claude Shannon", "abstract": "A glance at the accomplishments, life, and legacy of Claude Shannon. Claude Elwood Shannon is best known as the founder of information theory. However, Shannon also started or made significant contributions to many other disciplines. In fact, upon considering the totality of his work, it may not be an overstatement to say that Shannon was the key visionary behind the Information Age we find ourselves in today. Despite all his contributions, Shannon remains unknown to the general public. He was a very low-profile person that shied away from fame, and had no interest in recognition or promoting his own research. Perhaps he was more concerned with having fun. 1. Selected Research \u2022 Circuit Design, Boolean Logic, and Switching Theory (1937): At the age of 21, Shannon writes his Master\u2019s thesis \u201cA Symbolic Analysis of Relay and Switching Circuits\u201d [Sha38] demonstrating that electrical circuits could be used to do Boolean logic, setting the foundation underlying all electronic digital computers and digital circuit design. \u2022 Genetics (1940): Shannon writes his PhD thesis at MIT, \u201cAn Algebra for Theoretical Genetics\u201d, developing mathematical relationships for Mendelian genetics. It remained unpublished until the 1990s, although many considered it to be 30-40 years ahead of its time [RW02]. Date: Last updated on March 8, 2009. Author\u2019s e-mail: willywutang AT gmail DOT com.", "venue": "Secret History", "year": 2021, "referenceCount": 29, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2711170", "name": "Craig P. Bauer"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "84f16a36a1a3f9609dafca6235e127ad6a94cced", "externalIds": {"DBLP": "journals/itpro/Charette21", "DOI": "10.1109/MITP.2021.3070986"}, "url": "https://www.semanticscholar.org/paper/84f16a36a1a3f9609dafca6235e127ad6a94cced", "title": "Demanding Fair and Ethically Aligned IT for the Future", "abstract": "For 75 years, the basic measure of computing success has been focused on improving information technology's (IT) technical know-how. Concerns about whether that know-how was being used fairly and ethically was a remote afterthought. Emerging technical know-how and its potential for genuinely harmful misuse demand that the fair and ethical use of IT must be intimately tied to the concept of success.", "venue": "IT Professional", "year": 2021, "referenceCount": 52, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2663266", "name": "R. Charette"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b188eaad12a810f21fe7fc3863d9e1a6c0d6cb50", "externalIds": {"MAG": "3185744688", "DOI": "10.1007/978-3-030-70373-8_6"}, "url": "https://www.semanticscholar.org/paper/b188eaad12a810f21fe7fc3863d9e1a6c0d6cb50", "title": "Technology Development Strains Standardization of Human Communication", "abstract": null, "venue": "History of Computing", "year": 2021, "referenceCount": 29, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "49732434", "name": "B. Longo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "c3f4dcb3660dd759072d743b993e9752c284b93d", "externalIds": {"MAG": "3186553056", "DOI": "10.1007/978-3-030-70373-8_5"}, "url": "https://www.semanticscholar.org/paper/c3f4dcb3660dd759072d743b993e9752c284b93d", "title": "Defining Relationships Among Computers, People, and Information", "abstract": null, "venue": "History of Computing", "year": 2021, "referenceCount": 33, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "49732434", "name": "B. Longo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "eb6c33a43bffc8bedb01462e82b35afc75be2563", "externalIds": {"ArXiv": "2103.05705"}, "url": "https://www.semanticscholar.org/paper/eb6c33a43bffc8bedb01462e82b35afc75be2563", "title": "The Los Alamos Computing Facility during the Manhattan Project", "abstract": "This article describes the history of the computing facility at Los Alamos during the Manhattan Project, 1944 to 1946. The hand computations are briefly discussed, but the focus is on the IBM Punch Card Accounting Machines (PCAM). During WWII the Los Alamos facility was one of most advanced PCAM facilities both in the machines and in the problems being solved.", "venue": "", "year": 2021, "referenceCount": 28, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Physics"], "authors": [{"authorId": "144046727", "name": "B. Archer"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "f8eed079b1feec88217f33a8bd8b4a13932efb79", "externalIds": {"ArXiv": "2103.06211"}, "url": "https://www.semanticscholar.org/paper/f8eed079b1feec88217f33a8bd8b4a13932efb79", "title": "Trinity by the Numbers: The Computing Effort that Made Trinity Possible", "abstract": "This article addresses shortcomings in the existing secondary literature describing the nature and involvement of computing at the World War II Los Alamos Lab. Utilizing rarely used source materials, and identifying points of bias among more commonly used sources, this article provides a more complete representation of wartime Los Alamos' computing operations and personnel, including the Lab's typically under-represented human computers, and how they contributed to the success of the Trinity test. This article also identifies how the Lab's unusual wartime computing demands served as a formative experience among many Los Alamos personnel and consultants, who contributed significantly to the development and use of mechanized computing at and beyond Los Alamos after the war.", "venue": "", "year": 2021, "referenceCount": 55, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Physics"], "authors": [{"authorId": "153378830", "name": "N. Lewis"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "06fc0887dd0919f40ff8105e5617ff99e8341fc0", "externalIds": {"MAG": "3012543320", "DOI": "10.1016/j.metip.2020.100019"}, "url": "https://www.semanticscholar.org/paper/06fc0887dd0919f40ff8105e5617ff99e8341fc0", "title": "\u2018Mental rotation\u2019 in depth as the superficial correlation of pictures", "abstract": "Abstract The Shepard-Metzler effect has been taken as evidence of a mental process of rotation in depth. Simple methods of image processing account for response time differences from the effect as well. The correlation of picture pairs presents an intervening variable between pictured solids and response times. This superficial description illuminates empirical findings such as variation in mental rotation over differences in solid form, and the rate of mental rotation over pictured directions of rotation in depth. The Shepard-Metzler effect is well described in terms of the absolute difference of the areas of profiles of pictures: in other terms, the autocorrelation coefficient of their absolute difference. It follows that \u2018mental rotation\u2019 in depth has no necessary bearing on the identity of solid forms.", "venue": "", "year": 2020, "referenceCount": 40, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "2442183", "name": "K. Niall"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0b705ff54a03a705fde4dbde2d57f598dcbd2e44", "externalIds": {"MAG": "3097695228", "DOI": "10.1201/9781003072676-9"}, "url": "https://www.semanticscholar.org/paper/0b705ff54a03a705fde4dbde2d57f598dcbd2e44", "title": "Is There Chaos Without Noise?", "abstract": null, "venue": "", "year": 2020, "referenceCount": 308, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "36557692", "name": "R. Abraham"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "38a5bc9171e28212c14a9d1c4bdae68ace434f23", "externalIds": {"DBLP": "conf/chi/Arawjo20", "MAG": "3031701916", "DOI": "10.1145/3313831.3376731"}, "url": "https://www.semanticscholar.org/paper/38a5bc9171e28212c14a9d1c4bdae68ace434f23", "title": "To Write Code: The Cultural Fabrication of Programming Notation and Practice", "abstract": "Writing and its means have become detached. Unlike written and drawn practices developed prior to the 20th century, notation for programming computers developed in concert and conflict with discretizing infrastructure such as the shift-key typewriter and data processing pipelines. In this paper, I recall the emergence of high-level notation for representing computation. I show how the earliest inventors of programming notations borrowed from various written cultural practices, some of which came into conflict with the constraints of digitizing machines, most prominently the typewriter. As such, I trace how practices of \"writing code\" were fabricated along social, cultural, and material lines at the time of their emergence. By juxtaposing early visions with the modern status quo, I question long-standing terminology, dichotomies, and epistemological tendencies in the field of computer programming. Finally, I argue that translation work is a fundamental property of the practice of writing code by advancing an intercultural lens on programming practice rooted in history.", "venue": "CHI", "year": 2020, "referenceCount": 250, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Sociology"], "authors": [{"authorId": "9046119", "name": "Ian A. Arawjo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "53f8c600bcd8416cc2d6e81055710d3b682f67f8", "externalIds": {"MAG": "2999679439", "DOI": "10.1007/s00048-019-00238-3", "PubMed": "31932849"}, "url": "https://www.semanticscholar.org/paper/53f8c600bcd8416cc2d6e81055710d3b682f67f8", "title": "Computers and Computing in Society", "abstract": null, "venue": "NTM", "year": 2020, "referenceCount": 36, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Sociology"], "authors": [{"authorId": "2958907", "name": "Val\u00e9rie Schafer"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "9679508be2e5ce68ddca73dff946a2c0c3798def", "externalIds": {"MAG": "3067652060", "DOI": "10.1007/s13347-020-00422-7"}, "url": "https://www.semanticscholar.org/paper/9679508be2e5ce68ddca73dff946a2c0c3798def", "title": "In the Frame: the Language of AI", "abstract": "In this article, drawing upon a feminist epistemology, we examine the critical roles that philosophical standpoint, historical usage, gender, and language play in a knowledge arena which is increasingly opaque to the general public. Focussing on the language dimension in particular, in its historical and social dimensions, we explicate how some keywords in use across artificial intelligence (AI) discourses inform and misinform non-expert understandings of this area. The insights gained could help to imagine how AI technologies could be better conceptualised, explained, and governed, so that they are leveraged for social good.", "venue": "", "year": 2020, "referenceCount": 82, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "118532448", "name": "H. Bones"}, {"authorId": "117350108", "name": "Susan Ford"}, {"authorId": "16926042", "name": "Rachel Hendery"}, {"authorId": "2053560977", "name": "Kate Richards"}, {"authorId": "10999744", "name": "T. Swist"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "a04fcf709df743afabbeda5858b6957a6d204db4", "externalIds": {"MAG": "3106963051", "DOI": "10.1063/5.0029701"}, "url": "https://www.semanticscholar.org/paper/a04fcf709df743afabbeda5858b6957a6d204db4", "title": "Unpredictable random number generator", "abstract": null, "venue": "", "year": 2020, "referenceCount": 14, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "47941127", "name": "Yutaka Shikano"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "aaa35ff098200444477a45f4f138ddd3e409c2c1", "externalIds": {"MAG": "3094884604", "DOI": "10.1201/9781003072676-4"}, "url": "https://www.semanticscholar.org/paper/aaa35ff098200444477a45f4f138ddd3e409c2c1", "title": "Chaos and Intermittency in an Endocrine System Model", "abstract": null, "venue": "", "year": 2020, "referenceCount": 307, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "36557692", "name": "R. Abraham"}, {"authorId": "24496636", "name": "H. Ko\u00e7ak"}, {"authorId": "1390439498", "name": "W. R. Smith"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "de72385738038ca94905967a41e37c0f06dcf805", "externalIds": {"MAG": "3096269727", "DOI": "10.1201/9781003072676-2"}, "url": "https://www.semanticscholar.org/paper/de72385738038ca94905967a41e37c0f06dcf805", "title": "Chaostrophes, Intermittency, and Noise", "abstract": null, "venue": "", "year": 2020, "referenceCount": 307, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "36557692", "name": "R. Abraham"}]}}, {"contexts": ["phasised that the American information technology industry thrives to a large extent on the basis of an extraordinary track record of public research funded by the Federal government since the 1940s (Goldstine, 1972; Flamm, 1987; NRC, 1995; Zimet et al., 2009, Mazzucato, 2015). IBM has systematically co-operated with many of the American governmental agencies interested in faster and faster computing, and superc", "\u2026systematically emphasised that the American information technology industry thrives to a large extent on the basis of an extraordinary track record of public research funded by the Federal government since the 1940s (Goldstine, 1972; Flamm, 1987; NRC, 1995; Zimet et al., 2009, Mazzucato, 2015)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "0d740636842eecb4c1dc6efc6e9eba681bf28e90", "externalIds": {"MAG": "2979476381", "ArXiv": "1912.05504", "DOI": "10.1093/icc/dtz051"}, "url": "https://www.semanticscholar.org/paper/0d740636842eecb4c1dc6efc6e9eba681bf28e90", "title": "Cold numbers: Superconducting supercomputers and presumptive anomaly", "abstract": "\n In February 2014 Time magazine announced to the world that the first quantum computer had been put in use. One key component of this computer is the \u201cJosephson-junction,\u201d a superconducting device, based on completely different scientific and technological principles with respect to semiconductors. The origin of superconductors dates back to the 1960s, to a large-scale 20-year long IBM project aimed at building ultrafast computers. We present a detailed study of the relationship between Science and Technology making use of the theoretical tools of presumptive anomaly and technological paradigms: superconductors were developed while the semiconductors revolution was in full swing. We adopt a historiographical approach\u2014using a snowballing technique to sift through the relevant literature from various epistemological domains and technical publications\u2014to extract theoretically robust insights from a narrative which concerns great scientific advancements, technological leaps forward and business-driven innovation. The study we present shows how technological advancements, business dynamics, and policy intertwine.", "venue": "Industrial and Corporate Change", "year": 2019, "referenceCount": 190, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Engineering"], "authors": [{"authorId": "98923347", "name": "N. Liso"}, {"authorId": "2496302", "name": "G. Filatrella"}, {"authorId": "70917967", "name": "D. Gagliardi"}, {"authorId": "1388575950", "name": "C. Napoli"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "14d1ac7c4fd5a5af4778bc33ee4f72d697392cb3", "externalIds": {"MAG": "2943673326", "DOI": "10.1007/978-3-030-02152-8_3"}, "url": "https://www.semanticscholar.org/paper/14d1ac7c4fd5a5af4778bc33ee4f72d697392cb3", "title": "Forgotten Machines: The Need for a New Master Narrative", "abstract": "History of computing seeks, amongst other things, to provide a narrative for the overwhelming success of the modern electronic digital computer. The datum for these accounts tends to be a finite set of machines identified as developmental staging posts. The reduction of the datum to a set of canonical machines crops out of the frame other machines that were part of the contemporary context but which do not feature in the prevailing narrative. This paper describes two pre-electronic systems designed and built before the general-purpose stored programme digital computer became the universal default and the de facto explanandum for modern history. Both machines belong to the era in which \u201canalog\u201d and \u201cdigital\u201d emerged as defining descriptors. Neither of these machines can be definitively categorised as one or the other. The Julius Totalisator is a large online real-time multi-user system for managing dog and horse race betting; the Spotlight Golf Machine is an indoor interactive golf gaming simulator. Their descriptions here are with a view to expanding the gene pool of early devices of reference and at the same time voice historiographic concerns about the way in which master narratives influence criteria of historical significance.", "venue": "History of Computing", "year": 2019, "referenceCount": 23, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "12948267", "name": "D. Swade"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "165d6d4f56fac5f33cf9c88891481dd4619d3901", "externalIds": {"DBLP": "conf/icict/GulyaevaA19", "MAG": "2996954767", "DOI": "10.1007/978-981-32-9343-4_7"}, "url": "https://www.semanticscholar.org/paper/165d6d4f56fac5f33cf9c88891481dd4619d3901", "title": "The Ontological Approach in Organic Chemistry Intelligent System Development", "abstract": "The amount of knowledge in organic chemistry grows exponentially inducing a need for robust intelligent systems that can promote the process of R&D. Although the methods of intelligent system design vary significantly juxtaposing expert systems, neural networks, genetic algorithms, and fuzzy logic, effective intelligent system development can start only after answering the following essential questions: \u201cHow is the application area structured? What is its ontology?\u201d. Ever since the DENDRAL Project, the challenge of knowledge representation has been embraced by the scientific community. The notion of ontology has appeared in knowledge engineering delivering a possible solution. As the practice shows, taxonomies provide little expressiveness. Therefore, we suggest that the ontological approach advocates consider applied logic methodology. This framework proposes that complex-structured domains, such as organic chemistry, be represented as interconnected modules of applied logic theories. Employing the described technique, we introduce the model of organic chemistry intelligent system. Most special aspects of this methodology are depicted together with a historical overview of intelligent systems and the roots of knowledge representation models.", "venue": "ICICT", "year": 2019, "referenceCount": 19, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1379927108", "name": "K. Gulyaeva"}, {"authorId": "51451482", "name": "I. Artemieva"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "26a3fb79748813db97f76dc0c01f35b07f6485b1", "externalIds": {"MAG": "2939058797", "DOI": "10.1007/S40509-019-00191-9"}, "url": "https://www.semanticscholar.org/paper/26a3fb79748813db97f76dc0c01f35b07f6485b1", "title": "A fluctuation theory of communications", "abstract": "Abstract(1) In this paper, we investigate fluctuation properties of communication configurations by considering an ensemble of measurements in the space of probabilities. In the limit of a large number of symbols, in a message communicated in a noisy channel, we explore statistical properties of fluctuations by analyzing the critical behavior and geometric invariants at a given pair of true and false probabilities as the model parameters. (2) In the light of the system\u2019s global stability, we examine the long-range statistical correlations, whereby discuss the nature of the underlying interacting/noninteracting domains and associated phase transitions under variations of the model probabilities. (3) Finally, we provide possible directions towards the instrumentation technology and engineering designing, their optimal developments and perspective implications of the intrinsic geometric towards the fluctuation theory understanding of joint configurations, entanglement entropy, generalized measurements, quantum channels in communication systems and simultaneous measurements.", "venue": "Quantum Studies: Mathematics and Foundations", "year": 2019, "referenceCount": 55, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Physics"], "authors": [{"authorId": "2788082", "name": "B. N. Tiwari"}, {"authorId": "2069418485", "name": "P. K. Kalra"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3ccbe459fc13e5b24c8aab1a9a0f21fe1d2a7996", "externalIds": {"MAG": "2943721673", "DOI": "10.1007/978-3-030-02152-8_5"}, "url": "https://www.semanticscholar.org/paper/3ccbe459fc13e5b24c8aab1a9a0f21fe1d2a7996", "title": "Switching the Engineer\u2019s Mind-Set to Boolean: Applying Shannon\u2019s Algebra to Control Circuits and Digital Computing (1938\u20131958)", "abstract": "It belongs to the lore of computer science that Claude Shannon\u2019s master\u2019s thesis (1937) revolutionized the design of (relay) switching circuit design. However, as often is the case when taking a closer look at the historical records, things were slightly more complex. Neither was Shannon\u2019s paper an isolated result in switching theory, nor was it immediately absorbed into the engineers\u2019 daily practice. It proved to be only usable in a small number of situations and had to be used in conjunction with other techniques and the engineer\u2019s know-how. Boolean algebra would only become more important and more generally useful once standard situations were created.", "venue": "History of Computing", "year": 2019, "referenceCount": 29, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3282150", "name": "Maarten Bullynck"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3d5673cc7abb420a8651b64fca43de4344cbc37d", "externalIds": {"DBLP": "journals/ijmms/Gaines19", "MAG": "2946471652", "DOI": "10.1016/J.IJHCS.2019.05.007"}, "url": "https://www.semanticscholar.org/paper/3d5673cc7abb420a8651b64fca43de4344cbc37d", "title": "From facilitating interactivity to managing hyperconnectivity: 50 years of human-computer studies", "abstract": "Abstract Five decades ago advances in integrated circuits and time-sharing operating systems made interactive use of computers economically feasible. Visions of man-computer symbiosis and the augmentation of man\u2019s intellect became realistic research objectives. The initial focus was on facilitating interactivity through improved interface technology, and supporting its application through good practices based on experience and psychological principles. Within a decade technology advances made low cost personal computers commonly available in the home, office and industry, and these were rapidly enhanced with software that made them attractive in a wide range of applications from games to office automation and industrial process control. Within three decades the Internet enabled human\u2013computer interaction to extend across local, national and international networks, and, within four, smartphones and tablets had made access to computers and networks almost ubiquitous to any person, at any time and any place. Banking, commerce, institutional and company operations, utility and government infrastructures, took advantage of, and became dependent on, interactive computer and networking capabilities to such an extent that they have now been assimilated in our society and are taken for granted. This hyperconnectivity has been a major economic driver in the current millennium, but it has also raised new problems as malevolent users anywhere in the world have become able to access and interfere with critical personal, commercial and national resources. A major issue for human\u2013computer studies now is to maintain and enhance functionality, usability and likeability for legitimate users whilst protecting them from malevolent users. Understanding the issues involved requires a far broader consideration of socio-economic issues than was required five decades ago. This article reviews various models of the role of technology in human civilization that can provide insights into our current problematique.", "venue": "Int. J. Hum. Comput. Stud.", "year": 2019, "referenceCount": 446, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Business"], "authors": [{"authorId": "145677147", "name": "B. Gaines"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5f460094e7583ea78b2d52f0af4257c1fbe0e23b", "externalIds": {"MAG": "2962474542", "DOI": "10.1007/978-3-030-18815-3_5"}, "url": "https://www.semanticscholar.org/paper/5f460094e7583ea78b2d52f0af4257c1fbe0e23b", "title": "Scientific Progress and Its Impact on Wind", "abstract": "This chapter deals with fluid dynamics, probability theory and automatic computation. Just in the first half of the twentieth century they displayed major improvements, both in general terms and as regards the basic tools for the knowledge of wind that would come to maturity in the second half of the twentieth century. After almost two centuries of trials, fluid dynamics overcame the doubts about D\u2019Alembert\u2019s paradox and the resistance of bodies, formulating the founding principles of the boundary layer, of the vortex wake, and of the transition from laminar to turbulent flows. The probability theory, reorganised on axiomatic grounds, produced a broad range of developments including extreme value theory, principal component analysis, random processes and Monte Carlo methods. The appearance of the electronic computer gave rise to advances firstly addressed to meteorological forecasts, then aimed to solve the increasingly complex problems posed by the renewed culture of the wind.", "venue": "Wind Science and Engineering", "year": 2019, "referenceCount": 184, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "47344808", "name": "G. Solari"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "bab5c4d03b77a9a3cdc1a05c1a26f88924ff2861", "externalIds": {"MAG": "2957602575", "DOI": "10.1007/978-3-030-18815-3_3"}, "url": "https://www.semanticscholar.org/paper/bab5c4d03b77a9a3cdc1a05c1a26f88924ff2861", "title": "The Wind and the New Science", "abstract": "This chapter starts dealing with the transition from speculation to experience that took place in the two centuries going from the birth of Leonardo da Vinci to the death of Galileo Galilei, showing how the wind culture received stimuli from the evolution pervading the various knowledge sectors, starting with the ones connected to the appearance and diffusion of weather instruments and measurements. Similarly, the revitalised wind culture drew essential concepts and principles from the basic disciplines that arose and developed from the sixteenth century onward, first mathematics and the related tools capable of automatically performing complex and repetitive computations, probability theory, destined to become an essential tool for wind engineering, mechanics, in its broadest meaning, fluid dynamics, that mostly provided direct and innovative contributions to the wind culture, thermodynamics, essential to interpret the Earth atmosphere as a giant thermal machine and as a key issue for the development of the steam machine, which bounded its progress to the measure and knowledge of wind, the gas kinetic theory and an essential reference for the first theories about turbulence. The chapter ends with a synthesis of the main aspects characterising the origins and the first developments of structural mechanics and dynamics, two matters that became increasingly vital to protect human works from wind actions.", "venue": "Wind Science and Engineering", "year": 2019, "referenceCount": 58, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "47344808", "name": "G. Solari"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "ef70bf8d9507df74cfabb23a9f26aef513cf222d", "externalIds": {"MAG": "3011594242", "DOI": "10.1109/HISTELCON47851.2019.9039963"}, "url": "https://www.semanticscholar.org/paper/ef70bf8d9507df74cfabb23a9f26aef513cf222d", "title": "Tinkerers Ever to Chance: Engineers, Computers, and the Rise of Probablistic Thinking", "abstract": "Historians of scientific thought and philosophy have emphasized during the Enlightenment a shift from chance-based medieval thought to probability-based modern thought. They have not, however, focused on the technology that makes the implementation of such thought possible. At the same time, historians of computing have emphasized the rise of mechanical calculators\u2014predecessors of modern computers\u2014in this same period, but only in reference to solving practical problems arising from increasingly complex societies. This paper will draw on the two different streams of historical thought to show how the rise of probability and statistics and the rise of mechanical calculating devices were inextricably linked in complex ways.", "venue": "2019 6th IEEE History of Electrotechnology Conference (HISTELCON)", "year": 2019, "referenceCount": 51, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["History"], "authors": [{"authorId": "2130231", "name": "M. Geselowitz"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0edd1ed92988578c57753d9c50d06d7e873fcf4f", "externalIds": {"MAG": "2810460569", "DOI": "10.1007/978-94-024-1531-5_9"}, "url": "https://www.semanticscholar.org/paper/0edd1ed92988578c57753d9c50d06d7e873fcf4f", "title": "High-Performance Small-Scale Raster Map Projection Empowered by Cyberinfrastructure", "abstract": "This chapter reports on the merging of geospatial data transformation, high-performance computing (HPC), and cyberinfrastructure (CI) domains for map projection transformation through performance profiling and tuning of pRasterBlaster, a parallel map projection transformation program. pRasterBlaster is built on the desktop version of mapIMG. Profiling was employed in an effort to identify and resolve computational bottlenecks that could prevent the program from scaling to thousands of processors for map projection on large raster datasets. Performance evaluation of a parallel program is critical to achieving projection transformation as factors such as the number of processors, overhead of communications, and input/output (I/O) all contribute to efficiency in an HPC environment. Flaws in the workload distribution algorithm, in this reported work, could hardly be observed when the number of processors was small. Without being exposed to large-scale supercomputers through software integration efforts, such flaws might remain unidentified. Overall, the two computational bottlenecks highlighted in this chapter, workload distribution and data-dependent load balancing, showed that in order to produce scalable code, profiling is an important process and scaling tests are necessary to identify bottlenecks that are otherwise difficult to discover.", "venue": "CyberGIS for Geospatial Discovery and Innovation", "year": 2018, "referenceCount": 41, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "38010657", "name": "Michael P. Finn"}, {"authorId": "144102534", "name": "Yan Liu"}, {"authorId": "69870938", "name": "David M. Mattli"}, {"authorId": "2267427", "name": "Babak Behzad"}, {"authorId": "21795239", "name": "Kristina H. Yamamoto"}, {"authorId": "2138764256", "name": "Qingfeng (Gene) Guan"}, {"authorId": "2922516", "name": "Eric Shook"}, {"authorId": "144300025", "name": "Anand Padmanabhan"}, {"authorId": "150235209", "name": "Michael Stramel"}, {"authorId": "2989265", "name": "Shaowen Wang"}]}}, {"contexts": ["This problem had to wait until Bush\u2019s differential analyser built at MIT in 1931; that is, more than fifty years after Kelvin\u2019s work (Goldstine, 1973)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "163dd059be3920980f6370acbe79c7619b70e555", "externalIds": {"MAG": "2889726836"}, "url": "https://www.semanticscholar.org/paper/163dd059be3920980f6370acbe79c7619b70e555", "title": "Computing, Modelling, and Scientific Practice: Foundational Analyses and Limitations", "abstract": "This dissertation examines aspects of the interplay between computing and scientific practice. The appropriate foundational framework for such an endeavour is rather real computability than the classical computability theory. This is so because physical sciences, engineering, and applied mathematics mostly employ functions defined in continuous domains. But, contrary to the case of computation over natural numbers, there is no universally accepted framework for real computation; rather, there are two incompatible approaches \u2013computable analysis and BSS model\u2013, both claiming to formalise algorithmic computation and to offer foundations for scientific computing. The dissertation consists of three parts. In the first part, we examine what notion of \u2018algorithmic computation\u2019 underlies each approach and how it is respectively formalised. It is argued that the very existence of the two rival frameworks indicates that \u2018algorithm\u2019 is not one unique concept in mathematics, but it is used in more than one way. We test this hypothesis for consistency with mathematical practice as well as with key foundational works that aim to define the term. As a result, new connections between certain subfields of mathematics and computer science are drawn, and a distinction between \u2018algorithms\u2019 and \u2018effective procedures\u2019 is proposed. In the second part, we focus on the second goal of the two rival approaches to real computation; namely, to provide foundations for scientific computing. We examine both frameworks in detail, what idealisations they employ, and how they relate to floating-point arithmetic systems used in real computers. We explore limitations and advantages of both frameworks, and answer questions about which one is preferable for computational modelling and which one for addressing general computability issues. In the third part, analog computing and its relation to analogue (physical) modelling in science are investigated. Based on some paradigmatic cases of the former, a certain view about the nature of computation is defended, and the indispensable role of representation in it is emphasized and accounted for. We also propose a novel account of the distinction between analog and digital computation and, based on it, we compare analog computational modelling to physical modelling. It is concluded that the two practices, despite their apparent similarities, are orthogonal.", "venue": "", "year": 2018, "referenceCount": 114, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "74120031", "name": "F. Papagiannopoulos"}]}}, {"contexts": ["1 Multiprocessor Program Execution Models For sequential computers, the principal execution model in use today is the von Neumann model which consists of a sequential process running in a linear address space [45]."], "isInfluential": false, "intents": ["methodology"], "citingPaper": {"paperId": "2fb5531b38bb2296446130e1a85f9158e40aaaef", "externalIds": {"DOI": "10.1201/9781315275918-7"}, "url": "https://www.semanticscholar.org/paper/2fb5531b38bb2296446130e1a85f9158e40aaaef", "title": "Multiprocessor Systems", "abstract": "Title of Dissertation: COMMUNICATION-DRIVEN CODESIGN FOR MULTIPROCESSOR SYSTEMS Neal Kumar Bambha, Doctor of Philosophy, 2004 Dissertation directed by: Professor Shuvra S. Bhattacharyya Department of Electrical and Computer Engineering Several trends in technology have important implications for embedded systems of the future. One trend is the increasing density and number of transistors that can be placed on a chip. This allows designers to fit more functionality into smaller devices, and to place multiple processing cores on a single chip. Another trend is the increasing emphasis on low power designs. A third trend is the appearance of bottlenecks in embedded system designs due to the limitations of long electrical interconnects, and increasing use of optical interconnects to overcome these bottlenecks. These trends lead to rapidly increasing complexity in the design process, and the necessity to develop tools that automate the process. This thesis will present techniques and algorithms for developing such tools. Automated techniques are especially important for multiprocessor designs. Programming such systems is difficult, and this is one reason why they are not as prevalent today. In this thesis we explore techniques for automating and optimizing the process of mapping applications onto system architectures containing multiple processors. We examine different processor interconnection methods and topologies, and the design implications of different levels of connectivity between the processors. Using optics, it is practical to construct processor interconnections having arbitrary topologies. This can offer advantages over regular interconnection topologies. However, existing scheduling techniques do not work in general for such arbitrarily connected systems. We present an algorithm that can be used to supplement existing scheduling techniques to enable their use with arbitrary interconnection patterns. We use our scheduling techniques to explore the larger problem of synthesizing an optimal interconnection network for a problem or group of problems. We examine the problem of optimizing synchronization costs in multiprocessor systems, and propose new architectures that reduce synchronization costs and permit efficient performance analysis. All the trends listed above combine to add dimensions to the already vast design space for embedded systems. Optimizations in embedded system design invariably reduce to searching vast design spaces. We describe a new hybrid global/local framework that combines evolutionary algorithms with problem-specific local search and demonstrate that it is more efficient in searching these spaces. COMMUNICATION-DRIVEN CODESIGN FOR MULTIPROCESSOR SYSTEMS by Neal Kumar Bambha Dissertation submitted to the Faculty of the Graduate School of the University of Maryland, College Park in partial fulfillment of the requirements for the degree of Doctor of Philosophy 2004 Advisory Committee: Professor Shuvra S. Bhattacharyya, Chairman/Advisor Professor Gang Qu Professor K. J. Ray Liu Professor Joe Mait Professor Jeff Hollingsworth c \u00a9 Copyright by Neal Kumar Bambha 2004", "venue": "Advanced Computer Architectures", "year": 2018, "referenceCount": 94, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "145651668", "name": "S. Shiva"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5aeb136b0521df4bca9d6acfb090f9c533cd4663", "externalIds": {"MAG": "2708462996", "DOI": "10.1163/19552343-13900013"}, "url": "https://www.semanticscholar.org/paper/5aeb136b0521df4bca9d6acfb090f9c533cd4663", "title": "Programming Men and Machines. Changing Organisation in the Artillery Computations at Aberdeen Proving Ground (1916-1946)", "abstract": "\nAfter the First World War mathematics and the organisation of ballistic computations at Aberdeen Proving Ground changed considerably. This was the basis for the development of a number of computing aids that were constructed and used during the years 1920 to 1950. This article looks how the computational organisation forms and changes the instruments of calculation. After the differential analyzer relay-based machines were built by Bell Labs and, finally, the ENIAC, one of the first electronic computers, was built, to satisfy the need for computational power in ballistics during the second World War.", "venue": "Revue de Synth\u00e8se", "year": 2018, "referenceCount": 30, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "3282150", "name": "Maarten Bullynck"}]}}, {"contexts": ["It became a \u201cuniversal automatic computer with a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 centralized program\u201d (Goldstine, 1972: 198-199) and the programming, independent of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hardware processes, could be freed to be done \u201con paper\u201d, as Alan Turing (2004: 21) put it."], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "9bef42e3aeee5317646c2442ebec659e2d25fe2c", "externalIds": {"MAG": "2962889899"}, "url": "https://www.semanticscholar.org/paper/9bef42e3aeee5317646c2442ebec659e2d25fe2c", "title": "Neurons spike back: The Invention of Inductive Machines and the Artificial Intelligence Controversy", "abstract": "Since 2010, machine learning based predictive techniques, and more specifically deep learning neural networks, have achieved spectacular performances in the fields of image recognition or automatic translation, under the umbrella term of \u201cArtificial Intelligence\u201d. But their filiation to this field of research is not straightforward. In the tumultuous history of AI, learning techniques using so-called \"connectionist\" neural networks have long been mocked and ostracized by the \"symbolic\" movement. This article retraces the history of artificial intelligence through the lens of the tension between symbolic and connectionist approaches. From a social history of science and technology perspective, it seeks to highlight how researchers, relying on the availability of massive data and the multiplication of computing power have undertaken to reformulate the symbolic AI project by reviving the spirit of adaptive and inductive machines dating back from the era of cybernetics.", "venue": "", "year": 2018, "referenceCount": 59, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2044793", "name": "D. Cardon"}, {"authorId": "3201151", "name": "Jean-Philippe Cointet"}, {"authorId": "2560510", "name": "Antoine Mazi\u00e8res"}]}}, {"contexts": ["He early recognized the potentialities of general purpose digital computers, and his contributions to their design during the decade 1943-53 may well rank as his most important work (see [12])."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "9ec95a7795dbe32c8218ed24647b8628f7d59aac", "externalIds": {"PubMedCentral": "7120959", "DOI": "10.1007/978-3-319-99921-0_73"}, "url": "https://www.semanticscholar.org/paper/9ec95a7795dbe32c8218ed24647b8628f7d59aac", "title": "Laboratories", "abstract": "Patient test samples are taken and examined at outpatient clinics, in bed posts and at policlinic consultations and treatment units. They are collected and transported to central laboratories or examined by smaller laboratory units, adapted to the patient group. Samples are sent in pipes or transported by defined methods to the laboratory. A large number of samples are also sent to other hospitals, laboratories or diverse private laboratories. The following chapter is focused on laboratory safety for patients and personnel to avoid spread of infections between patients, personnel and environment.", "venue": "Prevention and Control of Infections in Hospitals", "year": 2018, "referenceCount": 192, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": null, "authors": [{"authorId": "145845228", "name": "B. Andersen"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "ae58f0633c443b667977c1fede18539bb943e55b", "externalIds": {"DOI": "10.1201/9781315220710"}, "url": "https://www.semanticscholar.org/paper/ae58f0633c443b667977c1fede18539bb943e55b", "title": "Low-Power CMOS Circuits", "abstract": "ion feedback loops with more efficient abstraction specific loops. Thus, the design that is fed forward to the lower abstraction levels is much less likely to be fed back for reworking, and the analysis performed at the lower levels becomes essentially a verification task. The key concept is to identify, as early as possible, the design parameters and trade-offs that are required to meet the project\u2019s power specs. This helps to ensure that the design being fed forward is fundamentally capable of achieving the power targets. Later in the design flow, optimizations at the lower levels can be used to further minimize the power as desired. The feed forward flow is enabled by a high-level analysis tool, such as PowerTheater [2], which can accurately predict power characteristics. These early, high-level analysis capabilities are employed to make informed trade-offs, such as which algorithms and architectures to employ, without having to resort to detailed design efforts or low-level implementations to assess performance against the target power \u00a9 2006 by Taylor & Francis Group, LLC By comparison, a feed forward approach, illustrated in Figure 22.1, replaces these lengthy, cross22-4 Low-Power CMOS Circuits specification. Compared with the traditional top-down methods, the key difference and advantage is added by the early prediction technology. Proceeding in parallel with, or sometimes ahead of, the architecture development is the design of the library macro functions and custom elements, such as datapath cells. These are used in the subsequent implementation phase in which the RTL design is converted into a gate-level netlist. At this point, appropriate optimizations are performed again, and power is reestimated with more detailed information, such as floorplanned wiring capacitances. The power grid is planned and laid out using this power data. Once the design has been synthesized into a technology mapped gate-level netlist, lower level power optimizations can be employed, using a tool such as PhysicalStudio [3], to further reduce dynamic or leakage power consumption. Specific goals or issues, such as battery life or noise margin repair, will determine the particular optimizations employed. These optimizations can be performed either before (using estimated wiring parasitics) or after routing (using extracted wiring parasitics). In either case, after the design has been routed and optimized, a final tape-out verification and electrical verification check is performed with an electrical sign-off tool such as CoolTime [4]. In this step, power is calculated and used to compute and validate key design parameters, such as total power consumption in active and standby modes, junction temperatures, power supply droop, noise margins, and signal delays. Thus, power is analyzed and optimized multiple times, at each abstraction layer following the feed forward approach. Each analysis is successively refined from the previous analysis by using information fed forward from prior design decisions along with new details produced by the most recent design activities. Each optimization, at the various abstraction layers, results in more efficient logic structures to feed forward to the downstream design tasks, thereby successively squeezing out the wasted power. FIGURE 22.1 Feed forward design flow. Create System Specifcation Design Algori thm and Architecture", "venue": "", "year": 2018, "referenceCount": 593, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1690566", "name": "C. Piguet"}]}}, {"contexts": ["Namjena tih signala je izvo\u0111enje takozvanog prekidnog sustava putem kojeg se obavlja izmjena podataka izme\u0111u ra\u010dunala i okoline, [9].", "Ovaj model je temeljen na izvr\u0161avanju slijeda instrukcija pomo\u0107u kojih se ulazni podaci i me\u0111urezultati pretvaraju u tra\u017eene rezultate, [9].", "Prema njoj, memorija je bila podijeljena na primarnu (radnu), sekundarnu i neaktivnu, [9].", "Prema [9] postavljena su tri temeljna zahtjeva koja su poslu\u017eila za odre\u0111ivanje von Neumannove arhitekture."], "isInfluential": true, "intents": ["background"], "citingPaper": {"paperId": "b2a2356b5a550e7f23a49d6433bbde8033eda18e", "externalIds": {"MAG": "2890214485"}, "url": "https://www.semanticscholar.org/paper/b2a2356b5a550e7f23a49d6433bbde8033eda18e", "title": "Analiza sustava SoC terminalnih ure\u0111aja", "abstract": "SoC (engl. System-on-Chip) koncept cini temelj vecine danasnjih terminalnih ure\u0111aja. Primjene tih sustava su gotovo neogranicene sto otvara mogu", "venue": "", "year": 2018, "referenceCount": 63, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Physics"], "authors": [{"authorId": "81298660", "name": "Marko Soldo"}]}}, {"contexts": ["\u2026are still used for 3D printing today (Mouritz et al., 1999), while the Jacquard head and the punch cards containing the weaving patterns are rightly considered early forms of computation hardware and software (Essinger, 2004; Fernaeus et al., 2012; Goldstine, 1972; Goldstine, 1977; Manovich, 2001)."], "isInfluential": false, "intents": ["methodology"], "citingPaper": {"paperId": "c3bc8fb7c028a3d661172d5faa4c8ede00b020cc", "externalIds": {"MAG": "1122464259", "DOI": "10.2139/SSRN.2520807"}, "url": "https://www.semanticscholar.org/paper/c3bc8fb7c028a3d661172d5faa4c8ede00b020cc", "title": "The Cash is in the Medium, Not in the Machine: Toward the Golden Moments of 3D Printing", "abstract": "\"Abstract: Since 3D printing technology has been available as early as in the early 19th century, the present article start from the question why this radical and probably disruptive technology has been observed as only incremental innovation for so long time. In answering this question, we assume that this incrementalization of the supposed key to the next industrial revolution occurred due to circumstances that complicated and complexed the observation, with the most important of which being that 3D printers do not print on the medium, but rather print the medium, which emerges as form. In this article, this paradox is unfolded in the form of a form-theoretical theory statement on the inherently paradox nature of observation, subsequent to which 3D printing can be observed as both form and medium. In exploring this paradox, we will show that suppliers of 3D printing solutions currently try to sell 3D printing as form, whereas demanders observe 3D printing as medium. In focussing the latter side of the distinction, we finally suggest that the key to successful 3D printing business models will be in solutions that relate observations of the technological multifunctionality of 3D printing to a social multifunctionality lens. [...]\" Steffen Roth", "venue": "", "year": 2018, "referenceCount": 82, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "36998810", "name": "Steffen Roth"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "06b09cfd3d9e5f536ea438d63383220e1138e7f5", "externalIds": {"MAG": "2760984708", "DOI": "10.1017/9781316823736"}, "url": "https://www.semanticscholar.org/paper/06b09cfd3d9e5f536ea438d63383220e1138e7f5", "title": "A First Course in Computational Fluid Dynamics", "abstract": null, "venue": "", "year": 2017, "referenceCount": 218, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Physics"], "authors": [{"authorId": "2826170", "name": "H. Aref"}, {"authorId": "145012048", "name": "S. Balachandar"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "148346c5290b54b64977bc43af064bdaf8a37beb", "externalIds": {"MAG": "3021135788", "DOI": "10.4324/9781315455891-8"}, "url": "https://www.semanticscholar.org/paper/148346c5290b54b64977bc43af064bdaf8a37beb", "title": "Module 1 Getting Started", "abstract": "Over 9 million people pass through America\u2019s local jails each year, and these individuals often receive little in the way of services, support, or supervision as they reenter the community. In response to the need for jurisdictions across the country to address the jail-to-community transition, the National Institute of Corrections (NIC) (http://www.nicic.org/JailTransition) partnered with the Urban Institute (UI) in 2007 to launch the Transition from Jail to Community (TJC) Initiative (http://www.urban.org/policy-centers/justice-policy-center/projects/transitionjail-community-tjc-initiative).", "venue": "", "year": 2017, "referenceCount": 121, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "1913237", "name": "G. Vega"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "26744b66b57484f7f79b4193dddfbb02ccf4a5d3", "externalIds": {"MAG": "2616882399", "DOI": "10.1016/j.pbiomolbio.2017.05.010", "PubMed": "28571717"}, "url": "https://www.semanticscholar.org/paper/26744b66b57484f7f79b4193dddfbb02ccf4a5d3", "title": "Mysticism in the history of mathematics.", "abstract": "We examine the relationship between mysticism and mathematical creativity through case studies from the history of mathematics.", "venue": "Progress in biophysics and molecular biology", "year": 2017, "referenceCount": 326, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Philosophy", "Medicine"], "authors": [{"authorId": "36557692", "name": "R. Abraham"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3471c052f32d4028ac0c4fd01398a6bc63d57256", "externalIds": {"DBLP": "conf/eurocast/Cull17a", "MAG": "2784421035", "DOI": "10.1007/978-3-319-74718-7_29"}, "url": "https://www.semanticscholar.org/paper/3471c052f32d4028ac0c4fd01398a6bc63d57256", "title": "The Computer and the Calculator", "abstract": "Some recent discussions [1, 2, 3, 4, 5] have suggested that the concept of universal stored program computer is not useful in understanding the history of computing. In particular, there is the suggestion that this idea was so well known that all of the early computing devices already incorporated this concept. Here, we argue that all or almost all of the early digital machines were based on the idea of a calculator and that the computer was a real and significant new concept. We attempt to explain the differences between calculator and computer, and try to show that our contemporary computing is based on the computer rather than the calculator, and that the calculator model is inadequate to describe our current notions of computing.", "venue": "EUROCAST", "year": 2017, "referenceCount": 18, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2636890", "name": "P. Cull"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "46f4930985a64c90c873ce50b11519c516da9a75", "externalIds": {"MAG": "2558809598", "DOI": "10.1016/J.IJNONLINMEC.2016.11.011"}, "url": "https://www.semanticscholar.org/paper/46f4930985a64c90c873ce50b11519c516da9a75", "title": "Reprogrammable logic-memory device of a mechanical resonator", "abstract": "Abstract From the viewpoint of application of nonlinear dynamics, we report multifunctional operation in a single microelectromechanical system (MEMS) resonator. This paper addresses a reprogrammable logic-memory device that uses a nonlinear MEMS resonator with multi-states. In order to develop the reprogrammable logic-memory device, we discuss the nonlinear dynamics of the MEMS resonator with and without control input as logic and memory operations. Through the experiments and numerical simulations, we realize the reprogrammable logic function that consists of OR/AND gate by adjusting the excitation amplitude and the memory function by storing logic information in the single nonlinear MEMS resonator.", "venue": "", "year": 2017, "referenceCount": 48, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2065246974", "name": "Atsushi Yao"}, {"authorId": "1689955", "name": "T. Hikihara"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4d7a36757abfeca81c05f0f2e6fb8d35c28aa956", "externalIds": {"DBLP": "journals/annals/CopelandHGW17", "MAG": "2343917781", "DOI": "10.1109/MAHC.2016.1"}, "url": "https://www.semanticscholar.org/paper/4d7a36757abfeca81c05f0f2e6fb8d35c28aa956", "title": "Screen History: The Haeff Memory and Graphics Tube", "abstract": "Haeff-type tubes formed the high-speed memory of Whirlwind I but had their greatest impact in graphics and display technology, remaining in widespread use until the 1980s. Haeff seems to have been the first to store and display graphics and text on an electronic screen for an unlimited period, in 1947.", "venue": "IEEE Annals of the History of Computing", "year": 2017, "referenceCount": 141, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144246233", "name": "B. Copeland"}, {"authorId": "3140551", "name": "Andre A. Haeff"}, {"authorId": "33527612", "name": "P. Gough"}, {"authorId": "2065128509", "name": "Cameron Wright"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "59e4c950206b810cef47eebdf90fcfe006f88856", "externalIds": {"MAG": "2586700437", "DOI": "10.4169/MATHHORIZONS.24.3.26"}, "url": "https://www.semanticscholar.org/paper/59e4c950206b810cef47eebdf90fcfe006f88856", "title": "The ENIAC at 70", "abstract": "D uring World War II\u2014before smart bombs\u2014 the U.S. military needed firing tables that would enable artillery operators to aim their guns accurately, taking into account factors such as the location of the target, the type of artillery, and atmospheric conditions. But constructing the tables required using numerical techniques to solve certain differential equations. These calculations were primarily carried out by a phalanx of human computers\u2014many of whom were women\u2014at the Army\u2019s Ballistic Research Lab (BRL) in Aberdeen, Maryland, using desktop calculators. They also used the BRL\u2019s differential analyzer, an analog device that could solve differential equations. Needless to say, both processes were time consuming. The Moore School of Electrical Engineering at the University of Pennsylvania also had a differential analyzer, so the BRL partnered with the Moore School to use its machine and to train additional human computers. However, the production of firing tables for new artillery still fell behind. The idea for what was to become the ENIAC (Electronic Numerical Integrator and Computer) originated with John Mauchly, a physics professor with an interest in electronic computing. In the summer of 1941, he took a course in electronics at the Moore School and then stayed on as an instructor. There he met J. Presper Eckert, a graduate student with whom he discussed his ideas for electronic digital computing. In 1942 Mauchly submitted a proposal, \u201cThe Use of High-Speed Vacuum Tube Devices for Calculation,\u201d which was ignored until 1943 when the need for firing tables became more acute. At this time, Mauchly, Eckert, and John Brainerd, the Moore School\u2019s director of research, submitted a proposal for a high-speed computing device that included Mauchly\u2019s report. The result was government funding for Project PX: the ENIAC. Just over 70 years ago, in February 1946, the U.S. Army unveiled the ENIAC. Though it is widely known that the ENIAC was the first general-purpose electronic computer, it is less generally known how it functioned, how it was programmed, or how it differs from today\u2019s stored program computers.* Here\u2019s an introduction to these topics.", "venue": "", "year": 2017, "referenceCount": 5, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "1860303", "name": "B. Shelburne"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5be366ede2a96a6dcc37bd5467719abbdf2409f5", "externalIds": {"MAG": "2736183614", "DOI": "10.5642/JHUMMATH.201702.18"}, "url": "https://www.semanticscholar.org/paper/5be366ede2a96a6dcc37bd5467719abbdf2409f5", "title": "Discrete and continuous: A fundamental dichotomy in mathematics", "abstract": "The distinction between the discrete and the continuous lies at the heart of mathematics. Discrete mathematics (arithmetic, algebra, combinatorics, graph theory, cryptography, logic) has a set of concepts, techniques, and application areas largely distinct from continuous mathematics (traditional geometry, calculus, most of functional analysis, differential equations, topology). The interaction between the two \u2013 for example in computer models of continuous systems such as fluid flow \u2013 is a central issue in the applicable mathematics of the last hundred years. This article explains the distinction and why it has proved to be one of the great organizing themes of mathematics.", "venue": "", "year": 2017, "referenceCount": 49, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "145170400", "name": "James Franklin"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "840beaad405be3955728ecfa79e3a6964b5ca9fe", "externalIds": {"DBLP": "conf/eurocast/KohlerS17", "MAG": "2787510331", "DOI": "10.1007/978-3-319-74718-7_7"}, "url": "https://www.semanticscholar.org/paper/840beaad405be3955728ecfa79e3a6964b5ca9fe", "title": "Kurt G\u00f6del: A Godfather of Computer Science", "abstract": "We argue that Kurt Godel exercised a major influence on computer science. Although not immediately involved in building computers, he was a pioneer in defining central concepts of computer theory. Godel was the first to show how the precision of the formal language systems of Frege, Peano and Russell could be put to work to prove important facts about those language systems themselves, with important consequences for mathematics. As Hilbert\u2019s collaborator, Paul Bernays put it, in his famous Incompleteness Proof Godel did the \u201chomework\u201d that the people in Gottingen working on Hilbert\u2019s Program to prove the consistency of mathematics missed. (Hilbert\u2019s Metamathematics assumed that all mathematical proofs could be treated as coding problems, and enciphering is applied arithmetic.) The core of Godel\u2019s Proof also gave exact definitions of the central concept of arithmetic, namely of the recursion involved in mathematical induction (with help from the great French logician Jacques Herbrand). This immediately led to whirlwind developments in Gottingen, Cambridge and Princeton, the working headquarters of major researchers: Paul Bernays and John von Neumann; Alan Turing; and Alonzo Church, respectively. Turing\u2019s and von Neumann\u2019s ideas on computer architecture can be traced to Godel\u2019s Proof. Especially interesting is the fact that Church and his lambda calculus was the main influence on John McCarthy\u2019s LISP, which became the major language of Artificial Intelligence.", "venue": "EUROCAST", "year": 2017, "referenceCount": 39, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "116246706", "name": "Eckehart K\u00f6hler"}, {"authorId": "102626475", "name": "Werner Schimanovich"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "98833caa0e5f5487efad76376c69e95ee8ac4ed2", "externalIds": {"DOI": "10.3905/jpm.2017.44.1.083"}, "url": "https://www.semanticscholar.org/paper/98833caa0e5f5487efad76376c69e95ee8ac4ed2", "title": "The Road Not Taken", "abstract": "The index fund revolution has changed the rules of the game for active mutual fund managers. This article offers a range of optimal business strategies for active managers\u2014including management companies owned by financial conglomerates\u2014who are competing in this new environment. The author then contrasts these business strategies for mutual fund managers with optimal fiduciary strategies for mutual fund directors, who are responsible for placing the interests of fund shareholders first.", "venue": "", "year": 2017, "referenceCount": 461, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "103923073", "name": "John C. Bogle"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b297c8cbb5c6e60b880cc93212be15508545344b", "externalIds": {"MAG": "2533306200", "DOI": "10.1007/978-3-319-48317-7_7"}, "url": "https://www.semanticscholar.org/paper/b297c8cbb5c6e60b880cc93212be15508545344b", "title": "Milestones of Information Technology\u2014A Survey", "abstract": "In the following we give a survey on important epochs in the development of the electrical means for transmission and processing of information between humans and machines. The goal is to contribute to a kind of holistic knowledge of the function of the different complex systems and devices of modern information technology. We are convinced that a historical projection on the different stages of development can be of great help. However, by the limited space of this paper we can do this only by a sketch. The list of relevant literature which we give at the end may help to get the desirable deeper knowledge.", "venue": "", "year": 2017, "referenceCount": 1, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145807191", "name": "F. Pichler"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "f31560360090781327c658cbab46d57a08a096ab", "externalIds": {"MAG": "2733718895", "DOI": "10.1007/978-3-319-54517-2_7"}, "url": "https://www.semanticscholar.org/paper/f31560360090781327c658cbab46d57a08a096ab", "title": "Emphasizing the Limits of a Machine Epistemology", "abstract": "This chapter articulates John von Neumann\u2019s contribution to the cybernetic enterprise. Beginning with von Neumann\u2019s appropriation and subsequent critique of Warren McCulloch and Walter Pitts\u2019 neural networks, it examines his criticism to the theoretical path that cybernetics was pursuing at the time. It concludes with addressing his proposal for an alternative venue of research for the scientific enterprise\u2014including his kinematic model and his theory of automata. The insights emerging from this avenue of investigation, as with Ashby, signaled deep theoretical problems that the enterprise carried at its core.", "venue": "", "year": 2017, "referenceCount": 23, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Philosophy"], "authors": [{"authorId": "1409266849", "name": "Alcibiades Malapi-Nelson"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "fa800342bcb035dcc7885761c6d7af3a6bd03e8b", "externalIds": {"MAG": "2181601503"}, "url": "https://www.semanticscholar.org/paper/fa800342bcb035dcc7885761c6d7af3a6bd03e8b", "title": "Chaotic Synchronization in Economic Networks", "abstract": "Synchronization, long an important topic in the theory of nonlinear oscillation, has recently become a research frontier in chaos theory as well. Here we introduce the sync phenomenon for forced and coupled R\u00a8ossler attractors, and its role in the context of complex economic systems. And we extend the basic framework developed by the late Richard Goodwin in his book, Chaotic Economic Dynamics, of 1990 to massively complex dynamical systems of chaotic elements. Recent experimental results and speculative applications to economic networks are presented", "venue": "", "year": 2017, "referenceCount": 307, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "36557692", "name": "R. Abraham"}, {"authorId": "2160432", "name": "M. Nivala"}]}}, {"contexts": ["In the beginning, there were graphs of finite automata (attributed to Shannon and Weaver [39]) and flowcharts of imperative programs (used by Turing [45]; attributed to Goldstine and von Neumann [17])."], "isInfluential": false, "intents": ["methodology"], "citingPaper": {"paperId": "0744dc8d04f569984e87e617f718c53786d5f104", "externalIds": {"MAG": "1995002960", "DBLP": "journals/logcom/DershowitzD16", "DOI": "10.1093/logcom/ext022"}, "url": "https://www.semanticscholar.org/paper/0744dc8d04f569984e87e617f718c53786d5f104", "title": "Universality in two dimensions", "abstract": "Turing, in his immortal 1936 paper, observed that '[human] computing is normally done by writing... symbols on [two- dimensional] paper', but noted that use of a second dimension 'is always avoidable' and that 'the two-dimensional character of paper is no essential of computation'. We propose to promote two-dimensional models of computation and exploit the naturalness of two-dimensional representations of data. In particular, programs for a two-dimensional Turing machine can be recorded most naturally on its own two-dimensional input-output grid in such a transparent fashion that schoolchildren would have no difficulty comprehending their behaviour. This two-dimensional rendering allows, furthermore, for a most perspicacious rendering of Turing's universal machine.", "venue": "J. Log. Comput.", "year": 2016, "referenceCount": 103, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1759551", "name": "N. Dershowitz"}, {"authorId": "1733100", "name": "Gilles Dowek"}]}}, {"contexts": ["2\nSharing in the disciplinarisation vogue for computer science, that same year Herman H. Goldstine published his book The Computer from Pascal to von Neumann ( Goldstine, 1972)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "08ae45c4e2b05e8d4fdf09c55d10ff257f4a20b9", "externalIds": {"MAG": "2197543377", "DOI": "10.1016/J.HM.2015.07.002"}, "url": "https://www.semanticscholar.org/paper/08ae45c4e2b05e8d4fdf09c55d10ff257f4a20b9", "title": "Histories of algorithms: Past, present and future", "abstract": "The book under review, first published some twenty years ago, is the product of a group of French historians of mathematics: Jean-Luc Chabert, Evelyn Barbin, Michel Guillemot, Anne Michel-Pajus, Jacques Borowczyk, Ahmed Djebbar et Jean-Claude Martzloff. An English translation by Chris Weeks was published by Springer in 1999 (Chabert et al., 1999) and a second French edition is now available and will be reviewed here. It was at the time of its first appearance\u2014and is still\u2014 the sole book-length study of the history of mathematical algorithms. Therefore this second corrected and extended edition of the French original is most welcome. The original edition of Histoire d'algorithmes was published at a time of developing interest in the history of algorithms within the mathematics community and it was, in part, conceived for use in the mathematics classroom. The historical range of the book is impressive; from Ancient Mesopotamia and Egypt to modern computer developments, this volume endeavours to forge a history of mathematics through highlighting the role of algorithms. The book contains 15 chapters and closes with short biographies of the authors discussed and a general index. Each chapter opens with a general overview that introduces the reader to the chapter's topic. The topic is illustrated and developed through a series of excerpts taken (or translated into French) from the original, historical documents. The text is always followed by a rewriting in terms of elementary modern mathematics and by some analysis and posthistoire of the algorithm. While the main text of the book has been largely kept as it was in the first edition, this more recent posthistoire has been updated for this second edition. The book, one finds, falls naturally into two parts. The first (chapters 1\u20135), comparative in nature, focuses on the structure of an algorithm at different times in different cultures and traditions, principally, though not exclusively, pre-modern and non-Western. The second part remains largely within the realm of modern Western mathematics (17th\u201320th centuries) and presents a more or less linear story of how certain kinds of algorithms used in numerical analysis were developed and generalized.", "venue": "", "year": 2016, "referenceCount": 67, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "3282150", "name": "Maarten Bullynck"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2758ba22a2c81f418ce82f07214995400f59f095", "externalIds": {"MAG": "2497015282", "DOI": "10.1002/9781118620762.CH33"}, "url": "https://www.semanticscholar.org/paper/2758ba22a2c81f418ce82f07214995400f59f095", "title": "Calculating Devices and Computers", "abstract": "Focusing upon computation, storage, and infrastructures for data from the early modern European period forward, this chapter stresses that the constraints of computing technologies, as well as their possibilities, are essential for the path of computational sciences. Mathematical tables and simple contrivances aided calculation well into the middle of the twentieth century. Digital machines replaced them slowly: adopting electronic digital computers for scientific work demanded creative responses to the limits of technologies of computation, storage, and communication. Transforming the evidence of existing scientific domains into data computable and storable in electronic form challenged ontology and practice alike. The ideational history of computing should pay close attention to its materiality and social forms, and the materialist history of computing must pay attention to its algorithmic ingenuity in the face of material constraints.", "venue": "", "year": 2016, "referenceCount": 109, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "152833508", "name": "Matthew L. Jones"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "56700cb640e052e007e6112ef65ad087cd56310b", "externalIds": {"MAG": "1029622729", "DOI": "10.1007/978-3-662-48918-5_12"}, "url": "https://www.semanticscholar.org/paper/56700cb640e052e007e6112ef65ad087cd56310b", "title": "Analysis auf Schritt und Tritt", "abstract": "\u201eWas sind und was sollen die Zahlen?\u201c fragte sich der Braunschweiger Mathematiker Richard Dedekind in seinem 1888 erschienenen und immer wieder aufgelegten Buch. \u201eWas ist und was soll Analysis?\u201c fragt sich entsprechend der Analytiker.Was Analysis ist \u2013 daruber ist in diesem Buch viel gesagt worden.", "venue": "", "year": 2016, "referenceCount": 4, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Philosophy"], "authors": [{"authorId": "66071670", "name": "T. Sonar"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "6bea705fac33e7d303ef95645218e7eea659f39f", "externalIds": {"DBLP": "journals/annals/Smith16", "MAG": "2526459602", "DOI": "10.1109/MAHC.2015.51"}, "url": "https://www.semanticscholar.org/paper/6bea705fac33e7d303ef95645218e7eea659f39f", "title": "The Dawn of Digital Light", "abstract": "Digital pictures and computers are now inseparable, so it's surprising how generally unremarked their association was in the beginning. Records reveal that the first digital pictures--the first still pictures, videogames, and computer animations--were made on the earliest computers. Historians have noted this before, but individually without a unifying context. This article shows that the original digital pictures were associated with the original computers in the late 1940s and early 1950s. This fresh perspective on digital pictures establishes a different take on the history of early computers and unifies the history of digital light itself.", "venue": "IEEE Annals of the History of Computing", "year": 2016, "referenceCount": 42, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2116707114", "name": "Alvy Ray Smith"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "107505c87f16e0f4754bda26daba954fd7a3ab84", "externalIds": {"MAG": "2184191772"}, "url": "https://www.semanticscholar.org/paper/107505c87f16e0f4754bda26daba954fd7a3ab84", "title": "A Bibliography of Books and Other Publications about the Ada Programming Language and Its History", "abstract": "+ [Tex82]. 0 [Tee94]. 1 [Bab27d, Bab31c, Bab15]. $10.95 [Wim83a]. $1000M [Ano84b]. 108 000 [Bab31c, Bab15]. 108000 [Bab27d]. 129 [Ano93a]. $149.50 [Gra94]. 1791 + 200 = 1991 [Sti91]. $19.95 [Dis91]. $21.50 [Mad86]. $25 [O\u2019H82, Yon75]. $25.00 [Hym78]. $26.50 [Enr80]. $27.95 [L.90]. $28 [Hun96]. $29.95 [Por01]. 3 [EW91, HL93]. $3.95 [LY79]. $32.95 [Ano98]. $35.00 [Ano91d]. $37.50 [Ano91d]. $45.00 [Ano91d]. 653 [CH97]. $7.90 [You82b]. $75.00 [Wol08]. $9.95 [Par89]. 1 [TS85]. 3 [Ano89d]. me [Kno88]. Ada [Bro80]. A 3 [Alb05]. : = [Tex82]. N [RSC93]. q [And99a, And99b].", "venue": "", "year": 2015, "referenceCount": 1371, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2377548", "name": "N. H. Beebe"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3939e0578166461d4c182afc6df1b24190e56e8e", "externalIds": {"ArXiv": "1409.0014", "MAG": "1901379248", "PubMedCentral": "5255938", "DOI": "10.1007/lrr-2015-1", "PubMed": "28179851"}, "url": "https://www.semanticscholar.org/paper/3939e0578166461d4c182afc6df1b24190e56e8e", "title": "Exploring New Physics Frontiers Through Numerical Relativity", "abstract": "The demand to obtain answers to highly complex problems within strong-field gravity has been met with significant progress in the numerical solution of Einstein\u2019s equations \u2014 along with some spectacular results \u2014 in various setups.We review techniques for solving Einstein\u2019s equations in generic spacetimes, focusing on fully nonlinear evolutions but also on how to benchmark those results with perturbative approaches. The results address problems in high-energy physics, holography, mathematical physics, fundamental physics, astrophysics and cosmology.", "venue": "Living reviews in relativity", "year": 2014, "referenceCount": 1001, "citationCount": 66, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Medicine"], "authors": [{"authorId": "143753647", "name": "V. Cardoso"}, {"authorId": "34846986", "name": "L. Gualtieri"}, {"authorId": "8839673", "name": "C. Herdeiro"}, {"authorId": "8844502", "name": "U. Sperhake"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4a650cc3c124af6f69f064cab80fe5c058b7d992", "externalIds": {"DBLP": "journals/logcom/MolCB15", "MAG": "2148579131", "DOI": "10.1093/logcom/exs072"}, "url": "https://www.semanticscholar.org/paper/4a650cc3c124af6f69f064cab80fe5c058b7d992", "title": "Haskell before Haskell: an alternative lesson in practical logics of the ENIAC", "abstract": "This article expands on Curry's work on how to implement the problem of inverse interpolation on the ENIAC (1946) and his subsequent work on developing a theory of program composition (1948-1950). It is shown that Curry's hands-on experience with the ENIAC on the one side and his acquaintance with systems of formal logic on the other, were conductive to conceive a compact \" notation for program construction \" which in turn would be instrumental to a mechanical synthesis of programs. Since Curry's systematic programming technique pronounces a critique of the Goldstine-von Neumann style of coding, his \" calculus of program composition \" not only anticipates automatic programming but also proposes explicit hardware optimisations largely unperceived by computer history until Backus' famous ACM Turing Award lecture (1977). The cohesion of these findings asks for an integrative historiographical approach. An appendix gives, for the first time, a full description of Curry's arithmetic compiler.", "venue": "J. Log. Comput.", "year": 2015, "referenceCount": 73, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "152134130", "name": "L. D. Mol"}, {"authorId": "144618853", "name": "Martin Carl\u00e9"}, {"authorId": "3282150", "name": "Maarten Bullynck"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4b0b03e06ce163780762cf4020c4da1d1762ac53", "externalIds": {"MAG": "1991437366", "DOI": "10.7560/IC50104"}, "url": "https://www.semanticscholar.org/paper/4b0b03e06ce163780762cf4020c4da1d1762ac53", "title": "Human Computing Practices and Patronage: Antiaircraft Ballistics and Tidal Calculations in First World War Britain", "abstract": "During the First World War, both antiaircraft defense and tide prediction became imperative for Britain. These required lengthy calculations by human \u201ccomputers.\u201d One such computer, Arthur Thomas Doodson, became prominent in both antiaircraft ballistics and tide calculations. This article examines the mathematical practices Doodson employed and, through analysis of his financial backers, why the calculations were carried out. Doodson utilized new calculation practices both to increase the complexity and amount of computations that could be carried out and to argue for further funding. Historians of science, including mathematics, can benefit from simultaneous analysis of both the practices and the patronage of scientists.", "venue": "", "year": 2015, "referenceCount": 45, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["History"], "authors": [{"authorId": "1403780837", "name": "Anna Carlsson-Hyslop"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5dfba4d0d2128c19438d711f68b1feb682a3bcf8", "externalIds": {"DBLP": "journals/annals/Cortada15b", "MAG": "2126933855", "DOI": "10.1109/MAHC.2015.55"}, "url": "https://www.semanticscholar.org/paper/5dfba4d0d2128c19438d711f68b1feb682a3bcf8", "title": "Studying History as it Unfolds, Part 1: Creating the History of Information Technologies", "abstract": "As part of a two-part series, this article explores the development of the early history of information technologies from the 1940s to the present. Looking at how historians wrestled with the history of computing is a useful modern case that can offer tactical recommendations for those in other fields of technology.", "venue": "IEEE Annals of the History of Computing", "year": 2015, "referenceCount": 118, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Engineering", "Computer Science"], "authors": [{"authorId": "2932674", "name": "J. Cortada"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "70e4e7732d29c8b4d1da2c26fba411a6c6aba388", "externalIds": {"PubMedCentral": "4435002", "MAG": "2163011812", "DOI": "10.13063/2327-9214.1119", "PubMed": "25992387"}, "url": "https://www.semanticscholar.org/paper/70e4e7732d29c8b4d1da2c26fba411a6c6aba388", "title": "Development and Applications of an Outcomes Assessment Framework for Care Management Programs in Learning Health Systems", "abstract": "Purpose: To develop and apply an outcomes assessment framework (OAF) for care management programs in health care delivery settings. Background: Care management (CM) refers to a regimen of organized activities that are designed to promote health in a population with particular chronic conditions or risk profiles, with focus on the triple aim for populations: improving the quality of care, advancing health outcomes, and lowering health care costs. CM has become an integral part of a care continuum for population-based health care management. To sustain a CM program, it is essential to assure and improve CM effectiveness through rigorous outcomes assessment. To this end, we constructed the OAF as the foundation of a systematic approach to CM outcomes assessment. Innovations: To construct the OAF, we first systematically analyzed the operation process of a CM program; then, based on the operation analysis, we identified causal relationships between interventions and outcomes at various implementation stages of the program. This set of causal relationships established a roadmap for the rest of the outcomes assessment. Built upon knowledge from multiple disciplines, we (1) formalized a systematic approach to CM outcomes assessment, and (2) integrated proven analytics methodologies and industrial best practices into operation-oriented CM outcomes assessment. Conclusion: This systematic approach to OAF for assessing the outcomes of CM programs offers an opportunity to advance evidence-based care management. In addition, formalized CM outcomes assessment methodologies will enable us to compare CM effectiveness across health delivery settings.", "venue": "EGEMS", "year": 2015, "referenceCount": 48, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "46659418", "name": "Lin Wang"}, {"authorId": "1403093221", "name": "Kara L. Kuntz-Melcavage"}, {"authorId": "3023728", "name": "C. Forrest"}, {"authorId": "50028375", "name": "Y. Lu"}, {"authorId": "34912291", "name": "L. Piet"}, {"authorId": "2057359292", "name": "Kathy Evans"}, {"authorId": "87225260", "name": "M. Uriyo"}, {"authorId": "29755921", "name": "M. Sherry"}, {"authorId": "2053790241", "name": "Regina Richardson"}, {"authorId": "31548008", "name": "Michelle R Hawkins"}, {"authorId": "4506937", "name": "D. Neale"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b89839c83602e1c7d2ee594f25911d4b79bbabe9", "externalIds": {"MAG": "2270118066", "DOI": "10.1007/978-1-4471-6732-7_1"}, "url": "https://www.semanticscholar.org/paper/b89839c83602e1c7d2ee594f25911d4b79bbabe9", "title": "The Development of Digital Computers", "abstract": "In the 1950s, the transistor replaced the vacuum tubes that had empowered Eniac, Colossus, and other early computers in the 1940s. In the 1960s and 1970s, computing moved from slow, expensive mainframes to faster mini- and microcomputers and multiprocessors, empowered by chip technology and integrated circuits, and leveraged by increasingly sophisticated operating systems and programming languages. By the 1980s, commercially available programs were able to perform commonly needed computational functions. With the growth of computer capabilities and computer storage capacities, database technology and database management systems gave rise to the development of distributed database systems. Efficient computer-stored databases proved essential to many medical computing applications, making vast amounts of data available to users. Over time computer applications became more numerous and complex, with software claiming a larger fraction of computing costs. Display terminals and clinical workstations offered graphic displays and supported structured data entry and reporting. Devices, such as the mouse, light pens, touch screens, and input technologies, such as speech and handwriting recognition, were developed to ease the user\u2019s tasks and foster physician acceptance. Over the same span of time, computer communications evolved as well, moving from copper wire to fiber optic cable and, most recently, to wireless systems. The Internet and the World Wide Web became the main modes used for local and global communications. By the 2010s laptops replaced desktop computers, and tablets and smart phones were commonplace in health care.", "venue": "", "year": 2015, "referenceCount": 173, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145124963", "name": "M. Collen"}, {"authorId": "3285789", "name": "C. Kulikowski"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "e37a384b43704a095e1be52fca16befae20f50e7", "externalIds": {"MAG": "2187335654"}, "url": "https://www.semanticscholar.org/paper/e37a384b43704a095e1be52fca16befae20f50e7", "title": "A Complete Bibliography of Publications of John von Neumann", "abstract": "This bibliography records publications of John von Neumann (1903\u2013 1957). Title word cross-reference 1 + 2 [vN51c]. $125 [Lup03]. $19.95 [Kev81]. 2, 000 [MRvN50]. $23.00 [MC00]. $25.00 [Jon04]. $29.95 [CK12]. 2 [vNT55]. $35.00 [Ano91, Pan92]. $37.50 [Ano91]. $45.00 [Ano91]. e [MRvN50, Rei50]. F\u221e [vN62]. H [vN29b, von10]. N [vN46b]. \u03bd [vN62]. p [DJ13]. p = K\u03c1 [vNxxb]. \u03c0 [MRvN50, Rei50, She12]. q [DJ13]. T \u03c1 \u03c1 = 0 [vN35h]. -theorem [von10]. -Theorems [vN29b]. /119.00 [Emc02]. /89.00 [Emc02].", "venue": "", "year": 2015, "referenceCount": 1130, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2377548", "name": "N. H. Beebe"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "06bfc200ed3053fcfb8c53bbbaff91d4ace35dff", "externalIds": {"MAG": "2323214159", "DOI": "10.1215/10407391-2420021"}, "url": "https://www.semanticscholar.org/paper/06bfc200ed3053fcfb8c53bbbaff91d4ace35dff", "title": "The cybernetic hypothesis", "abstract": null, "venue": "", "year": 2014, "referenceCount": 22, "citationCount": 60, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "51513816", "name": "A. Galloway"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "162b9358ed9ea3d0b2f521ed3470cccbac2c99ba", "externalIds": {"MAG": "2163576068", "DOI": "10.1111/MBE.12045"}, "url": "https://www.semanticscholar.org/paper/162b9358ed9ea3d0b2f521ed3470cccbac2c99ba", "title": "Agent\u2010Based Modeling of Growth Processes", "abstract": "Growth processes abound in nature, and are frequently the target of modeling exercises in the sciences. In this article we illustrate an agent-based approach to modeling, in the case of a single example from the social sciences: bullying. Language: en", "venue": "", "year": 2014, "referenceCount": 320, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "36557692", "name": "R. Abraham"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "236e2357c68f8e45317ebb1699744fbb4634bb95", "externalIds": {"DBLP": "journals/annals/Rankin14", "MAG": "2024955291", "DOI": "10.1109/MAHC.2014.31"}, "url": "https://www.semanticscholar.org/paper/236e2357c68f8e45317ebb1699744fbb4634bb95", "title": "Toward a History of Social Computing: Children, Classrooms, Campuses, and Communities", "abstract": "Modern computing involves not only computation but also communication among many people, along with their cultural norms, values, and expectations. Yet, historians have only begun to address how people made computing ubiquitous. The history of computing should include the myriad human interactions that have shaped and supported our digital, networked world.", "venue": "IEEE Ann. Hist. Comput.", "year": 2014, "referenceCount": 18, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "31134820", "name": "Joy Rankin"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "260cd9a4e3642433a1c170909c27ef10977645b3", "externalIds": {"DBLP": "journals/annals/HaighPR14", "MAG": "1965832452", "DOI": "10.1109/MAHC.2013.56"}, "url": "https://www.semanticscholar.org/paper/260cd9a4e3642433a1c170909c27ef10977645b3", "title": "Reconsidering the Stored-Program Concept", "abstract": "The first in a three-part series in IEEE Annals, this article gives a historical explanation of the endemic confusion surrounding the stored-program concept. The authors suggest the adoption of more precisely defined alternatives to capture specific aspects of the new approach to computing associated with the 1945 work of von Neumann and his collaborators. The second article, \"Engineering--The Miracle of the ENIAC: Implementing the Modern Code Paradigm,\"' examines the conversion of ENIAC to use the modern code paradigm identified in this article. The third, \"Los Alamos Bets on ENIAC: Nuclear Monte Carlo Simulations, 1947-1948,\"' examines in detail the first program written in the new paradigm to be executed.", "venue": "IEEE Annals of the History of Computing", "year": 2014, "referenceCount": 53, "citationCount": 22, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1714013", "name": "T. Haigh"}, {"authorId": "2069947446", "name": "M. Priestley"}, {"authorId": "2908526", "name": "C. Rope"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "291ee3637b2ba5208c4f10c0ced60cf484b21e81", "externalIds": {"MAG": "2409196108", "DOI": "10.5860/choice.190147"}, "url": "https://www.semanticscholar.org/paper/291ee3637b2ba5208c4f10c0ced60cf484b21e81", "title": "The Computing Universe: A Journey through a Revolution", "abstract": "Computers now impact almost every aspect of our lives, from our social interactions to the safety and performance of our cars. How did this happen in such a short time? And this is just the beginning. . . . In this book, Tony Hey and Gyuri Ppay lead us on a journey from the early days of computers in the 1930s to the cutting-edge research of the present day that will shape computing in the coming decades. Along the way, they explain the ideas behind hardware, software, algorithms, Moore's Law, the birth of the personal computer, the Internet and the Web, the Turing Test, Jeopardy's Watson, World of Warcraft, spyware, Google, Facebook, and quantum computing. This book also introduces the fascinating cast of dreamers and inventors who brought these great technological developments into every corner of the modern world. This exciting and accessible introduction will open up the universe of computing to anyone who has ever wondered where his or her smartphone came from.", "venue": "", "year": 2014, "referenceCount": 68, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "152181346", "name": "Tony (Anthony) John Grenville Hey"}, {"authorId": "66352813", "name": "G. P\u00e1pay"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3251161804bc812724bc52cef0b415372ae9b6f2", "externalIds": {"DBLP": "journals/annals/HaighPR14a", "MAG": "2038448793", "DOI": "10.1109/MAHC.2014.15"}, "url": "https://www.semanticscholar.org/paper/3251161804bc812724bc52cef0b415372ae9b6f2", "title": "Engineering \"The Miracle of the ENIAC\": Implementing the Modern Code Paradigm", "abstract": "In 1947 John von Neumann had the idea of converting ENIAC to the new style of programming first described in his celebrated \"First Draft of a Report on the EDVAC.\" By April 1948, Nick Metropolis, building on plans developed by Adele Goldstine and others, had implemented the conversion, making ENIAC the first computer to execute programs written in the new style, which we call the \"modern code paradigm.\" Treating this as a case of user-driven innovation, the authors document the conversion process and compare capabilities of the reconstructed machine to those of the first modern computers. This article is the second in a three-part series. The first article, \"Reconsidering the Stored Program Concept\" (published in IEEE Annals, vol. 36, no. 1, 2014; http://doi.ieeecomputersociety.org/10.1109/MAHC.2013.56), examined the history of the aforesaid idea and proposed a set of more specific alternatives. The third, \"Los Alamos Bets on ENIAC: Nuclear Monte Carlo Simulations, 1947-1948\" (to appear in IEEE Annals, vol. 36, no. 3, 2014; http://doi.ieeecomputersociety.org/10.1109/MAHC.2013.56), will examine in detail the first program run on the machine after its conversion to the new programming method.", "venue": "IEEE Annals of the History of Computing", "year": 2014, "referenceCount": 52, "citationCount": 9, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1714013", "name": "T. Haigh"}, {"authorId": "2069947446", "name": "M. Priestley"}, {"authorId": "2908526", "name": "C. Rope"}]}}, {"contexts": ["This is the breakthrough of parsing the mathematical expression and the development of FORTRAN language [9] Another achievement in communication was the development of the cell phone in 1960 and the development of the Satellite communication systems [10].", "available in 1951 is Univac 1 [9]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "37f827d6378f22289fcde7c9b5abd05f8f21ad15", "externalIds": {"MAG": "2183030699"}, "url": "https://www.semanticscholar.org/paper/37f827d6378f22289fcde7c9b5abd05f8f21ad15", "title": "Cyber Space: Cyber War or Cyber Space Peace Treaties", "abstract": "In this research paper the author investigates the major significant development of the cyber space and related technologies. Main security threats to cyber space security are also discussed. Significant amendments to the security of cyber space are highlighted. The national necessary preparation for cyber war is demonstrated. Also the advantages of having cyber space peace treaties over the cyber war option are discussed.", "venue": "", "year": 2014, "referenceCount": 10, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "1403973273", "name": "Hasan L. Al-Saedy"}]}}, {"contexts": ["ENIAC and Colossus managed to partially overcome this problem by avoiding repeated power cycling which was the main cause of vacuum tube failure at that time [13]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "3ea098a23338417d7b6803a65a6f8d5e12503c8f", "externalIds": {"MAG": "1606657528", "DOI": "10.3929/ETHZ-A-010376362"}, "url": "https://www.semanticscholar.org/paper/3ea098a23338417d7b6803a65a6f8d5e12503c8f", "title": "Liquid cooling of microprocessors - energy efficiency optimisation with waste heat recovery and passive hotspot targeting", "abstract": "vii Zusammenfassung ix Acknowledgments xi", "venue": "", "year": 2014, "referenceCount": 138, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "50312453", "name": "C. Sharma"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5810423124c04ceef2998a4b20e750bd2b102138", "externalIds": {"MAG": "2286788814", "DOI": "10.1007/978-1-4614-9155-2_28"}, "url": "https://www.semanticscholar.org/paper/5810423124c04ceef2998a4b20e750bd2b102138", "title": "History of Tools and Technologies in Mathematics Education", "abstract": "Although \u201ctechnology\u201d in mathematics education is often used exclusively to refer to electronic devices, it is important to recognize the longer and more varied history of tools in this field. We classify technology into two primary groups. General-purpose tools, of wide social importance, include the textbook, the blackboard, and the computer. Specialized technologies, sometimes found only in mathematics classrooms, include the slide rule, the protractor, and the cube root block. It is hoped that brief histories of these and other tools, although focused largely on the United States, will be usefully provocative for those with more global perspectives.", "venue": "", "year": 2014, "referenceCount": 45, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2087063362", "name": "David L. Roberts"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "6a0a5189a85a676af15a93d7328c2528e45955ae", "externalIds": {"MAG": "2026252290", "DOI": "10.1093/es/kht095"}, "url": "https://www.semanticscholar.org/paper/6a0a5189a85a676af15a93d7328c2528e45955ae", "title": "When Knowledge Transfer Goes Global: How People and Organizations Learned About Information Technology, 1945\u20131970", "abstract": "This article argues that an information ecosystem emerged rapidly after World War II that made possible the movement of knowledge about computing and its uses around the world. Participants included engineers, scientists, government officials, business management, and users of the technology. Vendors, government agencies, the military, and professors participated regardless of such barriers as languages, cold war politics, or varying levels of national economic levels of prosperity.", "venue": "Enterprise & Society", "year": 2014, "referenceCount": 112, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "2932674", "name": "J. Cortada"}]}}, {"contexts": ["He attributes much of the credit for coaxing collectively acceptable behavior from individually problematic parts to Jack Rosenberg, an electrical engineer who was overlooked in [1] and [5].", "does a good job outlining von Neumann\u2019s conversion to an applied mathematician, but the books of historian William Aspray and Herman Goldstine, Assistant ECP Director, chronicle this phase of von Neumann\u2019s career even more thoroughly [1], [5]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "80201b10f4a52293f4ea389cadfd7ccc3b62a294", "externalIds": {"MAG": "2075146080", "DOI": "10.1090/NOTI1142"}, "url": "https://www.semanticscholar.org/paper/80201b10f4a52293f4ea389cadfd7ccc3b62a294", "title": "Turing's Cathedral: The Origins of the Digital Universe---A Book Review", "abstract": "In 1936, Alan Turing, sparked by an interest in Hilbert\u2019s Entscheidungsproblem, introduced the notion of the stored-program Universal Machine. Nine years later, John von Neumann recruited a team of engineers to design and build a concrete realization of Turing\u2019s machine at the Institute for Advanced Study in Princeton. Von Neumann\u2019s machine, also known as the IAS or Princeton machine and, popularly but incorrectly, as the MANIAC (Mathematical Analyzer, Numerical Integrator, and Computer), was neither the first all-purpose, electronic, digital computer nor the first such machine to employ stored programs. Nevertheless, the IAS machine played an important role in shaping the digital revolution. It was, to borrow a modern phrase, open source. Not only were the design details enthusiastically shared, but teams of engineers from other institutions, national laboratories, and even commercial concerns regularly visited the construction site during the machine\u2019s six years of assembly to benefit first-hand from a difficult learning curve. For those unable to call in person, progress reports were disseminated. As a result, the logical architecture of the IAS computer was widely reproduced, and derivative machines were built around the world. By March 1953, the IAS", "venue": "", "year": 2014, "referenceCount": 16, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "3373159", "name": "B. Blank"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "a8313af97e2692ff16ca1aa107e78475bb35c128", "externalIds": {"MAG": "609129845"}, "url": "https://www.semanticscholar.org/paper/a8313af97e2692ff16ca1aa107e78475bb35c128", "title": "Atanasoff\u2019s invention input and early computing state of knowledge", "abstract": "This article investigates the dynamic relationship between a single pursue of an invention and the general US supply of similar activities in early computing during the 1930-1946 period. The objective is to illustrate how an early scientific state of knowledge affects the efficiency with which an theoretical effort is transformed into an invention. In computing, a main challenge in the pre-industrial phase of invention concerns the lack of or scattered demand for such ground-breaking inventions. I present historiographical evidences of the early stage of US computing providing an improved understanding of the dynamics of the supply of invention. It involves solving the alignment between a single inventor\u2019s incentives to research with the suppliers of technology. This step conditions the constitution of a stock of technical knowledge, and its serendipitous but purposeful organisation.", "venue": "", "year": 2014, "referenceCount": 75, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "5328130", "name": "P. Rouchy"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "aade91c572ecc7754f5f350e9007f14c74e1c828", "externalIds": {"PubMedCentral": "3932236", "MAG": "1978378806", "DOI": "10.1155/2014/549398", "PubMed": "24688404"}, "url": "https://www.semanticscholar.org/paper/aade91c572ecc7754f5f350e9007f14c74e1c828", "title": "Experiments in Computing: A Survey", "abstract": "Experiments play a central role in science. The role of experiments in computing is, however, unclear. Questions about the relevance of experiments in computing attracted little attention until the 1980s. As the discipline then saw a push towards experimental computer science, a variety of technically, theoretically, and empirically oriented views on experiments emerged. As a consequence of those debates, today's computing fields use experiments and experiment terminology in a variety of ways. This paper analyzes experimentation debates in computing. It presents five ways in which debaters have conceptualized experiments in computing: feasibility experiment, trial experiment, field experiment, comparison experiment, and controlled experiment. This paper has three aims: to clarify experiment terminology in computing; to contribute to disciplinary self-understanding of computing; and, due to computing's centrality in other fields, to promote understanding of experiments in modern science in general.", "venue": "TheScientificWorldJournal", "year": 2014, "referenceCount": 148, "citationCount": 23, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "144523037", "name": "M. Tedre"}, {"authorId": "50092400", "name": "Nella Moisseinen"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "bf083a3e73dd8cc291c3da5e0e6666efb75ae9cf", "externalIds": {"MAG": "2076215654", "DOI": "10.1016/J.JVOLGEORES.2014.07.012"}, "url": "https://www.semanticscholar.org/paper/bf083a3e73dd8cc291c3da5e0e6666efb75ae9cf", "title": "Knowledge engineering in volcanology: Practical claims and general approach", "abstract": "Abstract Knowledge engineering, being a branch of artificial intelligence, offers a variety of methods for elicitation and structuring of knowledge in a given domain. Only a few of them (ontologies and semantic nets, event/probability trees, Bayesian belief networks and event bushes) are known to volcanologists. Meanwhile, the tasks faced by volcanology and the solutions found so far favor a much wider application of knowledge engineering, especially tools for handling dynamic knowledge. This raises some fundamental logical and mathematical problems and requires an organizational effort, but may strongly improve panel discussions, enhance decision support, optimize physical modeling and support scientific collaboration.", "venue": "", "year": 2014, "referenceCount": 50, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Geology"], "authors": [{"authorId": "1678693", "name": "C. Pshenichny"}]}}, {"contexts": ["Introduction Evaluating interactive systems has always been central to Human-Computer Interaction (HCI) in that it is one of the first skills we teach students and is a staple for many practitioners."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "1266a310c148dd3540ca6fb5062385352e9fc1bc", "externalIds": {"DBLP": "conf/chi/MacDonaldA13", "MAG": "1985616444", "DOI": "10.1145/2468356.2468714"}, "url": "https://www.semanticscholar.org/paper/1266a310c148dd3540ca6fb5062385352e9fc1bc", "title": "Changing perspectives on evaluation in HCI: past, present, and future", "abstract": "Evaluation has been a dominant theme in HCI for decades, but it is far from being a solved problem. As interactive systems and their uses change, the nature of evaluation must change as well. In this paper, we outline the challenges our community needs to address to develop adequate methods for evaluating systems in modern (and future) use contexts. We begin by tracing how evaluation efforts have been shaped by a continuous adaptation to technological and cultural changes and conclude by discussing important research directions that will shape evaluation's future.", "venue": "CHI Extended Abstracts", "year": 2013, "referenceCount": 87, "citationCount": 43, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "34374362", "name": "Craig M. MacDonald"}, {"authorId": "1984198", "name": "M. Atwood"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2fce1f1a21d9eaa9b06c01ed2e02c10fa0f9e795", "externalIds": {"MAG": "2028410876", "DOI": "10.1177/2277975213507836"}, "url": "https://www.semanticscholar.org/paper/2fce1f1a21d9eaa9b06c01ed2e02c10fa0f9e795", "title": "Chaotic Synchronization and the Global Economy", "abstract": "A brief introductory article on the role of chaotic synchronization in the context of complex economic systems. The basic framework developed by the late Richard Goodwin in his book, Chaotic Economic Dynamics, of 1990 has been extended to massively complex dynamical systems of chaotic elements. Recent experimental results and speculative applications to global economic systems are presented.1", "venue": "", "year": 2013, "referenceCount": 315, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "36557692", "name": "R. Abraham"}, {"authorId": "2160432", "name": "M. Nivala"}]}}, {"contexts": ["For the first time, the sci\u00adence of integrating comput\u00aders into our daily lives has offered \nus the prospect of flu\u00adidly bringing together many mediums in a cohesive way."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "356ee1a3e7a5727955845673ce7d1f3d4dab78ad", "externalIds": {"DBLP": "journals/crossroads/DePalmaR13", "MAG": "1825439257", "DOI": "10.1145/2460436.2460439"}, "url": "https://www.semanticscholar.org/paper/356ee1a3e7a5727955845673ce7d1f3d4dab78ad", "title": "Creativity + Computer Science", "abstract": "Computer science only became established as a field in the 1950s, growing out of theoretical and practical research begun in the previous two decades. The field has exhibited immense creativity, ranging from innovative hardware such as the early mainframes to software breakthroughs such as programming languages and the Internet. Martin Gardner worried that \"it would be a sad day if human beings, adjusting to the Computer Revolution, became so intellectually lazy that they lost their power of creative thinking\" (Gardner, 1978, p. vi-viii). On the contrary, computers and the theory of computation have provided great opportunities for creative work. This chapter examines several key aspects of creativity in computer science, beginning with the question of how problems arise in computer science. We then discuss the use of analogies in solving key problems in the history of computer science. Our discussion in these sections is based on historical examples, but the following sections discuss the nature of creativity using information from a contemporary source, a set of interviews with practicing computer scientists collected by the Association of Computing Machinery's on-line student magazine, Crossroads. We then provide a general comparison of creativity in computer science and in the natural sciences. Computer science is closely related to both mathematics and engineering. It resembles engineering in that it is often concerned with building machines and making design decisions about complex interactive systems. Brian K. Reid wrote: \"Computer science is the first engineering discipline ever in which the complexity of the objects created is limited by the skill of the creator and not limited by the strength of the raw materials\" (Frenkel, 1987, p. 823). Like engineers, computer scientists draw on a collection of techniques to construct a solution to a particular problem, with the creativity consisting in development of new techniques. For example, during the creation of the first large-scale electronic computer, Eckert and Mauchly solved numerous engineering problems, resulting in solutions which became important contributions to computer science (Goldstine, 1972). Computer science also has a strong mathematical component. An early example is Alan Turing's invention of an abstract, theoretical computing machine in 1935 to solve David Hilbert's decidability problem in the foundations of mathematics (Hodges, 1983). The Turing machine has proven to be a very powerful tool for studying the theoretical limitations of computers. The theory of the class of NP-Complete problems, which appear to be computationally intractable, is another result in both \u2026", "venue": "XRDS", "year": 2013, "referenceCount": 41, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1908368", "name": "N. DePalma"}, {"authorId": "2054301238", "name": "David Robert"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "68b51fa16aa0c88f9ffbb4dbd63c52bc4797e29b", "externalIds": {"MAG": "2117742958", "DOI": "10.5120/13729-1526"}, "url": "https://www.semanticscholar.org/paper/68b51fa16aa0c88f9ffbb4dbd63c52bc4797e29b", "title": "Bandwidth Requirements of Large Scale Computing Systems \u2013 A Case Study", "abstract": "Complex scientific problems like weather forecasting, computational fluid and combustion dynamics, computational drug design etc. essentially require large scale computational resources in order to obtain solution to the equations governing them. These solutions can be obtained by developing large legacy codes and then executing them using parallel processing. The parallel processing computers generally demand huge bandwidth as they consist of large number of networked processing elements. One such legacy code VARSHA is a meteorological code used for weather forecasting developed at Flosolver, CSIR-NAL under the joint project from NMITLI (New Millennium Indian Technological Leadership Initiative) and MoES (Ministry of Earth Science). The parallel efficiency of VARSHA code using ethernet connectivity has been anything but satisfactory. This paper discusses the bandwidth utilisation of VARSHA code in its existing and modified forms in order to draw some important conclusions on the bandwidth requirements of the future stateof-art parallel computers used to execute such legacy codes. General Terms Bandwidth Scaling, Timing Analysis, VARSHA Code.", "venue": "", "year": 2013, "referenceCount": 28, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50724115", "name": "A. M. Khan"}, {"authorId": "143995847", "name": "Mohammed Mahfooz Sheikh"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "6d825938406335d1302e379c562441ebc9a63e4c", "externalIds": {"MAG": "585645701"}, "url": "https://www.semanticscholar.org/paper/6d825938406335d1302e379c562441ebc9a63e4c", "title": "Solutions manual to accompany An introduction to numerical methods and analysis, second edition", "abstract": "A solutions manual to accompany An Introduction to Numerical Methods and Analysis, Second Edition An Introduction to Numerical Methods and Analysis, Second Edition reflects the latest trends in the field, includes new material and revised exercises, and offers a unique emphasis on applications. The author clearly explains how to both construct and evaluate approximations for accuracy and performance, which are key skills in a variety of fields. A wide range of higher-level methods and solutions, including new topics such as the roots of polynomials, spectral collocation, finite element ideas, and Clenshaw-Curtis quadrature, are presented from an introductory perspective, and the Second Edition also features: Chapters and sections that begin with basic, elementary material followed by gradual coverage of more advanced material Exercises ranging from simple hand computations to challenging derivations and minor proofs to programming exercises Widespread exposure and utilization of MATLAB(R) An appendix that contains proofs of various theorems and other material", "venue": "", "year": 2013, "referenceCount": 73, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "69002421", "name": "James F. Epperson"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7fa81db67815069671361cfb8289721ad070e46b", "externalIds": {"MAG": "2168121399", "DBLP": "journals/annals/Aspray13", "DOI": "10.1109/MAHC.2013.46"}, "url": "https://www.semanticscholar.org/paper/7fa81db67815069671361cfb8289721ad070e46b", "title": "Computers, Information, and Everyday Life", "abstract": "One way to view the literature on the history of computing is to consider the function of the computing device that comes under historical investigation. However, new uses of computers supplement rather than supplant old uses of computers. For example, we still have scientific calculators, business database machines, and personal computers in the modern era. This article focuses on the computer as a personal machine as a device to gather information through email and Web searches, and hence the focus is on the period starting in the early 1990s and carrying forward to the present. Historians of computing are typically focused on information technology (including its uses), while information science scholars often focus instead on information and its organization, even if information technology plays an important supporting role. The particular concern in this case study is everyday information-seeking behavior, or everyday information for short.", "venue": "IEEE Ann. Hist. Comput.", "year": 2013, "referenceCount": 9, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1394496384", "name": "William Aspray"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "8f7f318b599c09ad4d8cc8659019bf4717e7a754", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/8f7f318b599c09ad4d8cc8659019bf4717e7a754", "title": "Floating-point Unit ( FPU ) Designs with Nano-electromechanical ( NEM ) Relays", "abstract": "Nano-electromechanical (NEM) relays are an alternative to CMOS transistors as the fabric of digital circuits. Circuits with NEM relays offer energy-efficiency benefits over CMOS since they have zero leakage power and are strategically designed to maintain throughput that is competitive with CMOS despite their slow actuation times. The floating-point unit (FPU) is the most complex arithmetic unit in a computational system. This thesis investigates if the energy-efficiency promise of NEM relays demonstrated before on smaller circuit blocks holds for complex computational structures such as the FPU. The energy, performance, and area trade-offs of FPU designs with NEM relays are examined and compared with that of state-of-the-art CMOS designs in an equivalent scaled process. Circuits that are critical path bottlenecks, including primarily the leading zero detector (LZD) and leading zero anticipator (LZA) blocks, are carefully identified and optimized for low latency and device count. We manage to drop the NEM relay FPU latency from 71 mechanical delays in a CMOS-style implementation to 16 mechanical delays in a NEM relay pass-logic style implementation. The FPU designed with NEM relays features 15x lower energy per operation compared to CMOS. Thesis Supervisor: Vladimir Stojanovi\u0107 Title: Associate Professor of Electrical Engineering", "venue": "", "year": 2013, "referenceCount": 36, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "153681041", "name": "S. Dutta"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b3c3156dbfc22e653157ef0102ebe8296db13e06", "externalIds": {"MAG": "2492400204", "DOI": "10.1142/9789814434867_0002"}, "url": "https://www.semanticscholar.org/paper/b3c3156dbfc22e653157ef0102ebe8296db13e06", "title": "The peregrinations of Poincar\u00e9", "abstract": "Dynamical Systems Theory in the spirit of Poincar\u00e9 (DST) has been in vogue since the controversial award of a prize by King Oscar II of Sweden and Norway, on his 60th birthday, January 21, 1889, to Poincar\u00e9. DST diffused Eastward (via Stockholm, Saint Petersburg, and Moscow) to Gorky in Russia, to Japan, and Westward (via Princeton, Mexico City, and Rio), to Berkeley in California. Now in the centennial year of the premature death of Poincar\u00e9, it is time for a review of these peregrinations. \u2217Mathematics Department, University of California, Santa Cruz, CA USA-95064. rha@ucsc.edu", "venue": "", "year": 2013, "referenceCount": 330, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "36557692", "name": "R. Abraham"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b6a6b972de2033b92f769b7210bc210c224c9e4c", "externalIds": {"MAG": "1840926214"}, "url": "https://www.semanticscholar.org/paper/b6a6b972de2033b92f769b7210bc210c224c9e4c", "title": "From Infology to Artificial Science", "abstract": "This paper studies the ideas of two actors in the Scandinavian field of Information Systems development. It analyzes the writings of Borje Langefors and Bo Dahlbom in the 1980s and 1990s, and focuses on their collaboration resulting in the publication of Langefors\u2019 Essays on Infology. Langefors was at that time honored as the founder of the information systems discipline in Scandinavia, but had also been criticized by several authors in the field. Dahlbom was a philosopher who had ventured into information systems development in the late 1980s. At the brink of the 1980s significant changes in both computer technology and Western society were evident. Computer technology saw a development from mainframe computing towards networked computing, as well as the advent of the home computer and the beginnings of the internet. Western societies changed significantly in the same period. I analyze the writings of Langefors using Paul N. Edwards concept of the cybernetic paradigm as a framework. Taking this as my starting point, I investigate whether the two writers can be said to operate within the cybernetic paradigm. Furthermore I interpret their theories along two axes. One seeing a shift from modernity to post-modernity, and one seeing a shift from humanism to post-humanism. I argue that both Langefors and Dahlbom can be understood as part of a cybernetic paradigm, although not univocally. Langefors can largely be interpreted as a product of Swedish post-war modernity, while Dahlbom related to a \u201cpostmodern condition\u201d in Lyotard\u2019s terms. As well as investigating the two authors as actors in the information systems development field, I investigate whether their theories also could be read as philosophy. I take Louis Althusser's notion of \u201cthe spontaneous philosophy of scientists\u201d as my starting point for this discussion. I argue that Langefors and Dahlbom can be understood as philosophers from two different perspectives. Langefors took his experiences as a practitioner and generalized them into philosophy, while Dahlbom wanted to bring philosophical reflection to the practice of systems development. Finally, I ask what motivated Dahlbom and Langefors, two very different theorists with very different backgrounds, to collaborate. My findings indicate that Dahlbom was partly motivated by his intention of developing a \u201cnew informatics\u201d in Sweden, and saw Langefors as an inspiration for this project. Both of the authors were motivated by seeing common adversaries in the information systems development field.", "venue": "", "year": 2013, "referenceCount": 48, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "113447586", "name": "Sjur Einen S\u00e6vik"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "cbacb752d2103b8d2751ae8a9efd58d62af9d301", "externalIds": {"MAG": "184801443", "DOI": "10.1007/978-3-642-29503-4_12"}, "url": "https://www.semanticscholar.org/paper/cbacb752d2103b8d2751ae8a9efd58d62af9d301", "title": "Image Entropy for Discrete Dynamical Systems", "abstract": "In the 1950s, Menzel, Stein, and Ulam performed one of the earliest digital simulations of discrete dynamics in two-dimensions. In their work they created a robot mathematician to scan the results of simulations for chaotic attractors. Here their idea is extended, using the image entropy concept to scan a family of endomorphisms for explosive and catastrophic bifurcations.", "venue": "", "year": 2013, "referenceCount": 321, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "36557692", "name": "R. Abraham"}]}}, {"contexts": ["[10] Herman H.", "Fruto de acuciantes necesidades b\u00e9licas \u2014el c\u00e1lculo de trayectorias bal\u00edsticas\u2014, su dise\u00f1o e implementaci\u00f3n tuvieron lugar en el que, m\u00e1s de sesenta a\u00f1os despu\u00e9s, podemos considerar el hipocentro de la inform\u00e1tica tal como la conocemos hoy [10]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "e4217b1577e85f1e0ed8917af4ae35dc30d44424", "externalIds": {"MAG": "171694111"}, "url": "https://www.semanticscholar.org/paper/e4217b1577e85f1e0ed8917af4ae35dc30d44424", "title": "ENIAC: una m\u00e1quina y un tiempo por redescubrir", "abstract": "Aunque los computadores actuales son mas pequenos, \nbaratos y rapidos que los precedentes, muy a menudo \nel estudio minucioso de las viejas glorias de la historia \nde la informatica nos permite comprender mejor, \nno solo algunos de los aspectos de la tecnologia informatica \nde aquel momento e incluso de la actual, sino \ntambien rasgos socioculturales propios del entorno \ncientifico y humano de la epoca concreta en que estas \nvenerables maquinas se desarrollaron. \nEl ENIAC (Electronic Numerical Integrator And Computer), \nuno de los primeros computadores electronicos \nde la historia, nunca imitado y el unico disponible en \nEEUU entre 1946 y 1949, es un buen ejemplo de todo \nello. Y aun mas: analizar sus restos puede convertirse \nen una sorprendente manera de mirarnos en un espejo \ny descubrir, al mismo tiempo, los aspectos mas humanos \nde la informatica. En este trabajo pretendemos \nmostrar que el estudio del proceso de diseno, construccion \ny uso posterior de esta maquina de caracter experimental \nnos puede aportar, todavia, un gran abanico de \nconocimientos utiles y, consecuentemente, podria formar \nparte de los contenidos de asignaturas basicas tanto \nde programacion como de arquitectura y estructura \nde computadores.", "venue": "", "year": 2013, "referenceCount": 21, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Philosophy"], "authors": [{"authorId": "134880800", "name": "X. Molero"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "e7c602e9250d8eae294e0b82a774e9e41b66f9cf", "externalIds": {"DBLP": "conf/siggraph/Welker13", "MAG": "2150795161", "DOI": "10.1145/2503649.2503657"}, "url": "https://www.semanticscholar.org/paper/e7c602e9250d8eae294e0b82a774e9e41b66f9cf", "title": "Early history of french CG", "abstract": "This paper provides an historical summary of the emergence of computer graphics research and creation in France between 1970 and 1990, a period of innovation that transformed artistic practice and French visual media. The paper shows the role of these developments in the history of art, the evolution of digital technology, and the expansion of animation and visual effects in the film industry.", "venue": "SIGGRAPH Art Gallery", "year": 2013, "referenceCount": 99, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Art"], "authors": [{"authorId": "2071221619", "name": "C\u00e9cile Welker"}]}}, {"contexts": ["Exponential growth is a powerful thing: ENIAC had 17,468 vacuum tubes [1], while a single Intel Xeon E7-8870 has 2.", "[1] ENIAC serves as an example of the forefront of computing: big machines solving big problems."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "0067571d8270bb0326ff6f682c9d4ffbc5031d6d", "externalIds": {"MAG": "32977144"}, "url": "https://www.semanticscholar.org/paper/0067571d8270bb0326ff6f682c9d4ffbc5031d6d", "title": "Power, interconnect, and reliability techniques for large scale integrated circuits", "abstract": "Historically, consumer computing products have moved to increasingly smaller form factors, from the personal computer, to the laptop, and now to devices such as smart phones and tablets. These products have a high amount of visibility, but in the background are datacenters solving problems beyond what can be solved by consumer devices. One of the first computers, ENIAC, was a massive machine that weighed 30 tons and occupied a full room to calculate artillery firing tables. Now it is possible to build a much more powerful system in a cubic millimeter form factor, but we still build warehouse-size systems, such as the \"K computer\", to solve complex problems like global weather simulation. \nAlthough advances in computing technology have greatly improved system capabilities and form factors, they have introduced problems in heat dissipation, reliability, and yield. Large scale systems are greatly affected by this, where their power usage is measured in megawatts, their reliability goal is running a mere 30 hours without system failure, and their processor count numbers in the hundreds of thousands. \nThis thesis addresses these issues on four fronts. First, 3D-stacking technology coupled with near-threshold computing (NTC) is used to address heat dissipation. A 3D-stacked NTC system, Centip3De, is presented as a demonstration of this strategy, with a 75x decrease in processor power and a 5.1x improvement in energy efficiency. Next, system yield is addressed using a demonstrated in-situ performance monitoring technique, Safety Razor, which uses a novel time-to-digital converter with sub-picosecond calibration accuracy. Third, interconnect and system reliability is addressed with a failure tolerant interconnect fabric, Vicis, which disables faulty components to maintain reliability, tolerating fault rates of over 1 in 2,000 gates. Finally, stochastic computing is proposed as an error-tolerant form of computation for advanced VLSI processes. An example application of an image sensor array with built-in edge detection is investigated, with a power savings improvement of over 50% compared to a conventional approach.", "venue": "", "year": 2012, "referenceCount": 123, "citationCount": 1, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "1687117", "name": "D. Blaauw"}, {"authorId": "145842362", "name": "D. Sylvester"}, {"authorId": "153654955", "name": "D. Fick"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "34445cea16fc42a80ccf8dd08d9972310ed7e026", "externalIds": {"MAG": "610983245"}, "url": "https://www.semanticscholar.org/paper/34445cea16fc42a80ccf8dd08d9972310ed7e026", "title": "Decoding Organization: Bletchley Park, Codebreaking and Organization Studies", "abstract": "Introduction: organization studies, history and Bletchley Park Part I. Decoding Structures: 1. The making of Bletchley Park 2. The making of signals intelligence at Bletchley Park Part II. Decoding Cultures: 3. Pillars of culture at Bletchley Park 4. Splinters of culture at Bletchley Park Part III. Decoding Work: 5. Making Bletchley Park work 6. Understanding Bletchley Park's work Conclusion: reviving organization studies.", "venue": "", "year": 2012, "referenceCount": 202, "citationCount": 17, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "144189100", "name": "C. Grey"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "34b40634787006ddfe589eebf0b01e3e0ef29d77", "externalIds": {"MAG": "2184992281"}, "url": "https://www.semanticscholar.org/paper/34b40634787006ddfe589eebf0b01e3e0ef29d77", "title": "Symposium on the History and Philosophy of Programming", "abstract": "Today one can say that programming has not only osmotically infused scientific and artistic research alike, but also that those new contexts elucidate what it may mean to be an algorithm. This talk will focus on the \u2018impatient practices\u2019 of experimental programming, which can never wait till the end, and for which it is essential that the modification of the program in some way integrates with its unfolding in time. A contemporary example is live coding, which performs programming (usually of sound and visuals) as a form of improvisation. Early in the history of computer languages, there was already a need felt for reprogramming processes at runtime. Nevertheless, this idea was of limited influence, maybe because, with increasing computational power, the fascination with interactive programs eclipsed the desire for interactive programming. This may not be an accidental omission, its reasons may also lie in a rather fundamental difficulty, on which we will focus here. In itself, the situation is almost obvious: not every part of the program-as-description has an equivalent in the program-as-process. Despite each computational process having a dynamic nature, an integration of programming into the program itself must in principle remain incomplete. As a result, a programmer is forced to oscillate between mutually exclusive perspectives. Arguably, this oscillation reveals a specific internal contradiction within algorithms, a necessary obstacle to semantic transparency. By calling this obstacle algorithmic complementarity, I intend to open it up for a discussion in a broader conceptual context, linking it with corresponding ideas from philosophy and physics. Here a few words about this terminology. Complementarity has been an influential idea in conceptualising the relation between the object of investigation, as opposed to the epistemic apparatus and the history of practice. Originating in the psychology of William James, where it referred to a subjective split of mutually exclusive observations, Niels Bohr used it to denote the existence of incommensurable observables of a quantum system (position vs. momentum, time vs. energy). Independent of the particular answer Bohr gave, complementarity raises the question of whether such a coexistence is induced by the experimental system or already present in the subject matter observed. Accordingly, in the case of programs, we may ask whether this obstacle is essential to their nature or whether it is a mere artefact of a specific formalisation. Algorithms, arguably situated between technical method and mathematical object, make an interesting candidate for a reconsideration of this discourse. The existence of an obstacle to semantic transparency within algorithms and their respective programs need not mean a relative impoverishment of computation. Conversely, prediction is the wager and 1 Institute for Music and Media, Dusseldorf, email: julian.rohrhuber@musikundmedien.net vital tension in every experimental system, as well as in interactive programming. After the conceptual discussion, I will try to exemplify this claim by introducing a few examples in the recent history of live coding. Again and again surfacing in form of symptoms such as an impossibility of immediacy, I hope this practice will be conceivable in terms of having algorithmic complementarity as one of its driving forces. Two Approaches to one Task: A Historical Case Study of the Implementation and Deployment of two Software Packages for the Design of Light-Weight Structures in Architecture and Civil Engineering", "venue": "", "year": 2012, "referenceCount": 42, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "152134130", "name": "L. D. Mol"}, {"authorId": "2614511", "name": "G. Primiero"}]}}, {"contexts": ["This research was partly motivated by the problem of the reliability of U.S. Air Force missiles (Goldstine, 1972)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "3707f8a2ec0d6a6cf23bc4dc3dce4cf32a6f0040", "externalIds": {"MAG": "1541155823"}, "url": "https://www.semanticscholar.org/paper/3707f8a2ec0d6a6cf23bc4dc3dce4cf32a6f0040", "title": "Crosscutting Issues in International Transformation: Interactions and Innovations among People, Organizations, Processes, and Technology", "abstract": "This book is a compilation of papers presented at the International Transformation Conference in Stockholm, Swweden on June 2-3, 2009. The conference was hosted by the Swedish Defence Research Agency at their Division of Information Systems in Kista. The papers are organized according to the categories of culture, interagency, transformation initiatives, leadership, and adaptive organizations. This sequence was chosen to group papers with common themes so that readers could follow the logic and findings of each paper more easily. The book represents the views of the authors, most of whom are members of the Internatinal transformation Chairs Network that was founded in the United States in 2004 by retired Vice Admiral Arthyr K. Cebrowski, who then served as the Director of the U.S. Department of Deeefense Office of Force Transformation. Since that time, the organization had added members from the United Kingdom,Sweden, Australia, Singapore, and NATO. The mision of the International Chairs Network is to provide a forum ro challenge thinkingm leverage shared knowledge, and inform the debate about the international security implications of global transformation. The vision of the group is that the efforts of these types of activities will ultimately result in a group of national security leaders who are prepared for a future filled with complexity, chaos, and surprise. Publication of this book is one step in the process of reaching this goal. We hope that this book is valuable to you as you seek to transform your part of the world.", "venue": "", "year": 2012, "referenceCount": 282, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "144014327", "name": "D. Neal"}, {"authorId": "71239944", "name": "H. Friman"}, {"authorId": "70011632", "name": "R. Doughty"}, {"authorId": "48720368", "name": "Linton Wells"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4edc03bb0d3ff5d082c8f0ddd1b2554a18dddf0d", "externalIds": {"DBLP": "journals/annals/Shelburne12", "MAG": "2041822303", "DOI": "10.1109/MAHC.2011.61"}, "url": "https://www.semanticscholar.org/paper/4edc03bb0d3ff5d082c8f0ddd1b2554a18dddf0d", "title": "The ENIAC's 1949 Determination of \u03c0", "abstract": "In January 1950, George W. Reitwiesner published an article describing the first use of a computer, the ENIAC, to calculate the decimal expansion of \u03c0. Starting with Reitwiesner's description of the calculation, the ENIAC's architecture, how it was programmed, and the mathematics used, this article examines why the calculation was undertaken, how it was done, and what was subsequently learned.", "venue": "IEEE Annals of the History of Computing", "year": 2012, "referenceCount": 38, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1860303", "name": "B. Shelburne"}]}}, {"contexts": ["Goldstine (1972) explains:\nThe purpose of the flow diagram is to give a picture of the motion of the control organ as it moves through the memory picking up and executing the instructions it finds there."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "4eeee7b8ed793a72145f50daa1c3ec1fa52ee077", "externalIds": {"MAG": "2171551825", "DOI": "10.1177/1046878110388239"}, "url": "https://www.semanticscholar.org/paper/4eeee7b8ed793a72145f50daa1c3ec1fa52ee077", "title": "Formalizing Game-play", "abstract": "Current computer conflict simulation games, or wargames, are opaque in the sense that most of the game mechanisms are not directly visible to the players and are frequently not described in user accessible documentation, have a transient lifetime that is mainly shaped by the evolution of graphics hardware and processor speed, and do not, in contrast with, for example, the well-known abstract board games CHESS and GO, have the technical prerequisites for critical intellectual discussion that the thought-intensive and knowledge-rewarding character of these games seems to warrant. The main reason for this state of affairs is that many of the mechanisms of the games, and in particular the details of how the game state changes over time, are directly expressed in computer code. This is purely a technical problem, and it has a straightforward solution, namely, treating this information as data by creating a formalism for describing not just the game map and playing pieces but also all the game rules including the \u201csequence of play.\u201d The article suggests such a formalism and shows a complete specification of a simple, but complicated enough for present purposes, \u201cintroductory\u201d board wargame. This formalism, with tools that support it, can provide an unambiguous authoritative definition of the rules, accessible by both human and computer players; would allow existing board wargames to be played on a computer, without any simplifications or sacrifices of rule details; and may allow construction of more advanced computer players, since a complete formal specification of the game rules is available as input to them.", "venue": "", "year": 2012, "referenceCount": 78, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2281637", "name": "Tomas By"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5110aec0cb90544b7c49392dc02a1e14812406c4", "externalIds": {"MAG": "2234265779", "DOI": "10.1007/978-1-4614-4684-2_17"}, "url": "https://www.semanticscholar.org/paper/5110aec0cb90544b7c49392dc02a1e14812406c4", "title": "From the Slate to the Web: Technology in the Mathematics Curriculum", "abstract": "The employment of physical tools to assist teaching and learning of mathematics did not begin with electronic devices, and has a much longer history than is often recognized. At times, technology has functioned as the inventive embodiment of mathematical ideas, progressing somewhat in step with the evolution of mathematics itself. At other times, technology has entered mathematics from outside, notably from commerce and science. This chapter surveys the evolution and curricular influence of technology in mathematics instruction in the Eastern and Western worlds from ancient times to the present day, with the primary focus being on the last 200 years. Past technology is categorized into tools for information storage, tools for information display, tools for demonstration, and tools for calculation. It is argued that today\u2019s computing technology offers teachers and students the potential to move beyond these categories, and to experience mathematics in ways that are different from traditional school mathematics curricula. A window is opened through which mathematics teaching and learning might enter into a new epistemological domain, where knowledge becomes both personal and communal, and in which connective and explorative mathematical knowledge becomes vastly more accessible.", "venue": "", "year": 2012, "referenceCount": 44, "citationCount": 9, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "2087063362", "name": "David L. Roberts"}, {"authorId": "145301827", "name": "A. Leung"}, {"authorId": "2053017798", "name": "Abigail Fregni Lins"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "6d267a388e98e38116e9c22fbd43b15e3b7318c6", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/6d267a388e98e38116e9c22fbd43b15e3b7318c6", "title": "On the Timing Analysis of Cluster based Communication Devices for Large Scale Computing Systems", "abstract": "Many parallel computing environments utilize cluster based architecture for large scale computing owing to the ease of their availability. As the cluster based approach may be used extensively, the interconnection mechanism plays a vital role in the performance of the system. The globally coupled class of problem is generally not amenable with the cluster based approach due to its substantial demand for communication across the architecture. In this paper we present a timing analysis of standard cluster based communication devices viz, Ethernet, InfiniBand and the custom designed Floswitch. KeywordsEthernet, InfiniBand, Floswitch, Cluster Computing.", "venue": "", "year": 2012, "referenceCount": 27, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "143995847", "name": "Mohammed Mahfooz Sheikh"}, {"authorId": "50724115", "name": "A. M. Khan"}]}}]}