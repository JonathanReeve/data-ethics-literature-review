{"offset": 0, "next": 100, "data": [{"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "65ada5eec961b70a495640ada0efb58a1e70d667", "externalIds": {"DOI": "10.1016/j.ijinfomgt.2021.102433"}, "url": "https://www.semanticscholar.org/paper/65ada5eec961b70a495640ada0efb58a1e70d667", "title": "Ethical framework for Artificial Intelligence and Digital technologies", "abstract": null, "venue": "International Journal of Information Management", "year": 2022, "referenceCount": 126, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "41205608", "name": "M. Ashok"}, {"authorId": "46287733", "name": "Rohit Madan"}, {"authorId": "1684227", "name": "Anton Joha"}, {"authorId": "2735257", "name": "U. Sivarajah"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b7aafe588905ce32611af772cebae5484640710d", "externalIds": {"DOI": "10.4018/978-1-7998-8467-5.ch015"}, "url": "https://www.semanticscholar.org/paper/b7aafe588905ce32611af772cebae5484640710d", "title": "Discussions on How to Best Prepare Students on the Ethics of Human-Machine Interactions at Work", "abstract": "This chapter analyzes the evolution of the new ways of working, especially in terms of algorithms and machine learning. Special attention is given to algorithmic management and its ethical concerns, as well as to practical examples of the application of algorithms in different sectors. Faculty discussions about how to best prepare students to deal with human-machine interactions at work are presented, with algorithmic management and accountability the discussion's central axis. In algorithmic management, there are distinct positions to analyze; one that favors innovation and efficiency and privileges dignified work and ethics. A brief proposal on introducing algorithmic ethics into the programs offered at a private business school in Mexico is included.", "venue": "Applied Ethics in a Digital World", "year": 2022, "referenceCount": 35, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1413345884", "name": "C. Montaudon-Tomas"}, {"authorId": "1413317846", "name": "I. N. Pinto-L\u00f3pez"}, {"authorId": "5588913", "name": "A. Amsler"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0003510ca25103ac605945c20e90586ee24de538", "externalIds": {"MAG": "3198908536", "DOI": "10.3390/LAWS10030070"}, "url": "https://www.semanticscholar.org/paper/0003510ca25103ac605945c20e90586ee24de538", "title": "Digital Transformation and Artificial Intelligence Applied to Business: Legal Regulations, Economic Impact and Perspective", "abstract": "Digital transformation can be defined as the integration of new technologies into all areas of a company. This technological integration will ultimately imply a need to transform traditional business models. Similarly, artificial intelligence has been one of the most disruptive technologies of recent decades, with a high potential impact on business and people. Cognitive approaches that simulate both human behavior and thinking are leading to advanced analytical models that help companies to boost sales and customer engagement, improve their operational efficiency, improve their services and, in short, generate new relevant information from data. These decision-making models are based on descriptive, predictive and prescriptive analytics. This necessitates the existence of a legal framework that regulates all digital changes with uniformity between countries and helps a proper digital transformation process under a clear regulation. On the other hand, it is essential that this digital disruption is not slowed down by the regulatory framework. This work will demonstrate that AI and digital transformation will be an intrinsic part of many applications and will therefore be universally deployed. However, this implementation will have to be done under common regulations and in line with the new reality.", "venue": "", "year": 2021, "referenceCount": 37, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "107725612", "name": "Ricardo Francisco Reier Forradellas"}, {"authorId": "2135150203", "name": "Luis Miguel Garay Gallastegui"}]}}, {"contexts": ["5 See e.g. Mittelstadt et al. 2016 who, when considering points of future research for algorithmic ethics, only focuses on the development of single concepts such as accountability, transparency, autonomy and privacy."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "00a8b288da140caaabe831d0bf25b117aa30e45d", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/00a8b288da140caaabe831d0bf25b117aa30e45d", "title": "Ethical guidelines for the use of artificial intelligence and value conflict challenges", "abstract": "The aim of this article is to articulate and critically discuss different answers to the following question: How should decision-makers deal with conflicts that arise when the values usually entailed in ethical guidelines \u2013 such as accuracy, privacy, nondiscrimination and transparency \u2013 for the use of Artificial Intelligence (e.g. algorithm-based sentencing) clash with one another? To begin with, I focus on clarifying some of the general advantages of using such guidelines in an ethical analysis of the use of AI. Some disadvantages will also be presented and critically discussed. Second, I will show that we need to distinguish between three kinds of conflict that can exist for ethical guidelines used in the moral assessment of AI. This section will be followed by a critical discussion of different answers to the question of how to handle what we shall call internal and external values conflicts. Finally, I will wrap up with a critical discussion of three different strategies to resolve what is called a \u2018genuine value conflict\u2019. These strategies are: the \u2018accepting the existence of irresolvable conflict\u2019 view, the ranking view, and value monism. This article defends the \u2018accepting the existence of irresolvable conflict\u2019 view. It also argues that even though the ranking view and value monism, from a merely theoretical (or philosophical) point of view, are better equipped to solve genuine value conflicts among values in ethical guidelines for artificial intelligence, this is not the case in real-life decision-making.", "venue": "", "year": 2021, "referenceCount": 32, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "14844880", "name": "T. Petersen"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "04f51d470a1b9b9cc9e2f7525c8840874041387f", "externalIds": {"MAG": "2981656102", "DBLP": "journals/corr/abs-1910-10241", "ArXiv": "1910.10241", "DOI": "10.3390/IOT2030021"}, "url": "https://www.semanticscholar.org/paper/04f51d470a1b9b9cc9e2f7525c8840874041387f", "title": "Achieving Ethical Algorithmic Behaviour in the Internet-of-Things: a Review", "abstract": "The Internet of Things is emerging as a vast, inter-connected space of devices and things surrounding people, many of which are increasingly capable of autonomous action, from automatically sending data to cloud servers for analysis, changing the behaviour of smart objects, to changing the physical environment. A wide range of ethical concerns has arisen in their usage and development in recent years. Such concerns are exacerbated by the increasing autonomy given to connected things. This paper reviews, via examples, the landscape of ethical issues, and some recent approaches to address these issues concerning connected things behaving autonomously as part of the Internet of Things. We consider ethical issues in relation to device operations and accompanying algorithms. Examples of concerns include unsecured consumer devices, data collection with health-related Internet of Things, hackable vehicles, behaviour of autonomous vehicles in dilemma situations, accountability with Internet of Things systems, algorithmic bias, uncontrolled cooperation among things, and automation affecting user choice and control. Current ideas towards addressing a range of ethical concerns are reviewed and compared, including programming ethical behaviour, white-box algorithms, black-box validation, algorithmic social contracts, enveloping IoT systems, and guidelines and code of ethics for IoT developers; a suggestion from the analysis is that a multi-pronged approach could be useful based on the context of operation and deployment.", "venue": "IoT", "year": 2019, "referenceCount": 152, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1704577", "name": "S. Loke"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "06a53bad1619d2e23f8f93bc7138c60f7d3af7da", "externalIds": {"DBLP": "journals/ais/AstromskePA21", "MAG": "3080706116", "DOI": "10.1007/s00146-020-01008-9"}, "url": "https://www.semanticscholar.org/paper/06a53bad1619d2e23f8f93bc7138c60f7d3af7da", "title": "Ethical and legal challenges of informed consent applying artificial intelligence in medical diagnostic consultations", "abstract": "This paper inquiries into the complex issue of informed consent applying artificial intelligence in medical diagnostic consultations. The aim is to expose the main ethical and legal concerns of the New Health phenomenon, powered by intelligent machines. To achieve this objective, the first part of the paper analyzes ethical aspects of the alleged right to explanation, privacy, and informed consent, applying artificial intelligence in medical diagnostic consultations. This analysis is followed by a legal analysis of the limits and requirements for the explainability of artificial intelligence. Followed by this analysis, recommendations for action are given in the concluding remarks of the paper.", "venue": "AI Soc.", "year": 2020, "referenceCount": 83, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Psychology"], "authors": [{"authorId": "2092240210", "name": "Kristina Astromske"}, {"authorId": "10026311", "name": "E. Peicius"}, {"authorId": "115081566", "name": "Paulius Astromskis"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0846ca4323615b9b9c39072d301318864473c104", "externalIds": {"MAG": "3137930360", "DOI": "10.1177/2378023121999581"}, "url": "https://www.semanticscholar.org/paper/0846ca4323615b9b9c39072d301318864473c104", "title": "Toward a Sociology of Artificial Intelligence: A Call for Research on Inequalities and Structural Change", "abstract": "This article outlines a research agenda for a sociology of artificial intelligence (AI). The authors review two areas in which sociological theories and methods have made significant contributions ...", "venue": "", "year": 2021, "referenceCount": 92, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "50711913", "name": "K. Joyce"}, {"authorId": "1399053359", "name": "Laurel Smith\u2010doerr"}, {"authorId": "47099210", "name": "Sharla Alegria"}, {"authorId": "35091291", "name": "S. Bell"}, {"authorId": "16202536", "name": "Taylor M. Cruz"}, {"authorId": "38636328", "name": "Steve G. Hoffman"}, {"authorId": "72273447", "name": "S. Noble"}, {"authorId": "101739970", "name": "Benjamin Shestakofsky"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0a9b721fb4500e422091782190ddcb5ea6d3685c", "externalIds": {"DBLP": "journals/pacmhci/DechantBSSM21", "DOI": "10.1145/3474675"}, "url": "https://www.semanticscholar.org/paper/0a9b721fb4500e422091782190ddcb5ea6d3685c", "title": "How Avatar Customization Affects Fear in a Game-based Digital Exposure Task for Social Anxiety", "abstract": "The treatment of social anxiety through digital exposure therapy is challenging due to the cognitive properties of social anxiety-individuals need to be fully engaged in the task and feel themselves represented in the social situation; however, avatar customization has been shown to increase both engagement and social presence. In this paper, we harness techniques used in commercial games, and investigate how customizing self-representation in a novel digital exposure task for social anxiety influences the experience of social threat. In an online experiment with 200 participants, participants either customized their avatar or were assigned a predefined avatar. Participants then controlled the avatar through a virtual shop, where they had to solve a math problem, while a simulated audience within the virtual world observed them and negatively judged their performance. Our findings show that we can stimulate the fear of evaluation by others in our task, that fear is driven primarily by trait social anxiety, and that this relationship is strengthened for people higher in trait social anxiety. We provide new insights into the effects of customization in a novel therapeutic context, and embed the discussion of avatar customization into related work in social anxiety and human-computer interaction. ?", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2021, "referenceCount": 225, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1990466", "name": "M. Dechant"}, {"authorId": "50518016", "name": "M. Birk"}, {"authorId": "2545799", "name": "Y. Shiban"}, {"authorId": "1750637", "name": "K. Schnell"}, {"authorId": "1788745", "name": "R. Mandryk"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "13508fc5268937ebf7eb80e58ebfb7e01eed3caa", "externalIds": {"DBLP": "journals/corr/abs-2007-15634", "ArXiv": "2007.15634", "PubMedCentral": "8331998", "MAG": "3046116192", "DOI": "10.1007/s41060-021-00265-1", "PubMed": "34368422"}, "url": "https://www.semanticscholar.org/paper/13508fc5268937ebf7eb80e58ebfb7e01eed3caa", "title": "On the nature and types of anomalies: a review of deviations in data", "abstract": "Anomalies are occurrences in a dataset that are in some way unusual and do not fit the general patterns. The concept of the anomaly is typically ill defined and perceived as vague and domain-dependent. Moreover, despite some 250 years of publications on the topic, no comprehensive and concrete overviews of the different types of anomalies have hitherto been published. By means of an extensive literature review this study therefore offers the first theoretically principled and domain-independent typology of data anomalies and presents a full overview of anomaly types and subtypes. To concretely define the concept of the anomaly and its different manifestations, the typology employs five dimensions: data type, cardinality of relationship, anomaly level, data structure, and data distribution. These fundamental and data-centric dimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of anomalies. The typology facilitates the evaluation of the functional capabilities of anomaly detection algorithms, contributes to explainable data science, and provides insights into relevant topics such as local versus global anomalies.", "venue": "Int. J. Data Sci. Anal.", "year": 2020, "referenceCount": 453, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics", "Medicine"], "authors": [{"authorId": "2408666", "name": "Ralph Foorthuis"}]}}, {"contexts": ["From the European Commission (AI HLEG, 2019) to IBM (McDade and Testman, 2019), and McKinsey (Silberg and Manyika, 2019), the so-called AI bias problem\u2014 generally associated with ethics and morality (Mittelstadt et al., 2016)\u2014is now one of the most frequently discussed topics."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "16e19fc59014b410151ef58cb35bbe9a2ec309aa", "externalIds": {"MAG": "3158349484", "DOI": "10.1177/20539517211013569"}, "url": "https://www.semanticscholar.org/paper/16e19fc59014b410151ef58cb35bbe9a2ec309aa", "title": "Assessing biases, relaxing moralism: On ground-truthing practices in machine learning design and application", "abstract": "This theoretical paper considers the morality of machine learning algorithms and systems in the light of the biases that ground their correctness. It begins by presenting biases not as a priori negative entities but as contingent external referents\u2014often gathered in benchmarked repositories called ground-truth datasets\u2014that define what needs to be learned and allow for performance measures. I then argue that ground-truth datasets and their concomitant practices\u2014that fundamentally involve establishing biases to enable learning procedures\u2014can be described by their respective morality, here defined as the more or less accounted experience of hesitation when faced with what pragmatist philosopher William James called \u201cgenuine options\u201d\u2014that is, choices to be made in the heat of the moment that engage different possible futures. I then stress three constitutive dimensions of this pragmatist morality, as far as ground-truthing practices are concerned: (I) the definition of the problem to be solved (problematization), (II) the identification of the data to be collected and set up (databasing), and (III) the qualification of the targets to be learned (labeling). I finally suggest that this three-dimensional conceptual space can be used to map machine learning algorithmic projects in terms of the morality of their respective and constitutive ground-truthing practices. Such techno-moral graphs may, in turn, serve as equipment for greater governance of machine learning algorithms and systems.", "venue": "Big Data & Society", "year": 2021, "referenceCount": 120, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2083652831", "name": "Florian Jaton"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "18b116d491fc62d4d6c472cfcece598d3f2e8438", "externalIds": {"DBLP": "conf/automotiveUI/Scott-SharoniFW21", "DOI": "10.1145/3473682.3477435"}, "url": "https://www.semanticscholar.org/paper/18b116d491fc62d4d6c472cfcece598d3f2e8438", "title": "To Customize or Not to Customize - Is That the Question?", "abstract": "As automated vehicles become more prevalent, designing interfaces that best fit all users, especially ones in minority populations, is a pressing but difficult goal. System-driven adaptation is a commonly used approach as it is easier and created by experts but, has innate flaws. Customization, on the other hand, allows users to consciously alter the interface to appear and operate in a manner most suited to their needs and wants. However, various components of the interface have different constraints, capabilities, and requirements with the amount of customization appropriate. In this workshop, we will dissect an expansive taxonomy for customization and develop a series of levels in order to get the full benefits from customization, which in turn can help engineers and designers in creating more user-centered systems.", "venue": "AutomotiveUI", "year": 2021, "referenceCount": 33, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1410450358", "name": "Sidney T. Scott-Sharoni"}, {"authorId": "134510569", "name": "Nadia Fereydooni"}, {"authorId": "3292444", "name": "B. Walker"}, {"authorId": "2572836", "name": "M. Jeon"}, {"authorId": "1766769", "name": "A. Riener"}, {"authorId": "2433027", "name": "Philipp Wintersberger"}]}}, {"contexts": ["\u2026this article aims to take sides in the philosophical debate that primarily focuses on the technical requirements of AI to ensure that these technologies are designed to protect and promote relevant ethical and societal values in healthcare (Mittelstadt et al. 2016; London 2019; Vayena and al.2018)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "19b6cf38ec85455c5242614c7563373a95cd579a", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/19b6cf38ec85455c5242614c7563373a95cd579a", "title": "Designing AI Nudging for Social Good: New Healthcare Skills for Digital Personal Assistants", "abstract": "Traditional medical practices and relationships are changing given the widespread adoption of AIdriven technologies across the various domains of health and healthcare. In many cases, these new technologies are not specific to the field of healthcare. Still, they are existent, ubiquitous, and commercially available systems upskilled to integrate these novel care practices. Given the widespread adoption, coupled with the dramatic changes in practices, new ethical and social issues emerge due to how these systems nudge users into making decisions and changing behaviours. This article discusses how these AI-driven systems pose particular ethical challenges with regards to nudging. To confront these issues, the value sensitive design (VSD) approach is adopted as a principled methodology that designers can adopt to design these systems to avoid harming and contribute to the social good. The AI for Social Good (AI4SG) factors are adopted as the norms constraining maleficence. In contrast, higher-order values specific to AI, such as those from the EU High-Level Expert Group on AI and the United Nations Sustainable Development Goals, are adopted as the values to be promoted as much as possible in design. The use case of Amazon Alexa's Healthcare Skills is used to illustrate this design approach. It provides an exemplar of how designers and engineers can begin to orientate their design programs of these technologies towards the social good.", "venue": "", "year": 2021, "referenceCount": 70, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2073594491", "name": "M. Capasso"}, {"authorId": "2119003619", "name": "Steven Umbrello"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "1a71d814f3c529e0e090b29605657c3a3535b0e8", "externalIds": {"MAG": "3153758591", "DOI": "10.1007/S00146-021-01180-6"}, "url": "https://www.semanticscholar.org/paper/1a71d814f3c529e0e090b29605657c3a3535b0e8", "title": "Political machines: a framework for studying politics in social machines", "abstract": "In the age of ubiquitous computing and artificially intelligent applications, social machines serves as a powerful framework for understanding and interpreting interactions in socio-algorithmic ecosystems. Although researchers have largely used it to analyze the interactions of individuals and algorithms, limited attempts have been made to investigate the politics in social machines. In this study, I claim that social machines are per se political machines, and introduce a five-point framework for classifying influence processes in socio-algorithmic ecosystems. By drawing from scholars from political theory, I use a notion of influence that functions as a meta-concept for connecting and comparing different conceptions of politics. In this way, I can associate multiple political aspects of social machines from a cybernetic perspective. I show that the framework efficiently categorizes dimensions of influence that shape interactions between individuals and algorithms. These categories are symbolic influence, political conduct, algorithmic influence, design, and regulatory influence. Using case studies, I describe how they interact with each other on online social networks and in algorithmic decision-making systems and illustrate how the framework is able to guide scientists in further research.", "venue": "", "year": 2021, "referenceCount": 202, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "20985588", "name": "O. Papakyriakopoulos"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "1caeb42a757e9521de11f6a46a7e2fbc7f1c446a", "externalIds": {"MAG": "3158794858", "DOI": "10.1002/9781119706519.CH8"}, "url": "https://www.semanticscholar.org/paper/1caeb42a757e9521de11f6a46a7e2fbc7f1c446a", "title": "Semi\u2010automated Journalism", "abstract": null, "venue": "", "year": 2021, "referenceCount": 42, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "2124109292", "name": "Jos\u00e9 Luis Rojas Torrijos"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "1e7ebf61cd0be98a9eb733a7e381832640559c01", "externalIds": {"DOI": "10.1007/978-3-030-75107-4_3"}, "url": "https://www.semanticscholar.org/paper/1e7ebf61cd0be98a9eb733a7e381832640559c01", "title": "Exploratory RegTech: Sandboxes Supporting Trust by Balancing Regulation of Algorithms with Automation of Regulations", "abstract": null, "venue": "Trust Models for Next-Generation Blockchain Ecosystems", "year": 2021, "referenceCount": 31, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "145158500", "name": "D. Kera"}]}}, {"contexts": ["A prominent ethical objection to the use of algorithms concerns the opaqueness of machine learning (see, for instance, Mittelstadt et al. 2016, Lepri et al. 2018)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "1efa8d0f87f4878003519c6670999e395f8d0b6d", "externalIds": {"DOI": "10.2139/ssrn.3857292"}, "url": "https://www.semanticscholar.org/paper/1efa8d0f87f4878003519c6670999e395f8d0b6d", "title": "People Prefer Moral Discretion to Procedurally Fair Algorithms: Algorithm Aversion Beyond Intransparency", "abstract": "We explore aversion to the use of algorithms in moral decision-making. So far, this aversion has been explained mainly by the fear of opaque decisions that are potentially biased. Using incentivized experiments, we study which role the desire for human discretion in moral decision-making plays. This seems justified in light of evidence suggesting that people might not doubt the quality of algorithmic decisions, but still reject them. In our first study, we found that people prefer humans with decision-making discretion to algorithms that rigidly apply exogenously given human-created fairness principles to specific cases. In the second study, we found that people do not prefer humans to algorithms because they appreciate flesh-and-blood decision-makers per se, but because they appreciate humans\u2019 freedom to transcend fairness principles at will. Our results contribute to a deeper understanding of algorithm aversion. They indicate that emphasizing the transparency of algorithms that clearly follow fairness principles might not be the only element for fostering societal algorithm acceptance and suggest reconsidering certain features of the decision-making process.", "venue": "", "year": 2021, "referenceCount": 41, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "116783805", "name": "J. Jauernig"}, {"authorId": "1696033469", "name": "Matthias W. Uhl"}, {"authorId": "3804302", "name": "Gari Walkowitz"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "201b3f0118c8501051dc9a1cf8a6f5558f990933", "externalIds": {"DBLP": "journals/polity/Guevara-GomezZC21", "DOI": "10.3233/IP-200299"}, "url": "https://www.semanticscholar.org/paper/201b3f0118c8501051dc9a1cf8a6f5558f990933", "title": "Feminist perspectives to artificial intelligence: Comparing the policy frames of the European Union and Spain", "abstract": "Artificial Intelligence (AI) is a disruptive technology that has gained interest among scholars, politicians, public servants, and citizens. In the debates on its advantages and risks, issues related to gender have arisen. In some cases, AI approaches depict a tool to promote gender equality, and in others, a contribution to perpetuating discrimination and biases. We develop a theoretical and analytical framework, combining the literature on technological frames and gender theory to better understand the gender perspective of the nature, strategy, and use of AI in two institutional contexts. Our research question is: What are the assumptions, expectations and knowledge of the European Union institutions and Spanish government on AI regarding gender? Methodologically, we conducted a document analysis of 23 official documents about AI issued by the European Union (EU) and Spain to understand how they frame the gender perspective in their discourses. According to our analysis, despite both the EU and Spain have developed gender-sensitive AI policy frames, doubts remain about the definitions of key terms and the practical implementation of their discourses.", "venue": "Inf. Polity", "year": 2021, "referenceCount": 38, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2031069337", "name": "Ariana Guevara-G\u00f3mez"}, {"authorId": "2119136599", "name": "Luc\u00eda O. de Z\u00e1rate-Alcarazo"}, {"authorId": "39249645", "name": "J. I. Criado"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "218ae7f456e15b9fec0032eec7c5dbff7a2bfbd0", "externalIds": {"DOI": "10.1016/j.techsoc.2021.101803"}, "url": "https://www.semanticscholar.org/paper/218ae7f456e15b9fec0032eec7c5dbff7a2bfbd0", "title": "How can \u2018orare et laborare\u2019 guide the person-technology relationship during the Fourth Industrial Revolution?", "abstract": null, "venue": "Technology in Society", "year": 2021, "referenceCount": 104, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2127674930", "name": "J. Fern\u00e1ndez- Fern\u00e1ndez"}, {"authorId": "1830413980", "name": "Javier Camacho Ib\u00e1\u00f1ez"}, {"authorId": "2137370454", "name": "Cristina D\u00edaz de la Cruz"}, {"authorId": "101854734", "name": "Bernardo Villaz\u00e1n Gil"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2289b56722250f56d512011d613b9aaa90eefd79", "externalIds": {"MAG": "3128884193", "DOI": "10.1007/978-3-658-30936-7_2"}, "url": "https://www.semanticscholar.org/paper/2289b56722250f56d512011d613b9aaa90eefd79", "title": "Zur Zul\u00e4ssigkeit automatisierter Entscheidungen im Einzelfall einschlie\u00dflich Profiling im Sinne des Art. 22 DSGVO \u2013 Praxisrelevanz und Wirksamkeit der Norm in Zeiten von Big Data und KI", "abstract": null, "venue": "", "year": 2021, "referenceCount": 31, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "123180622", "name": "Carsten Kunkel"}, {"authorId": "2090206210", "name": "Juliana Schoewe"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "235aeded8f93de49c4fab79325930781249b9679", "externalIds": {"DOI": "10.3390/laws10030070"}, "url": "https://www.semanticscholar.org/paper/235aeded8f93de49c4fab79325930781249b9679", "title": "Digital Transformation and Artificial Intelligence Applied to Business: Legal Regulations, Economic Impact and Perspective", "abstract": "Digital transformation can be defined as the integration of new technologies into all areas of a company. This technological integration will ultimately imply a need to transform traditional business models. Similarly, artificial intelligence has been one of the most disruptive technologies of recent decades, with a high potential impact on business and people. Cognitive approaches that simulate both human behavior and thinking are leading to advanced analytical models that help companies to boost sales and customer engagement, improve their operational efficiency, improve their services and, in short, generate new relevant information from data. These decision-making models are based on descriptive, predictive and prescriptive analytics. This necessitates the existence of a legal framework that regulates all digital changes with uniformity between countries and helps a proper digital transformation process under a clear regulation. On the other hand, it is essential that this digital disruption is not slowed down by the regulatory framework. This work will demonstrate that AI and digital transformation will be an intrinsic part of many applications and will therefore be universally deployed. However, this implementation will have to be done under common regulations and in line with the new reality.", "venue": "Laws", "year": 2021, "referenceCount": 53, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2124066329", "name": "Ricardo Francisco Reier Forradellas"}, {"authorId": "2138560980", "name": "Luis Miguel Garay Gallastegui"}]}}, {"contexts": ["\u2026term, algorithms have come to be perceived as a broader category of technology that can take (usually reliable) decisions based on more or less complex rules and using\n5\nTransparency International Anti-Corruption Helpdesk\nAlgorithmic transparency\navailable data (Mittelstadt et al 2016)."], "isInfluential": false, "intents": ["methodology"], "citingPaper": {"paperId": "247f95706b6efb99eb9a0cb28acc2d6c2cee6b56", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/247f95706b6efb99eb9a0cb28acc2d6c2cee6b56", "title": "Algorithmic transparency and accountability", "abstract": "The Anti-Corruption Helpdesk is operated by Transparency International and funded by the European Union. Computer algorithms are being deployed in ever more areas of our economic, political and social lives. The decisions these algorithms make have profound effects in sectors such as healthcare, education, employment, and banking. Their application in the anti-corruption field is also becoming increasingly evident, notably in the domain of anti-money laundering.", "venue": "", "year": 2021, "referenceCount": 71, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "89230683", "name": "Niklas Kossow"}, {"authorId": "2103570536", "name": "Svea Windwehr"}]}}, {"contexts": ["Mittelstadt et al. (2016) describe this as one of the ethical challenges with algorithmic decisions as it may result in \u2018inconclusive evidence leading to unjustified actions\u2019.", "understanding and conceptualizing it in new, unexpected ways, and triggering and motivating actions based on the insights it generates\u2019 (Mittelstadt et al., 2016).", "The fact that some algorithms modify their decision-rules during operation render the rationale of the algorithm obscure, resulting in some machine-learning algorithms often being referred to as \u2018black boxes\u2019 (Mittelstadt et al., 2016, p. 6).", "At face-value such a decision is no more than a substitute for work that has previously been undertaken by a human, but is arguably adhering to a different decisionmaking logic capable of considering much greater inputs (Mittelstadt et al., 2016).", "Algorithmic decision-making may also re-ontologize the world \u2018by understanding and conceptualizing it in new, unexpected ways, and triggering and motivating actions based on the insights it generates\u2019 (Mittelstadt et al., 2016)."], "isInfluential": true, "intents": ["background"], "citingPaper": {"paperId": "2600eb59d37c6b18d8594134cff6b5345aa8c079", "externalIds": {"DOI": "10.1080/14494035.2021.1929729"}, "url": "https://www.semanticscholar.org/paper/2600eb59d37c6b18d8594134cff6b5345aa8c079", "title": "What rules? Framing the governance of artificial agency", "abstract": "ABSTRACT Artificial Intelligence (AI) is not new, but recent years have seen a growing concern about the technology\u2019s political, economic and social impact, including debates about its governance. This paper describes how an analysis of the technology\u2019s governance should build on the understanding of AI as the creation of artificial agents, and that the challenge that governance seeks to address is best understood as one of material agency. It describes how relevant rules can be systematically analyzed across different applications by considering the fundamental properties of artificial agents. The paper concludes by describing how the framework can be applied for further governance studies, and as a means to bridge insights across social science and technical perspectives on AI.", "venue": "", "year": 2021, "referenceCount": 80, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "117286444", "name": "Carl Gahnberg"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "268f8a134c1fd9d076f8c1df124f59810fdf5273", "externalIds": {"PubMedCentral": "8516568", "DOI": "10.1007/s00146-021-01259-0", "PubMed": "34667374"}, "url": "https://www.semanticscholar.org/paper/268f8a134c1fd9d076f8c1df124f59810fdf5273", "title": "Speeding up to keep up: exploring the use of AI in the research process", "abstract": "There is a long history of the science of intelligent machines and its potential to provide scientific insights have been debated since the dawn of AI. In particular, there is renewed interest in the role of AI in research and research policy as an enabler of new methods, processes, management and evaluation which is still relatively under-explored. This empirical paper explores interviews with leading scholars on the potential impact of AI on research practice and culture through deductive, thematic analysis to show the issues affecting academics and universities today. Our interviewees identify positive and negative consequences for research and researchers with respect to collective and individual use. AI is perceived as helpful with respect to information gathering and other narrow tasks, and in support of impact and interdisciplinarity. However, using AI as a way of \u2018speeding up\u2014to keep up\u2019 with bureaucratic and metricised processes, may proliferate negative aspects of academic culture in that the expansion of AI in research should assist and not replace human creativity. Research into the future role of AI in the research process needs to go further to address these challenges, and ask fundamental questions about how AI might assist in providing new tools able to question the values and principles driving institutions and research processes. We argue that to do this an explicit movement of meta-research on the role of AI in research should consider the effects for research and researcher creativity. Anticipatory approaches and engagement of diverse and critical voices at policy level and across disciplines should also be considered.", "venue": "AI & society", "year": 2021, "referenceCount": 145, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "38137487", "name": "J. Chubb"}, {"authorId": "1714467", "name": "P. Cowling"}, {"authorId": "40319925", "name": "D. Reed"}]}}, {"contexts": ["[34, 44, 74, 98, 99])."], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "26e08b66d1a0756bec41692f8280e23f042b39a8", "externalIds": {"MAG": "3154527076", "DBLP": "journals/corr/abs-2001-08301", "ArXiv": "2001.08301", "DOI": "10.1145/3397481.3450694"}, "url": "https://www.semanticscholar.org/paper/26e08b66d1a0756bec41692f8280e23f042b39a8", "title": "How to Support Users in Understanding Intelligent Systems? Structuring the Discussion", "abstract": "The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human-computer interaction. Research in the field therefore highlights the need for transparency, scrutability, intelligibility, interpretability and explainability, among others. While all of these terms carry a vision of supporting users in understanding intelligent systems, the underlying notions and assumptions about users and their interaction with the system often remain unclear. We review the literature in HCI through the lens of implied user questions to synthesise a conceptual framework integrating user mindsets, user involvement, and knowledge outcomes to reveal, differentiate and classify current notions in prior work. This framework aims to resolve conceptual ambiguity in the field and enables researchers to clarify their assumptions and become aware of those made in prior work. We thus hope to advance and structure the dialogue in the HCI research community on supporting users in understanding intelligent systems.", "venue": "IUI", "year": 2020, "referenceCount": 173, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3396220", "name": "Malin Eiband"}, {"authorId": "1768653", "name": "Daniel Buschek"}, {"authorId": "46751563", "name": "H. Hu\u00dfmann"}]}}, {"contexts": [" how consent is obtained, and the data collection process needs to keep \u201cthe receipts\u201d as evidence of due process. To facilitate this, it is therefore necessary that dataset requirements be traceable [100], and in many cases requirements should be measurable. Non-boolean test outcomes are still useful in flagging concerns, and frequently this can lead to discussion and iteration on requirements. When r"], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "27ad3d92a9d02698ae10be1a86f1f6e52c8f0644", "externalIds": {"MAG": "3093779786", "DBLP": "journals/corr/abs-2010-13561", "ArXiv": "2010.13561", "DOI": "10.1145/3442188.3445918"}, "url": "https://www.semanticscholar.org/paper/27ad3d92a9d02698ae10be1a86f1f6e52c8f0644", "title": "Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure", "abstract": "Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.", "venue": "FAccT", "year": 2020, "referenceCount": 258, "citationCount": 30, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2083807", "name": "B. Hutchinson"}, {"authorId": "152735378", "name": "A. Smart"}, {"authorId": "40540250", "name": "A. Hanna"}, {"authorId": "40081727", "name": "Emily L. Denton"}, {"authorId": "2064880341", "name": "Christina Greer"}, {"authorId": "38325692", "name": "Oddur Kjartansson"}, {"authorId": "80940648", "name": "P. Barnes"}, {"authorId": "49501003", "name": "Margaret Mitchell"}]}}, {"contexts": ["These biases transfer to the output data, and an AI model will learn the data\u2019s prejudice (Mittelstadt et al. 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "2a1c78341ac1004ea9bc05b399ee5d313f9f75ff", "externalIds": {"PubMedCentral": "8519529", "DOI": "10.1080/17453674.2021.1918389", "PubMed": "33988081"}, "url": "https://www.semanticscholar.org/paper/2a1c78341ac1004ea9bc05b399ee5d313f9f75ff", "title": "Presenting artificial intelligence, deep learning, and machine learning studies to clinicians and healthcare stakeholders: an introductory reference with a guideline and a Clinical AI Research (CAIR) checklist proposal", "abstract": "Background and purpose \u2014 Artificial intelligence (AI), deep learning (DL), and machine learning (ML) have become common research fields in orthopedics and medicine in general. Engineers perform much of the work. While they gear the results towards healthcare professionals, the difference in competencies and goals creates challenges for collaboration and knowledge exchange. We aim to provide clinicians with a context and understanding of AI research by facilitating communication between creators, researchers, clinicians, and readers of medical AI and ML research. Methods and results \u2014 We present the common tasks, considerations, and pitfalls (both methodological and ethical) that clinicians will encounter in AI research. We discuss the following topics: labeling, missing data, training, testing, and overfitting. Common performance and outcome measures for various AI and ML tasks are presented, including accuracy, precision, recall, F1 score, Dice score, the area under the curve, and ROC curves. We also discuss ethical considerations in terms of privacy, fairness, autonomy, safety, responsibility, and liability regarding data collecting or sharing. Interpretation \u2014 We have developed guidelines for reporting medical AI research to clinicians in the run-up to a broader consensus process. The proposed guidelines consist of a Clinical Artificial Intelligence Research (CAIR) checklist and specific performance metrics guidelines to present and evaluate research using AI components. Researchers, engineers, clinicians, and other stakeholders can use these proposal guidelines and the CAIR checklist to read, present, and evaluate AI research geared towards a healthcare setting.", "venue": "Acta orthopaedica", "year": 2021, "referenceCount": 52, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "11807028", "name": "Jakub Olczak"}, {"authorId": "2332587", "name": "John Pavlopoulos"}, {"authorId": "2092037356", "name": "J. Prijs"}, {"authorId": "145915737", "name": "F. IJpma"}, {"authorId": "6525606", "name": "J. Doornberg"}, {"authorId": "72943957", "name": "C. Lundstr\u00f6m"}, {"authorId": "21210212", "name": "J. Hedlund"}, {"authorId": "2075401393", "name": "Max Gordon"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2b928cb21894a27aada6719a77a93d0223cc09c2", "externalIds": {"MAG": "3137084994", "DOI": "10.37417/RPD/VOL_3_2021_535"}, "url": "https://www.semanticscholar.org/paper/2b928cb21894a27aada6719a77a93d0223cc09c2", "title": "Decisiones automatizadas: problemas y soluciones jur\u00eddicas. M\u00e1s all\u00e1 de la protecci\u00f3n de datos", "abstract": "The growing implementation of algorithmic systems in all kinds of human activities is the result of their increasing computational capacity. These systems can process massive amounts of data and provide very accurate results that help decision-makers in both the public and private sector to classify humans and predict their actions. However, algorithms, and the actors that use them, are also increasingly producing significant harms to the fundamental rights of individuals and democratic principles and values. To date, informational privacy (data protection) regulatory frameworks, have been the main legal instruments tasked with protecting against the wide array of risks and harms caused by the automated processing of personal data. This paper maps the main hazards caused by algorithmic systems and aims to prove the shortcomings of the data protection framework in order to justify further regulatory intervention in the public and private use of automated systems. It also draws a series of brief proposals that, were they to be implemented, should help to overcome some of the ineffective aspects of the current regulatory framework when dealing with the risks and harms caused by algorithms.", "venue": "", "year": 2021, "referenceCount": 85, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2124085423", "name": "Alba Soriano Arnanz"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2bec71c72bca4eb42e0db6462b1c81d354b17236", "externalIds": {"DBLP": "journals/jamia/MorrisSLOCWTGHE21", "DOI": "10.1093/jamia/ocaa294", "PubMed": "33594410"}, "url": "https://www.semanticscholar.org/paper/2bec71c72bca4eb42e0db6462b1c81d354b17236", "title": "Enabling a learning healthcare system with automated computer protocols that produce replicable and personalized clinician actions", "abstract": "Clinical decision-making is based on knowledge, expertise, and authority, with clinicians approving almost every intervention-the starting point for delivery of \"All the right care, but only the right care,\" an unachieved healthcare quality improvement goal. Unaided clinicians suffer from human cognitive limitations and biases when decisions are based only on their training, expertise, and experience. Electronic health records (EHRs) could improve healthcare with robust decision-support tools that reduce unwarranted variation of clinician decisions and actions. Current EHRs, focused on results review, documentation, and accounting, are awkward, time-consuming, and contribute to clinician stress and burnout. Decision-support tools could reduce clinician burden and enable replicable clinician decisions and actions that personalize patient care. Most current clinical decision-support tools or aids lack detail and neither reduce burden nor enable replicable actions. Clinicians must provide subjective interpretation and missing logic, thus introducing personal biases and mindless, unwarranted, variation from evidence-based practice. Replicability occurs when different clinicians, with the same patient information and context, come to the same decision and action. We propose a feasible subset of therapeutic decision-support tools based on credible clinical outcome evidence: computer protocols leading to replicable clinician actions (eActions). eActions enable different clinicians to make consistent decisions and actions when faced with the same patient input data. eActions embrace good everyday decision-making informed by evidence, experience, EHR data, and individual patient status. eActions can reduce unwarranted variation, increase quality of clinical care and research, reduce EHR noise, and could enable a learning healthcare system.", "venue": "J. Am. Medical Informatics Assoc.", "year": 2021, "referenceCount": 204, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "8136618", "name": "A. Morris"}, {"authorId": "17055489", "name": "Brian Stagg"}, {"authorId": "5394601", "name": "M. Lanspa"}, {"authorId": "34723766", "name": "J. Orme"}, {"authorId": "2181639", "name": "T. Clemmer"}, {"authorId": "2244107", "name": "L. Weaver"}, {"authorId": "144264847", "name": "F. Thomas"}, {"authorId": "144059164", "name": "C. Grissom"}, {"authorId": "39235481", "name": "E. Hirshberg"}, {"authorId": "2785113", "name": "T. East"}, {"authorId": "144032343", "name": "C. Wallace"}, {"authorId": "2114084609", "name": "M. Young"}, {"authorId": "1690314", "name": "Dean F. Sittig"}, {"authorId": "28654730", "name": "A. Pesenti"}, {"authorId": "2676847", "name": "M. Bombino"}, {"authorId": "31551725", "name": "E. Beck"}, {"authorId": "1849904", "name": "K. Sward"}, {"authorId": "1713789", "name": "C. Weir"}, {"authorId": "144975264", "name": "S. Phansalkar"}, {"authorId": "2105497", "name": "G. Bernard"}, {"authorId": "49735309", "name": "B. Taylor Thompson"}, {"authorId": "34876443", "name": "R. Brower"}, {"authorId": "2802016", "name": "J. Truwit"}, {"authorId": "6628598", "name": "J. Steingrub"}, {"authorId": "4813558", "name": "R. Duncan Hite"}, {"authorId": "4996575", "name": "D. Willson"}, {"authorId": "34676609", "name": "J. Zimmerman"}, {"authorId": "4675369", "name": "V. Nadkarni"}, {"authorId": "2246093", "name": "A. Randolph"}, {"authorId": "144615442", "name": "M. Curley"}, {"authorId": "49867210", "name": "C. Newth"}, {"authorId": "145269096", "name": "J. Lacroix"}, {"authorId": "32221022", "name": "M. Agus"}, {"authorId": "113861576", "name": "Kang H. Lee"}, {"authorId": "8200421", "name": "B. deBoisblanc"}, {"authorId": "2051145400", "name": "R. Scott Evans"}, {"authorId": "36378690", "name": "D. Sorenson"}, {"authorId": "145463501", "name": "Anthony Wong"}, {"authorId": "3003411", "name": "Michael V. Boland"}, {"authorId": "4327841", "name": "D. Grainger"}, {"authorId": "5349552", "name": "W. Dere"}, {"authorId": "1759373", "name": "A. Crandall"}, {"authorId": "1740369", "name": "J. Facelli"}, {"authorId": "1705281", "name": "S. Huff"}, {"authorId": "2519295", "name": "P. Haug"}, {"authorId": "2196366", "name": "U. Pielmeier"}, {"authorId": "50354685", "name": "S. Rees"}, {"authorId": "30968816", "name": "D. Karbing"}, {"authorId": "1972776", "name": "S. Andreassen"}, {"authorId": "3836817", "name": "E. Fan"}, {"authorId": "5344402", "name": "R. Goldring"}, {"authorId": "6011160", "name": "K. Berger"}, {"authorId": "34218602", "name": "B. Oppenheimer"}, {"authorId": "146674548", "name": "E. Wesley Ely"}, {"authorId": "1826124", "name": "O. Gajic"}, {"authorId": "2069057", "name": "B. Pickering"}, {"authorId": "2488952", "name": "D. Schoenfeld"}, {"authorId": "31773420", "name": "I. Tocino"}, {"authorId": "6015821", "name": "R. Gonnering"}, {"authorId": "2161793", "name": "P. Pronovost"}, {"authorId": "6028935", "name": "L. Savitz"}, {"authorId": "38518365", "name": "D. Dreyfuss"}, {"authorId": "6205784", "name": "Arthur S Slutsky"}, {"authorId": "2908916", "name": "J. Crapo"}, {"authorId": "1834539", "name": "D. Angus"}, {"authorId": "2910412", "name": "M. Pinsky"}, {"authorId": "2594548", "name": "B. James"}, {"authorId": "2966141", "name": "D. Berwick"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2c8d617029ff0db04d9ee6649fcff8f5d5ab8e06", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/2c8d617029ff0db04d9ee6649fcff8f5d5ab8e06", "title": "Establishing a Case for Developing a Governance Framework for AI Regulations in the Gulf Cooperation Council Countries", "abstract": "Despite AI adoption in the Gulf Cooperation Council (GCC) countries being relatively recent, it has become an integral part in the Information Technology (IT) agenda of most of these countries. Therefore, this paper establishes a case for developing a common governance framework for AI regulations in the GCC countries. With this goal in mind, this paper examines the status of AI governance globally and in the GCC countries specifically. A framework for AI governance in the GCC countries is presented, which is based on a layered and modular approach towards AI governance.", "venue": "", "year": 2021, "referenceCount": 99, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2133488022", "name": "Abdullah Abdul Hamid Al-Barakati"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2d21ed235c49e1e204a2f7d5b2abd2d16bcd002c", "externalIds": {"MAG": "3097381406", "DBLP": "journals/scientometrics/Polonioli21", "DOI": "10.1007/s11192-020-03766-1"}, "url": "https://www.semanticscholar.org/paper/2d21ed235c49e1e204a2f7d5b2abd2d16bcd002c", "title": "The ethics of scientific recommender systems", "abstract": "Scientific recommender systems have become increasingly popular as a tool to overcome information overload, allowing researchers to access fresh and relevant content. However, this article presents an analysis of the most pressing ethical challenges posed by recommender systems in the context of scientific research. In particular, it is argued that scientific recommender systems may risk isolating scholars in information bubbles and insulating them from exposure to different viewpoints. Further, they also risk suffering from popularity biases which may lead to a winner-takes-all scenario and reinforce discrepancies in recognition received by eminent scientists and unknown researchers. The article concludes with recommendations for scientists, journals, and digital libraries to facilitate progress in the study of the ethics of scientific recommender systems.", "venue": "Scientometrics", "year": 2020, "referenceCount": 50, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3399230", "name": "A. Polonioli"}]}}, {"contexts": ["The use of data derived from digital sources is part of a growing discussion [44, 69, 96, 98]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "2edfa40cbe5e9843a2a7ff198ed0df59a6269865", "externalIds": {"DBLP": "conf/chi/DechantFM21", "DOI": "10.1145/3411764.3445238"}, "url": "https://www.semanticscholar.org/paper/2edfa40cbe5e9843a2a7ff198ed0df59a6269865", "title": "Assessing Social Anxiety Through Digital Biomarkers Embedded in a Gaming Task", "abstract": "Digital biomarkers of mental health issues offer many advantages, including timely identification for early intervention, ongoing assessment during treatment, and reducing barriers to assessment stemming from geography, age, fear, or disparities in access to systems of care. Embedding digital biomarkers into games may further increase the reach of digital assessment. In this study, we explore game-based digital biomarkers for social anxiety, based on interaction with a non-player character (NPC). We show that social anxiety affects a player's accuracy and their movement path in a gaming task involving an NPC. Further, we compared first versus third-person camera perspectives and the use of customized versus predefined avatars to explore the influence of common game interface factors on the expression of social anxiety through in-game movements. Our findings provide new insights about how game-based digital biomarkers can be effectively used for social anxiety, affording the benefits of early and ongoing digital assessment.", "venue": "CHI", "year": 2021, "referenceCount": 154, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1990466", "name": "M. Dechant"}, {"authorId": "1995636", "name": "Julian Frommel"}, {"authorId": "1788745", "name": "R. Mandryk"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2eef2f10cb5505bad49152c2174a2b9ec425aaf7", "externalIds": {"MAG": "3186991159", "DOI": "10.1590/1982-2553202150301"}, "url": "https://www.semanticscholar.org/paper/2eef2f10cb5505bad49152c2174a2b9ec425aaf7", "title": "\u00c9tica Algor\u00edtmica: quest\u00f5es e desafios \u00e9ticos do avan\u00e7o tecnol\u00f3gico da sociedade da informa\u00e7\u00e3o", "abstract": "Resumo A Quarta Revolu\u00e7\u00e3o Industrial inseriu na vida cotidiana a Intelig\u00eancia Artificial (IA) e seus algoritmos. Essa nova realidade tecnol\u00f3gica requer uma profunda reflex\u00e3o \u00e9tica e enseja a necessidade de pensar sobre uma \u00c9tica Algor\u00edtmica. Partindo de uma pesquisa bibliogr\u00e1fica, este artigo prop\u00f5e uma defini\u00e7\u00e3o do termo e desenvolve uma reflex\u00e3o sobre as principais quest\u00f5es suscitadas pela presen\u00e7a dos algoritmos nas media\u00e7\u00f5es da vida humana contempor\u00e2nea, referentes \u00e0 falibilidade, opacidade, vi\u00e9s, discrimina\u00e7\u00e3o, autonomia, privacidade e responsabilidade. Situando-a no dom\u00ednio de uma \u00e9tica aplicada, pondera-se sobre a conjun\u00e7\u00e3o das camadas humanas e computacionais na tomada de consci\u00eancia diante dos dilemas.", "venue": "Gal\u00e1xia (S\u00e3o Paulo)", "year": 2021, "referenceCount": 27, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "52007428", "name": "Regina Rossetti"}, {"authorId": "23181724", "name": "A. Angeluci"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2f73965d21eab177af4734704b3f31a24236c29c", "externalIds": {"DBLP": "journals/ais/RochelE21", "MAG": "3084815908", "DOI": "10.1007/S00146-020-01069-W"}, "url": "https://www.semanticscholar.org/paper/2f73965d21eab177af4734704b3f31a24236c29c", "title": "Getting into the engine room: a blueprint to investigate the shadowy steps of AI ethics", "abstract": "Enacting an AI system typically requires three iterative phases where AI engineers are in command: selection and preparation of the data, selection and configuration of algorithmic tools, and fine-tuning of the different parameters on the basis of intermediate results. Our main hypothesis is that these phases involve practices with ethical questions. This paper maps these ethical questions and proposes a way to address them in light of a neo-republican understanding of freedom, defined as absence of domination. We thereby identify different types of responsibility held by AI engineers and link them to concrete suggestions on how to improve professional practices. This paper contributes to the literature on AI and ethics by focusing on the work necessary to configure AI systems, thereby offering an input to better practices and an input for societal debates.", "venue": "AI Soc.", "year": 2020, "referenceCount": 69, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2775394", "name": "J. Rochel"}, {"authorId": "1771698", "name": "F. Ev\u00e9quoz"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "34b661074d98631141562939ceb3d2cdc74d1e2d", "externalIds": {"MAG": "3160695076", "DOI": "10.2139/ssrn.3772500"}, "url": "https://www.semanticscholar.org/paper/34b661074d98631141562939ceb3d2cdc74d1e2d", "title": "AI Impact Assessment: A Policy Prototyping Experiment", "abstract": "This report presents the outcomes of the Open Loop policy prototyping program on Automated Decision Impact Assessment (ADIA) in Europe. Open Loop (www.openloop.org) is a global program that connects policymakers and technology companies to help develop effective and evidence-based policies around AI and other emerging technologies. \n \nIn this particular case, Open Loop partnered with 10 European AI companies to co-create an ADIA framework (policy prototype) that those companies could test by applying it to their own \nAI applications. The policy prototype was structured into two parts: the prototype law, which was drafted as legal text, and the prototype guidance, which was drafted as a playbook. The latter provided participants with additional guidance on procedural and substantive aspects of performing the ADIA through: \n \n- A step-by-step risk assessment methodology; \n \n- An overview of values often associated with AI applications; \n \n- A taxonomy of harms; \n \n- Examples of mitigating measures. \n \nThe prototype was tested against the following three \ncriteria: 1) policy understanding; 2) policy effectiveness; 3) policy costs.The goal was to derive evidence-based recommendations relevant to ongoing policy debates around the future of AI regulation. \n \nBased on the results of the prototyping exercise and the feedback on the prototype law and playbook, the report advises lawmakers formulating requirements for AI risk assessments to take the following recommendations into account: \n \n- Focus on procedure instead of prescription as a way to determine high-risk AI applications; \n \n- Leverage a procedural risk assessment approach to determine what is the right set of regulatory requirements that apply to organisations deploying AI applications; \n \n- Provide specific and detailed guidance on how to implement an ADIA process, and release it alongside the law; \n \n- Be as specific as possible in the definition of risks within regulatory scope; \n \n- Improve documentation of risk assessment and decision-making processes by including justifications for mitigation choices; \n \n- Develop a sound taxonomy of the different AI actors involved in risk assessment; \n \n- Specify, as much as possible, the set of values that may be impacted by AI/ADM and provide guidance on how they may be in tension with one another; \n \n- Don\u2019t reinvent the wheel; combine new processes with established ones, improving the overall approach.", "venue": "", "year": 2021, "referenceCount": 47, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "34586862", "name": "Norberto Nuno Gomes de Andrade"}, {"authorId": "148298552", "name": "V. Kontschieder"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "36f6c4aa6552aafe085dfe4ad05f11ff00b73d0a", "externalIds": {"MAG": "3185501258", "DOI": "10.4018/978-1-7998-7258-0.ch023"}, "url": "https://www.semanticscholar.org/paper/36f6c4aa6552aafe085dfe4ad05f11ff00b73d0a", "title": "Libraries and Artificial Intelligence", "abstract": "Libraries are increasingly entering the digital age, and demands on them to offer more digital services are widening, with user expectations of \u201cremote or distant access,\u201d \u201cdistant learning,\u201d and the use of other modern internet technologies. To this end, libraries must accelerate their use of technologies like AI, \u201cdata mining,\u201d \u201cmachine-readable data,\u201d \u201cmachine-generated classification,\u201d \u201csemantic ontologies,\u201d and internet accessible catalogs and content because their aim should always be user benefit, user convenience, and user satisfaction. In this chapter, the author examines ways in which technologies and libraries are trying to fulfill their modern role and expectations of the modern user. Additionally, the author will examine how to strengthen data ethics in those particular fields of library use that most endanger the user's intellectual freedom on one side and his right to privacy on the other. One of the essential roles of modern libraries, in their new \u201cinformational\u201d identity, will be as \u201cguardians of data ethics and intellectual freedom.\u201d", "venue": "Advances in Library and Information Science", "year": 2021, "referenceCount": 24, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2129749418", "name": "Mojca Rupar Korosec"}]}}, {"contexts": ["However, translating ethical guidelines and policies into regulatory mechanisms and standards remains a challenge [10]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "3a244dc36e9febb146552bbccea5f982e66aa559", "externalIds": {"DBLP": "journals/corr/abs-2106-14975", "ArXiv": "2106.14975"}, "url": "https://www.semanticscholar.org/paper/3a244dc36e9febb146552bbccea5f982e66aa559", "title": "Design Considerations for Data Daemons: Co-creating Design Futures to Explore Ethical Personal Data Management", "abstract": "Mobile applications and online service providers track our virtual and physical behaviour more actively and with a broader scope than ever before. This has given rise to growing concerns about ethical personal data management. Even though regulation and awareness around data ethics are increasing, end-users are seldom engaged when defining and designing what a future with ethical personal data management should look like. We explore a participatory process that uses design futures, the Future workshop method and design fictions to envision ethical personal data management with end-users and designers. To engage participants effectively, we needed to bridge their differential expertise and make the abstract concepts of data and ethics tangible. By concretely presenting personal data management and control as fictitious entities called Data Daemons, we created a shared understanding of these abstract concepts, and empowered non-expert end-users and designers to become actively engaged in the design process.", "venue": "ArXiv", "year": 2021, "referenceCount": 22, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "122929064", "name": "Wiebke Toussaint"}, {"authorId": "2116609820", "name": "Alejandra Gomez Ortega"}, {"authorId": "2447459", "name": "J. Vroon"}, {"authorId": "20253442", "name": "Julian Harty"}, {"authorId": "2730434", "name": "G\u00fcrkan Solmaz"}, {"authorId": "144429598", "name": "O. Kudina"}, {"authorId": "2264026", "name": "Ella Peltonen"}, {"authorId": "35050013", "name": "J. Bourgeois"}, {"authorId": "1934437", "name": "Aaron Yi Ding"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3a7594e2f940e2b9f6531d5d26ae332b94a3d313", "externalIds": {"MAG": "3148471254", "DOI": "10.1007/S00146-021-01192-2"}, "url": "https://www.semanticscholar.org/paper/3a7594e2f940e2b9f6531d5d26ae332b94a3d313", "title": "Discrimination in the age of artificial intelligence", "abstract": "In this paper, I examine whether the use of artificial intelligence (AI) and automated decision-making (ADM) aggravates issues of discrimination as has been argued by several authors. For this purpose, I first take up the lively philosophical debate on discrimination and present my own definition of the concept. Equipped with this account, I subsequently review some of the recent literature on the use AI/ADM and discrimination. I explain how my account of discrimination helps to understand that the general claim in view of the aggravation of discrimination is unwarranted. Finally, I argue that the use of AI/ADM can, in fact, increase issues of discrimination, but in a different way than most critics assume: it is due to its epistemic opacity that AI/ADM threatens to undermine our moral deliberation which is essential for reaching a common understanding of what should count as discrimination. As a consequence, it turns out that algorithms may actually help to detect hidden forms of discrimination.", "venue": "", "year": 2021, "referenceCount": 49, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "1770514730", "name": "B. Heinrichs"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3c25b757b902281007ff141e2dc649a8c3e84acc", "externalIds": {"MAG": "3137895840", "DOI": "10.1007/S43681-021-00047-2"}, "url": "https://www.semanticscholar.org/paper/3c25b757b902281007ff141e2dc649a8c3e84acc", "title": "Community-in-the-loop: towards pluralistic value creation in AI, or\u2014why AI needs business ethics", "abstract": "Today, due to growing computing power and the increasing availability of high-quality datasets, artificial intelligence (AI) technologies are entering many areas of our everyday life. Thereby, however, significant ethical concerns arise, including issues of fairness, privacy and human autonomy. By aggregating current concerns and criticisms, we identify five crucial shortcomings of the current debate on the ethics of AI. On the threshold of a third wave of AI ethics, we find that the field eventually fails to take sufficient account of the business context and deep societal value conflicts the use of AI systems may evoke. For even a perfectly fair AI system, regardless of its feasibility, may be ethically problematic, a too narrow focus on the ethical implications of technical systems alone seems insufficient. Therefore, we introduce a business ethics perspective based on the normative theory of contractualism and conceptualise ethical implications as conflicts between values of diverse stakeholders. We argue that such value conflicts can be resolved by an account of deliberative order ethics holding that stakeholders of an economic community deliberate the costs and benefits and agree on rules for acceptable trade-offs when AI systems are employed. This allows AI ethics to consider business practices, to recognise the role of firms, and ethical AI not being at risk to provide a competitive disadvantage or in conflict with the current functioning of economic markets. By introducing deliberative order ethics, we thus seek to do justice to the fundamental normative and political dimensions at the core of AI ethics.", "venue": "", "year": 2021, "referenceCount": 137, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "100708560", "name": "Johann Jakob H\u00e4u\u00dfermann"}, {"authorId": "91990029", "name": "C. L\u00fctge"}]}}, {"contexts": ["Fairness has thus become an important consideration in the development of machine learning applications and in the framing of ethical artificial intelligence [15]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "3d7645a0eacd43a779bb448702938143e90274bb", "externalIds": {"ArXiv": "2107.12049", "DBLP": "journals/corr/abs-2107-12049"}, "url": "https://www.semanticscholar.org/paper/3d7645a0eacd43a779bb448702938143e90274bb", "title": "SVEva Fair: A Framework for Evaluating Fairness in Speaker Verification", "abstract": "Despite the success of deep neural networks (DNNs) in enabling on-device voice assistants, increasing evidence of bias and discrimination in machine learning is raising the urgency of investigating the fairness of these systems. Speaker verification is a form of biometric identification that gives access to voice assistants. Due to a lack of fairness metrics and evaluation frameworks that are appropriate for testing the fairness of speaker verification components, little is known about how model performance varies across subgroups, and what factors influence performance variation. To tackle this emerging challenge, we design and develop SVEva1 Fair, an accessible, actionable and model-agnostic framework for evaluating the fairness of speaker verification components. The framework provides evaluation measures and visualisations to interrogate model performance across speaker subgroups and compare fairness between models. We demonstrate SVEva Fair in a case study with end-to-end DNNs trained on the VoxCeleb datasets to reveal potential bias in existing embedded speech recognition systems based on the demographic attributes of speakers. Our evaluation shows that publicly accessible benchmark models are not fair and consistently produce worse predictions for some nationalities, and for female speakers of most nationalities. To pave the way for fair and reliable embedded speaker verification, SVEva Fair has been implemented as an open-source python library and can be integrated into the embedded ML development pipeline to facilitate developers and researchers in troubleshooting unreliable speaker verification performance, and selecting high impact approaches for mitigating fairness challenges.", "venue": "ArXiv", "year": 2021, "referenceCount": 27, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Engineering"], "authors": [{"authorId": "122929064", "name": "Wiebke Toussaint"}, {"authorId": "1934437", "name": "Aaron Yi Ding"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3fc40dcb262222b37e2d345143f855dbaeedc24a", "externalIds": {"MAG": "3198965420", "DOI": "10.1093/JCMC/ZMAB013"}, "url": "https://www.semanticscholar.org/paper/3fc40dcb262222b37e2d345143f855dbaeedc24a", "title": "In AI We Trust? Effects of Agency Locus and Transparency on Uncertainty Reduction in Human\u2013AI Interaction", "abstract": null, "venue": "", "year": 2021, "referenceCount": 32, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": null, "name": "Bingjie Liu"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4008ab900c73d349a419652af156cad724fb3fc0", "externalIds": {"MAG": "3193220896", "DOI": "10.1016/J.TECHSOC.2021.101688"}, "url": "https://www.semanticscholar.org/paper/4008ab900c73d349a419652af156cad724fb3fc0", "title": "The legitimacy gap of algorithmic decision-making in the public sector: Why it arises and how to address it", "abstract": "Abstract Algorithmic decision-making (ADM) systems are increasingly adopted by the state to support various administrative functions and improve the effectiveness and efficiency of public services, such as in unemployment services or policing. While these systems create challenges of opaqueness, unfairness, and value trade-offs, the present paper argues that a more fundamental challenge lies in the way these systems alter the epistemic bases of decision-making. It contributes to the literature by highlighting why procedural standards of legitimacy in operative decision-making no longer suffice for certain applications and by discussing how the resulting legitimacy gap can be addressed through stakeholder involvement. By adapting research on participatory technology assessments to the particularities of ADM system design, it is possible to identify the core challenges of such a stakeholder process and the necessary steps to deal with them.", "venue": "", "year": 2021, "referenceCount": 73, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Business"], "authors": [{"authorId": "90468386", "name": "Pascal D. K\u00f6nig"}, {"authorId": "14198009", "name": "Georg Wenzelburger"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "416893cf0fb5118181536b9124302507fdbd0607", "externalIds": {"MAG": "3116890626", "DOI": "10.1016/j.techfore.2020.120482"}, "url": "https://www.semanticscholar.org/paper/416893cf0fb5118181536b9124302507fdbd0607", "title": "Are we preparing for a good AI society? A bibliometric review and research agenda", "abstract": "Abstract Artificial intelligence (AI) may be one of the most disruptive technologies of the 21st century, with the potential to transform every aspect of society. Preparing for a \u201cgood AI society\u201d has become a hot topic, with growing public and scientific interest in the principles, policies, incentives, and ethical frameworks necessary for society to enjoy the benefits of AI while minimizing the risks associated with its use. However, despite the renewed interest in artificial intelligence, little is known of the direction in which AI scholarship is moving and whether the field is evolving towards the goal of building a \u201cgood AI society\u201d. Based on a bibliometric analysis of 40147 documents retrieved from the Web of Science database, this study describes the intellectual, social, and conceptual structure of AI research. It provides 136 evidence-based research questions about how AI research can help understand the social changes brought about by AI and prepare for a \u201cgood AI society.\u201d The research agenda is organized according to ten social impact domains identified from the literature, including crisis response, economic empowerment, educational challenges, environmental challenges, equality and inclusion, health and hunger, information verification and validation, infrastructure management, public and social sector management, security, and justice.", "venue": "", "year": 2021, "referenceCount": 140, "citationCount": 17, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "1652007813", "name": "S. Fosso Wamba"}, {"authorId": "31204744", "name": "R. Bawack"}, {"authorId": "153489359", "name": "C. Guthrie"}, {"authorId": "97137835", "name": "M. Queiroz"}, {"authorId": "2002608", "name": "K\u00e9vin D. Carillo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "456452902b87c4112ebb8f87109fe02beb6500cc", "externalIds": {"MAG": "3136486749", "DOI": "10.1016/B978-0-12-820203-6.00001-1"}, "url": "https://www.semanticscholar.org/paper/456452902b87c4112ebb8f87109fe02beb6500cc", "title": "Big Data analytics and artificial intelligence in mental healthcare", "abstract": null, "venue": "", "year": 2021, "referenceCount": 89, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "39890672", "name": "Ariel Rosenfeld"}, {"authorId": "50817013", "name": "D. Benrimoh"}, {"authorId": "2105986026", "name": "C. Armstrong"}, {"authorId": "51970365", "name": "N. Mirchi"}, {"authorId": "1410656564", "name": "Timothe Langlois-Therrien"}, {"authorId": "51013147", "name": "C. Rollins"}, {"authorId": "1410671900", "name": "M. Tanguay-Sela"}, {"authorId": "82742855", "name": "J. Mehltretter"}, {"authorId": "50814269", "name": "R. Fratila"}, {"authorId": "50813061", "name": "S. Israel"}, {"authorId": "51450613", "name": "E. Snook"}, {"authorId": "50813137", "name": "K. Perlman"}, {"authorId": "51119622", "name": "Akiva Kleinerman"}, {"authorId": "4038972", "name": "Bechara John Saab"}, {"authorId": "1410675989", "name": "Mark Thoburn"}, {"authorId": "1410653927", "name": "Cheryl Gabbay"}, {"authorId": "1410648475", "name": "Amit Yaniv-Rosenfeld"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "45fd9c1917225dafa30bc6afb2940c022817d1a2", "externalIds": {"DBLP": "journals/ais/Innerarity21", "MAG": "3126704554", "DOI": "10.1007/S00146-020-01130-8"}, "url": "https://www.semanticscholar.org/paper/45fd9c1917225dafa30bc6afb2940c022817d1a2", "title": "Making the black box society transparent", "abstract": "The growing presence of smart devices in our lives turns all of society into something largely unknown to us. The strategy of demanding transparency stems from the desire to reduce the ignorance to which this automated society seems to condemn us. An evaluation of this strategy first requires that we distinguish the different types of non-transparency. Once we reveal the limits of the transparency needed to confront these devices, the article examines the alternative strategy of explainable artificial intelligence and concludes with the idea that these types of complex realities exceed individual capacities and are only comprehensible in a collective fashion.", "venue": "AI Soc.", "year": 2021, "referenceCount": 77, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Business"], "authors": [{"authorId": "67109850", "name": "Daniel Innerarity"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4a5e6fc6af13fe9dc4c5dca626d3adbcb0ea25ea", "externalIds": {"DOI": "10.1007/978-3-030-80083-3_11"}, "url": "https://www.semanticscholar.org/paper/4a5e6fc6af13fe9dc4c5dca626d3adbcb0ea25ea", "title": "The Explanation Game: A Formal Framework for Interpretable Machine Learning", "abstract": null, "venue": "Digital Ethics Lab Yearbook", "year": 2021, "referenceCount": 102, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2140403023", "name": "David S. Watson"}, {"authorId": "2134633922", "name": "Luciano Floridi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4a7f4acebbb735c5cac4da764360b79475db1a8a", "externalIds": {"MAG": "3160005967", "DOI": "10.1007/S13347-021-00450-X"}, "url": "https://www.semanticscholar.org/paper/4a7f4acebbb735c5cac4da764360b79475db1a8a", "title": "Four Responsibility Gaps with Artificial Intelligence: Why they Matter and How to Address them", "abstract": "The notion of \u201cresponsibility gap\u201d with artificial intelligence (AI)\u00a0was originally introduced in the philosophical debate to indicate the concern that \u201clearning automata\u201d may make more difficult or impossible to attribute moral culpability to persons for untoward events. Building on literature in moral and legal philosophy, and ethics of technology, the paper proposes a broader and more comprehensive analysis of the responsibility gap. The responsibility gap, it is argued, is not one problem but a set of at least four interconnected problems \u2013 gaps in culpability, moral and public accountability, active responsibility\u2014caused by different sources, some technical, other organisational, legal, ethical, and societal. Responsibility gaps may also happen with non-learning systems. The paper clarifies which aspect of AI may cause which gap in which form of responsibility, and why each of these gaps matter. It proposes a critical review of partial and non-satisfactory attempts to address the responsibility gap: those which present it as a new and intractable problem (\u201cfatalism\u201d), those which dismiss it as a false problem (\u201cdeflationism\u201d), and those which reduce it to only one of its dimensions or sources and/or present it as a problem that can be solved by simply introducing new technical and/or legal tools (\u201csolutionism\u201d). The paper also outlines a more comprehensive approach to address the responsibility gaps with AI in their entirety, based on the idea of designing socio-technical systems for \u201cmeaningful human control\", that is systems aligned with the relevant human reasons and capacities.", "venue": "", "year": 2021, "referenceCount": 72, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "8623431", "name": "F. S. D. Sio"}, {"authorId": "31122515", "name": "G. Mecacci"}]}}, {"contexts": ["[18] Brent Daniel Mittelstadt, Patrick Allo, Mariarosaria Taddeo, Sandra Wachter, and Luciano Floridi.", "Mais de 30 materiais foram disponibilizados para leitura, de variados temas como \u00c9tica em algoritmos [18], hist\u00f3ria das mulheres na Computa\u00e7\u00e3o [21] e rela\u00e7\u00e3o da Computa\u00e7\u00e3o com a Matem\u00e1tica [14], entre outros textos como [11] [9] [6]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "4b1e42662cc7dbc8b6a34b1505ac0d5887539b09", "externalIds": {"MAG": "3144659917", "DOI": "10.5753/EDUCOMP.2021.14485"}, "url": "https://www.semanticscholar.org/paper/4b1e42662cc7dbc8b6a34b1505ac0d5887539b09", "title": "Hello World: 17 habilidades para exercitar desde o in\u00edcio da gradua\u00e7\u00e3o em computa\u00e7\u00e3o", "abstract": "RESUMO Reconhecendo diferentes desafios do ensino de Computa\u00e7\u00e3o e conscientes do contexto situado em que atuamos, desde 2019 oferecemos uma disciplina de Introdu\u00e7\u00e3o \u00e0 Ci\u00eancia da Computa\u00e7\u00e3o, como disciplina curricular obrigat\u00f3ria, no primeiro semestre do curso de Bacharelado em Ci\u00eancia da Computa\u00e7\u00e3o. Nessa disciplina, a ementa, o conte\u00fado program\u00e1tico e as estrat\u00e9gias did\u00e1ticas t\u00eam o prop\u00f3sito de exercitar 17 habilidades que consideramos essenciais para profissionais da \u00e1rea de Computa\u00e7\u00e3o (tanto no sentido t\u00e9cnico quanto social e \u00e9tico), al\u00e9m de situar estudantes na \u00e1rea e no curso escolhidos. Neste artigo apresentamos um relato da disciplina, das 17 habilidades que ela se prop\u00f5e a exercitar, da nossa estrat\u00e9gia did\u00e1tica e dos resultados de nossa experi\u00eancia com a sua aplica\u00e7\u00e3o de forma totalmente remota. Com base na observa\u00e7\u00e3o docente e na opini\u00e3o discente obtida por meio de um question\u00e1rio online, identificamos que a disciplina foi efetiva em promover o exerc\u00edcio das diferentes habilidades, que foi capaz de ampliar o entendimento discente sobre o curso e sobre a responsabilidade profissional, e que o formato remoto apresentou mais vantagens do que desvantagens para a estrat\u00e9gia did\u00e1tica empregada. Os resultados tamb\u00e9m revelaram aspectos da estrat\u00e9gia did\u00e1tica que foram bem sucedidos e aspectos que ainda precisam ser aprimorados.", "venue": "", "year": 2021, "referenceCount": 27, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2068950423", "name": "Roberto Pereira"}, {"authorId": "2070755687", "name": "L. Peres"}, {"authorId": "2107110306", "name": "F. G. Silva"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4bac78db21bb8671896d8f55d433677198814945", "externalIds": {"MAG": "3182036221", "DOI": "10.1007/978-3-030-80126-7_51"}, "url": "https://www.semanticscholar.org/paper/4bac78db21bb8671896d8f55d433677198814945", "title": "Enduring Questions, Innovative Technologies: Educational Theories Interface with AI", "abstract": "This paper aims to tie literature in AI to enduring questions in education about teaching and learning and discern ethical considerations that define those ties. The challenge was to answer the question: how do we merge our learning and leadership theories to technologies and the algorithmic biases that may maintain today's social injustices into our future? The paper first reviews the literature to identify the dialogue on AI by computer scientists in relation to enduring questions in education, learning theories, and ethics. Then we summarize data in the form of vignettes written by experts from the humanities, computer science, and social sciences. Some of the vignettes focused on how educational and technological systems are products of the social system and the ethical implications of such connections. Other writings centered data-driven approaches to incorporating AI technologies in classrooms, with concerns around uneven implementation and differential access. The paper concludes that to dialogue with educators AIED will need to move away from discussions of efficiency as measured by educational assessments and incorporate humanistic and social learning theories that embrace the complexities of human relationships. Developers should seek to work directly with educational leaders to establish optimal teaching strategies for the ethical \u2018good\u2019 of the learner, while attending to social justice parameters. Equally critical is the need to create ethical parameters between the AI and the student.", "venue": "Lecture Notes in Networks and Systems", "year": 2021, "referenceCount": 44, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "48781377", "name": "R. Papa"}, {"authorId": "39411957", "name": "K. Jackson"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4e49da48ac4616a0d355954682ae82b80e20521f", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/4e49da48ac4616a0d355954682ae82b80e20521f", "title": "Addressing Ethical Dilemmas in AI : Listening to Engineers", "abstract": null, "venue": "", "year": 2021, "referenceCount": 19, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [",[2,6,26,27]); the second is to change the system when its decision is unwarranted (e."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "4f8e20e8436f563eab5880779121ca004086452c", "externalIds": {"DBLP": "journals/corr/abs-2108-10437", "ArXiv": "2108.10437"}, "url": "https://www.semanticscholar.org/paper/4f8e20e8436f563eab5880779121ca004086452c", "title": "Longitudinal Distance: Towards Accountable Instance Attribution", "abstract": "Previous research in interpretable machine learning (IML) and explainable artificial intelligence (XAI) can be broadly categorized as either focusing on seeking interpretability in the agent\u2019s model (i.e., IML) or focusing on the context of the user in addition to the model (i.e., XAI). The former can be categorized as feature or instance attribution. Exampleor sample-based methods such as those using or inspired by case-based reasoning (CBR) rely on various approaches to select instances that are not necessarily attributing instances responsible for an agent\u2019s decision. Furthermore, existing approaches have focused on interpretability and explainability but fall short when it comes to accountability. Inspired in case-based reasoning principles, this paper introduces a pseudo-metric we call Longitudinal distance and its use to attribute instances to a neural network agent\u2019s decision that can be potentially used to build accountable CBR agents.", "venue": "ArXiv", "year": 2021, "referenceCount": 35, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1684264", "name": "Rosina O. Weber"}, {"authorId": "2066197440", "name": "Prateek Goel"}, {"authorId": "98523873", "name": "S. Amiri"}, {"authorId": "48138448", "name": "Gideon Simpson"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "502e965aeec5e6e2c4c7e8662eb5fc2e4f39f72d", "externalIds": {"DBLP": "journals/polity/Gerrits21", "MAG": "3120360074", "DOI": "10.3233/ip-200224"}, "url": "https://www.semanticscholar.org/paper/502e965aeec5e6e2c4c7e8662eb5fc2e4f39f72d", "title": "Soul of a new machine: Self-learning algorithms in public administration", "abstract": "Big data sets in conjunction with self-learning algorithms are becoming increasingly important in public administration. A growing body of literature demonstrates that the use of such technologies poses fundamental questions about the way in which predictions are generated, and the extent to which such predictions may be used in policy making. Complementing other recent works, the goal of this article is to open the machine\u2019s black box to understand and critically examine how self-learning algorithms gain agency by transforming raw data into policy recommendations that are then used by policy makers. I identify five major concerns and discuss the implications for policy making.", "venue": "Inf. Polity", "year": 2021, "referenceCount": 62, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "46190813", "name": "Lasse Gerrits"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "525350e37676f17ec3a9c19d41c5493552769beb", "externalIds": {"PubMedCentral": "8126774", "DOI": "10.1073/pnas.2104563118", "PubMed": "33941682"}, "url": "https://www.semanticscholar.org/paper/525350e37676f17ec3a9c19d41c5493552769beb", "title": "Reply to Swartz et al.: Challenges and opportunities for identifying forced labor using satellite-based fishing vessel monitoring", "abstract": "We appreciate Swartz et al. (1) for highlighting several key considerations for interpreting our results (2). While we discuss many of these in our paper, we are grateful to further highlight our work\u2019s strengths, limitations, and future opportunities. A major challenge with understanding fisheries labor abuses is a lack of data. Automatic identification system (AIS) is only used by a subset of the global fishing fleet. However, AIS is valuable for monitoring certain types of fishing vessels, especially those that are large (\u223c52 to 85% carry AIS) (3) and those fishing on the high seas (\u223c80% carry AIS) (4). Mandating AIS and unique identifiers on fishing vessels and publishing vessel registries would facilitate more inclusive AIS-based analyses (5). Data on fisheries labor conditions are also limited. We spent over 1 y identifying public reports of forced labor onboard specific fishing vessels (\u201cpositives\u201d). We also tried to identify a public list of specific fishing vessels free of forced labor (\u201cnegatives\u201d) but were unable to, and therefore we were compelled to use positive-unlabeled learning. We assessed model performance using 10-fold cross-validation, an appropriate technique for small datasets (6) that uses resampling to train and validate multiple models using multiple training and separate validation data subsets. We estimated an average recall of 92%, the fraction of known positives correctly classified as positive (2). We used the term \u201chigh risk\u201d for vessels classified as positive by the model for being above the threshold that maximizes a modified F1 score (7). While we cannot infer probability using this approach, it theoretically minimizes false positives and false negatives and equally weights the practical risks associated with both error types (7). Publishing information from forced labor vessel sanctions (5) (positives) and information from vessel inspections that identify either forced labor (positives) or decent working conditions (negatives) would increase training and testing data and facilitate more accurate analyses. Our analysis focused on prediction not causation. We did not estimate what causes forced labor but predicted whether vessels have forced labor using observable vessel features. While unpacking correlations between features would be critical in causal inference, understanding these correlations is less important for prediction. Nevertheless, we removed highly correlated model features during data preprocessing (2), which reduces model complexity while increasing feature importance interpretability (8). Moving forward, new research on causal relationships is critical, as are interventions that address causal drivers. When using predictive models, there is a risk that spurious or biased trends in the training data could lead to unjustified actions with serious human consequences (9). We recognize this ethical concern and stress the importance of further validation and evaluation of potential biases using new data. Nonetheless, predictive models can inform decisions within an otherwise opaque decision-making landscape (10). The path forward should include a suite of forced labor detection methods alongside interventions that address underlying drivers, reform labor policy, promote social responsibility in seafood production, and support victims. While we acknowledge the limitations of our approach, it lays the foundation for new opportunities to improve fisher working conditions.", "venue": "Proceedings of the National Academy of Sciences", "year": 2021, "referenceCount": 10, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "122343144", "name": "Gavin G McDonald"}, {"authorId": "2034058", "name": "C. Costello"}, {"authorId": "2086120564", "name": "Jennifer Bone"}, {"authorId": "49712967", "name": "R. Cabral"}, {"authorId": "2046810112", "name": "Valerie Farabee"}, {"authorId": "2327498", "name": "T. Hochberg"}, {"authorId": "13200709", "name": "D. Kroodsma"}, {"authorId": "51225634", "name": "T. Mangin"}, {"authorId": "144791117", "name": "K. Meng"}, {"authorId": "2046810835", "name": "Oliver Zahn"}]}}, {"contexts": ["Bias in these systems has myriad sources, such as design choices [18], models and data [17], and interpretation of results [8]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "52de2b3a1ef3bdde76446807f5e3b572cc0170f6", "externalIds": {"ArXiv": "2102.06265", "DBLP": "journals/corr/abs-2102-06265", "DOI": "10.1109/LRA.2021.3067283"}, "url": "https://www.semanticscholar.org/paper/52de2b3a1ef3bdde76446807f5e3b572cc0170f6", "title": "Fair Robust Assignment Using Redundancy", "abstract": "We study the consideration of fairness in redundant assignment for multi-agent task allocation. It has recently been shown that redundant assignment of agents to tasks provides robustness to uncertainty in task performance. However, the question of how to <italic>fairly</italic> assign these redundant resources across tasks remains unaddressed. In this letter, we present a novel problem formulation for fair redundant task allocation, which we cast as the optimization of worst-case task costs under a cardinality constraint. Solving this problem optimally is NP-hard. We exploit properties of supermodularity to propose a polynomial-time, near-optimal solution. In supermodular redundant assignment, the use of additional agents always improves task costs. Therefore, we provide a solution set that is <inline-formula><tex-math notation=\"LaTeX\">$\\alpha$</tex-math></inline-formula> times larger than the cardinality constraint. This constraint relaxation enables our approach to achieve a super-optimal cost by using a sub-optimal assignment size. We derive the sub-optimality bound on this cardinality relaxation, <inline-formula><tex-math notation=\"LaTeX\">$\\alpha$</tex-math></inline-formula>. Additionally, we demonstrate that our algorithm performs near-optimally without the cardinality relaxation. We show simulations of redundant assignments of robots to goal nodes on transport networks with uncertain travel times. Empirically, our algorithm outperforms benchmarks, scales to large problems, and provides improvements in both fairness and average utility.", "venue": "IEEE Robotics and Automation Letters", "year": 2021, "referenceCount": 41, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1961429575", "name": "Matthew Malencia"}, {"authorId": "37956314", "name": "Vijay R. Kumar"}, {"authorId": "118992395", "name": "G. Pappas"}, {"authorId": "2081613", "name": "Amanda Prorok"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "559c89d86159239113b3b41662f9cc977abee334", "externalIds": {"MAG": "3176523578", "DOI": "10.1016/b978-0-12-823410-5.00006-1"}, "url": "https://www.semanticscholar.org/paper/559c89d86159239113b3b41662f9cc977abee334", "title": "A literature review on artificial intelligence and ethics in online learning", "abstract": "Abstract In recent years, artificial intelligence (AI) has been used in online learning to improve teaching and learning, with the aim of providing a more efficient, purposeful, adaptive, ubiquitous, and fair learning experiences. However, and as it has been seen in other contexts, the integration of AI can have unforeseen consequences with detrimental effects which can result in unfair and discriminatory decisions. Therefore it is worth thinking about potential risks that learning environments integrating AI systems might pose. This work explores the intersections between AI, online learning, and ethics in order to understand the ethical concerns surrounding this crossroads. We review the main ethical challenges identified in the literature and distill a set of guidelines to support the ethical design and integration of AI systems in online learning environments. This should help ensure that online learning is how is meant to be: accessible, inclusive, fair, and beneficial to society.", "venue": "Intelligent Systems and Learning Data Analytics in Online Education", "year": 2021, "referenceCount": 63, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1404289979", "name": "Joan Casas-Roma"}, {"authorId": "145324033", "name": "J. Conesa"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "55ac812a42e00d3b1fb8f1f8a66e8686356f3ca6", "externalIds": {"MAG": "3133828189", "DOI": "10.1007/S13347-021-00443-W"}, "url": "https://www.semanticscholar.org/paper/55ac812a42e00d3b1fb8f1f8a66e8686356f3ca6", "title": "The Influence of Business Incentives and Attitudes on Ethics Discourse in the Information Technology Industry", "abstract": "As information technologies have become synonymous with progress in modern society, several ethical concerns have surfaced about their societal implications. In the past few decades, information technologies have had a value-laden impact on social evolution. However, there is limited agreement on the responsibility of businesses and innovators concerning the ethical aspects of information technologies. There is a need to understand the role of business incentives and attitudes in driving technological progress and to understand how they steer the ethics discourse on technology. In the information technology industry, there is an observed trivialization of ethics supported by a business driven and technology-centric approach to ethics. This trivialization rests on hardened beliefs, ideologies, and arguments which hint at reduced accountability for business and tend to individualize social responsibility. The phenomenon of ethics trivialization needs to be duly addressed to resolve the tensions between business needs and ethical concerns. This paper has identified from literature and conceptually analyzed the beliefs and ideologies underlying ethics trivialization which undermine ethics in business contexts. The paper has attempted to address the business concerns indicated by this phenomenon as well as highlight the weaknesses of its assumptions, rhetoric, and justifications. The aim of this paper is to systematically present the justifications in favor of and against the practice of ethics trivialization in the information technology domain, thereby highlighting the need to develop frameworks for assessment of ethical responsibility, accountability, and democratization of the value trade-offs involved in the design of technologies.", "venue": "", "year": 2021, "referenceCount": 143, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "2028218284", "name": "Sanju Ahuja"}, {"authorId": "152723276", "name": "Jyotish Kumar"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5626213ca6852d436b9e6dc923848a42c016cd41", "externalIds": {"PubMedCentral": "8404542", "DOI": "10.1007/s00146-021-01262-5", "PubMed": "34483498"}, "url": "https://www.semanticscholar.org/paper/5626213ca6852d436b9e6dc923848a42c016cd41", "title": "AI, big data, and the future of consent", "abstract": "In this paper, we discuss several problems with current Big data practices which, we claim, seriously erode the role of informed consent as it pertains to the use of personal information. To illustrate these problems, we consider how the notion of informed consent has been understood and operationalised in the ethical regulation of biomedical research (and medical practices, more broadly) and compare this with current Big data practices. We do so by first discussing three types of problems that can impede informed consent with respect to Big data use. First, we discuss the transparency (or explanation) problem. Second, we discuss the re-repurposed data problem. Third, we discuss the meaningful alternatives problem. In the final section of the paper, we suggest some solutions to these problems. In particular, we propose that the use of personal data for commercial and administrative objectives could be subject to a \u2018soft governance\u2019 ethical regulation, akin to the way that all projects involving human participants (e.g., social science projects, human medical data and tissue use) are regulated in Australia through the Human Research Ethics Committees (HRECs). We also consider alternatives to the standard consent forms, and privacy policies, that could make use of some of the latest research focussed on the usability of pictorial legal contracts.", "venue": "AI & society", "year": 2021, "referenceCount": 103, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "115598481", "name": "Adam J. Andreotta"}, {"authorId": "116236792", "name": "Nin Kirkham"}, {"authorId": "2065573976", "name": "M. Rizzi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "567bf049c5d596a0b08bea7639b97bca2693ce64", "externalIds": {"MAG": "3173583702", "DOI": "10.1007/978-3-030-64590-8_6"}, "url": "https://www.semanticscholar.org/paper/567bf049c5d596a0b08bea7639b97bca2693ce64", "title": "Brain-Computer Interfaces: Current and Future Investigations in the Philosophy and Politics of Neurotechnology", "abstract": "Important insights have been generated by ethicists, philosophers, sociologists, lawyers, and representatives from other disciplines regarding the ethics of neurotechnology in general and of brain-computer interfaces (BCIs) in particular. However, since (medical) BCIs have yet to leave the laboratory and the context of clinical studies and enter the \u201creal\u201d world, many important normative questions remain unanswered. In this paper we summarize the main lines of ethical inquiry regarding BCIs, both from the general academic discussion and with a view on the results gained in INTERFACES, an interdisciplinary project on the normative dimensions of BCIs. Furthermore, we offer our perspective on future research and argue that the ethics of technology should explore decision-making processes by which communities and societies regulate emerging technologies, such as BCIs.", "venue": "Advances in Neuroethics", "year": 2021, "referenceCount": 31, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "145331865", "name": "Andreas Wolkenstein"}, {"authorId": "48685400", "name": "O. Friedrich"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "57cd2ac19c1eb3f2e60da2271dade62723b40004", "externalIds": {"MAG": "3187959909", "DOI": "10.1080/13669877.2021.1957985"}, "url": "https://www.semanticscholar.org/paper/57cd2ac19c1eb3f2e60da2271dade62723b40004", "title": "Ignorance and the regulation of artificial intelligence", "abstract": "Much has been written about the risks posed by artificial intelligence (AI). This article is interested not only in what is known about these risks, but what remains unknown and how that unknowing ...", "venue": "Journal of Risk Research", "year": 2021, "referenceCount": 52, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": null, "name": "James M. White"}, {"authorId": "152141509", "name": "R. Lidskog"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5c9aafbaefbc6e7ec11a05726e21c166f5b2cfdd", "externalIds": {"DOI": "10.3989/arbor.2021.800001"}, "url": "https://www.semanticscholar.org/paper/5c9aafbaefbc6e7ec11a05726e21c166f5b2cfdd", "title": "Presentaci\u00f3n. Inteligencia artificial y nuevas \u00e9ticas de la convivencia", "abstract": "Las tecnolog\u00edas de la inteligencia artificial (IA) hacen emerger con mayor fuerza una pregunta central para la filosof\u00eda contempor\u00e1nea: \u00bfc\u00f3mo se generan los desplazamientos \u00e9ticos a trav\u00e9s de la producci\u00f3n de nuevas formas de convivencia tecnol\u00f3gica? Saber en qu\u00e9 consisten estos desplazamientos y si contribuyen, o no, a determinados tipos de convivencia es m\u00e1s urgente que precipitarse a una producci\u00f3n de normativa que no se enfrenta a los cambios inherentes al nuevo entorno. Pero una de las consecuencias que apuntan en este enfoque es que la programaci\u00f3n, sometida a las l\u00f3gicas de la anticipaci\u00f3n, no puede resolver el problema simplemente. La programaci\u00f3n tiende a la formaci\u00f3n de nichos o esferas en los que los individuos se desenvuelven perdiendo visi\u00f3n de la complejidad; la delegaci\u00f3n de los c\u00e1lculos a las m\u00e1quinas hace que disminuya no s\u00f3lo la capacidad del c\u00e1lculo aritm\u00e9tico sino la imaginaci\u00f3n matem\u00e1tica, creando nichos epist\u00e9micos crecientemente reservados a las m\u00e1quinas; la miner\u00eda de datos borra las diferencias entre lo p\u00fablico y lo privado, afianzando modos de comportamiento que esquivan la autorreflexi\u00f3n y el escrutinio de nuestras acciones en relaci\u00f3n al bien com\u00fan. Pero \u00bfpueden los programas desenvolverse en un sentido contrario, fortaleciendo las destrezas epist\u00e9micas, haciendo visible la complejidad, la participaci\u00f3n pol\u00edtica y la diversificaci\u00f3n de bienes comunes en un mundo tecnol\u00f3gicamente denso? O en otros t\u00e9rminos, \u00bfpodemos encontrar en ellos claves para una \u00e9tica robusta de la convivencia tecnol\u00f3gica? Las contribuciones de este n\u00famero desgranan las transformaciones en las nociones de autonom\u00eda, creatividad y precariedad en el contexto de la IA.", "venue": "Arbor", "year": 2021, "referenceCount": 15, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2139541693", "name": "Nuria Valverde P\u00e9rez"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5e27eb3dde8560574670051e89e9a953418ca49c", "externalIds": {"MAG": "3172711130", "DOI": "10.1111/1467-9752.12566"}, "url": "https://www.semanticscholar.org/paper/5e27eb3dde8560574670051e89e9a953418ca49c", "title": "Unprepared humanities: A pedagogy (forced) online", "abstract": "Abstract The forcing online of higher education classes should have constituted a major reckoning of pedagogical practices in universities, particularly in the humanities. Such a reckoning seems to have been muted by a focus on logistical concerns and by what might be called a false sense of preparedness within university departments. This study attempts to counter that general trend by taking seriously the cognitive and emotional demands of the online transition through a philosophical lens. This essay presents a phenomenological study of the abrupt transition from physical to online classes during the COVID-19 lockdown of universities. Reworking two central theses of Marshall McLuhan's (1994) study of media, the author proposes two dispositions as necessary to encountering the role of new media in the teaching of the humanities: a state of wilful unpreparedness and a state of artistic creativity. These in turn pose educational dilemmas that require changes to the relationship between teachers, students and the learning environment, wherein these relationships become subjects of conscious study by the participants. The article draws on a range of classroom experiences in which the instructor and students attempted such a study. In encountering the shock of the new media, a series of concepts emerge that can help promote an artistic approach towards the medium itself.", "venue": "", "year": 2021, "referenceCount": 35, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "69359193", "name": "H. Harouni"}]}}, {"contexts": ["Unlike rule-based AI systems, they develop their own rules to process information, which cannot meaningfully be interpreted by a human (Mittelstadt et al., 2016).", "own rules to process information, which cannot meaningfully be interpreted by a human (Mittelstadt et al., 2016).", "A major concern about algorithms, which transparency can help mitigate, is that they are biased (Mittelstadt et al., 2016; Jobin et al., 2019)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "5fa1ca52f54ec170d42f899ff918ec3609895d42", "externalIds": {"ArXiv": "2106.16122"}, "url": "https://www.semanticscholar.org/paper/5fa1ca52f54ec170d42f899ff918ec3609895d42", "title": "Zombies in the Loop? Humans Trust Untrustworthy AI-Advisors for Ethical Decisions", "abstract": "Departing from the claim that AI needs to be trustworthy, we find that ethical advice from an AI-powered algorithm is trusted even when its users know nothing about its training data and when they learn information about it that warrants distrust. We conducted online experiments where the subjects took the role of decision-makers who received advice from an algorithm on how to deal with an ethical dilemma. We manipulated the information about the algorithm and studied its influence. Our findings suggest that AI is overtrusted rather than distrusted. We suggest digital literacy as a potential remedy to ensure the responsible use of AI.", "venue": "", "year": 2021, "referenceCount": 51, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2047482059", "name": "Sebastian Krugel"}, {"authorId": "20658415", "name": "Andreas Ostermaier"}, {"authorId": "1696033469", "name": "Matthias W. Uhl"}]}}, {"contexts": ["Scopus is known to have an expansive database [58], and other literature reviews in computing have utilized it during the identification phase [1, 114].", "A 2016 review article sought to \"map the debate\" around ethics and algorithms [114], and a 2018 review article [1] identified a research agenda for developing \"Explainable, Accountable and Intelligible Systems."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "61001ef1907a3dbdd78fdc5201bc5457c9eee807", "externalIds": {"DBLP": "journals/pacmhci/Bandy21", "DOI": "10.1145/3449148"}, "url": "https://www.semanticscholar.org/paper/61001ef1907a3dbdd78fdc5201bc5457c9eee807", "title": "Problematic Machine Behavior", "abstract": "While algorithm audits are growing rapidly in commonality and public importance, relatively little scholarly work has gone toward synthesizing prior work and strategizing future research in the area. This systematic literature review aims to do just that, following PRISMA guidelines in a review of over 500 English articles that yielded 62 algorithm audit studies. The studies are synthesized and organized primarily by behavior (discrimination, distortion, exploitation, and misjudgement), with codes also provided for domain (e.g. search, vision, advertising, etc.), organization (e.g. Google, Facebook, Amazon, etc.), and audit method (e.g. sock puppet, direct scrape, crowdsourcing, etc.). The review shows how previous audit studies have exposed public-facing algorithms exhibiting problematic behavior, such as search algorithms culpable of distortion and advertising algorithms culpable of discrimination. Based on the studies reviewed, it also suggests some behaviors (e.g. discrimination on the basis of intersectional identities), domains (e.g. advertising algorithms), methods (e.g. code auditing), and organizations (e.g. Twitter, TikTok, LinkedIn) that call for future audit attention. The paper concludes by offering the common ingredients of successful audits, and discussing algorithm auditing in the context of broader research working toward algorithmic justice.", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2021, "referenceCount": 237, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3363483", "name": "Jack Bandy"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "61321cca9cc37bcc69f283837a96b87e4a4a8b37", "externalIds": {"DOI": "10.1007/978-3-030-81907-1_11"}, "url": "https://www.semanticscholar.org/paper/61321cca9cc37bcc69f283837a96b87e4a4a8b37", "title": "The Explanation Game: A Formal Framework for Interpretable Machine Learning", "abstract": null, "venue": "Philosophical Studies Series", "year": 2021, "referenceCount": 67, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2140403023", "name": "David S. Watson"}, {"authorId": "2134633922", "name": "Luciano Floridi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "6220b7d0c97ded4a695efc4584d3d175024d7b25", "externalIds": {"MAG": "3184989224", "DOI": "10.1177/09646639211032957"}, "url": "https://www.semanticscholar.org/paper/6220b7d0c97ded4a695efc4584d3d175024d7b25", "title": "Reconceptualising \u2018risk\u2019: Towards a humanistic paradigm of sexual offending", "abstract": "Within Western criminal justice traditions, the \u2018risk\u2019 paradigm has become the defining logic of contemporary laws and policies on sex offender management. This article critically examines the limitations of current technocratic and algorithmic approaches to risk in relation to sexual offending and how they might be addressed. Drawing on nearly two decades of theoretical and empirical research conducted by the author, it applies the learning on sex offender reintegration and desistance to advance a \u2018humanistic\u2019 paradigm of sexual offending. The paper attempts to counter some of the dangers of algorithmic justice and shift risk-based discourse away from its predominantly \u2018scientific\u2019 origins. It argues that such a move towards a more expansive and progressive version of risk within criminal justice discourses would better capture the realities of sexual offending behaviour and its real-world governance.", "venue": "Social & Legal Studies", "year": 2021, "referenceCount": 101, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "30038070", "name": "Anne-Marie McAlinden"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "64af20a0ffbe105e5c701bc7761da35ea1956c28", "externalIds": {"DOI": "10.1007/978-3-030-81907-1_8"}, "url": "https://www.semanticscholar.org/paper/64af20a0ffbe105e5c701bc7761da35ea1956c28", "title": "The Ethics of Algorithms: Key Problems and Solutions", "abstract": null, "venue": "Philosophical Studies Series", "year": 2021, "referenceCount": 95, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2005338318", "name": "Andreas Tsamados"}, {"authorId": "153400591", "name": "Nikita Aggarwal"}, {"authorId": "3011486", "name": "Josh Cowls"}, {"authorId": "1751614630", "name": "J. Morley"}, {"authorId": "145226144", "name": "Huw Roberts"}, {"authorId": "2084659", "name": "M. Taddeo"}, {"authorId": "2134633922", "name": "Luciano Floridi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "68e0666df8c6193ae56cb800bb64df5336ff1247", "externalIds": {"DOI": "10.1590/es.250099"}, "url": "https://www.semanticscholar.org/paper/68e0666df8c6193ae56cb800bb64df5336ff1247", "title": "NOVOS CAMINHOS PARA A SOCIOLOGIA: TECNOLOGIAS EM EDUCA\u00c7\u00c3O E ACCOUNTABILITY DIGITAL", "abstract": "RESUMO O artigo come\u00e7a por abordar a sociologia digital e descrever algumas express\u00f5es do capitalismo atual, para depois situar a emerg\u00eancia de uma accountability digital. \u00c9 tida em conta a realidade de alguns sistemas educativos em rela\u00e7\u00e3o ao impacto das plataformas e \u00e0 urg\u00eancia de discutir como o seu uso est\u00e1 a ser, ou pode ser, relacionado com a problem\u00e1tica da accountability. Considerando que a datafica\u00e7\u00e3o n\u00e3o \u00e9 neutra, nem \u00e9tica nem politicamente, ressalta-se a necessidade de garantir processos de coleta e tratamento de dados que sejam vi\u00e1veis em termos de transpar\u00eancia e democraticamente escrutin\u00e1veis, evitando que quest\u00f5es importantes em educa\u00e7\u00e3o sejam deixadas \u00e0 a\u00e7\u00e3o (ou decis\u00e3o) dos algoritmos e plataformas digitais como instrumentos, cada vez mais poderosos, de administra\u00e7\u00e3o e gest\u00e3o.", "venue": "Educa\u00e7\u00e3o & Sociedade", "year": 2021, "referenceCount": 59, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "144799300", "name": "Almerindo Janela Afonso"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "698894d8a651a7de3291dcf12084a99d0edb70e6", "externalIds": {"MAG": "3152759574", "DOI": "10.1515/dzph-2021-0016"}, "url": "https://www.semanticscholar.org/paper/698894d8a651a7de3291dcf12084a99d0edb70e6", "title": "Algorithmisches Entscheiden, Ambiguit\u00e4tstoleranz und die Frage nach dem Sinn", "abstract": "Abstract In more and more contexts, human decision-making is replaced by algorithmic decision-making. While promising to deliver efficient and objective decisions, algorithmic decision systems have specific weaknesses, some of which are particularly dangerous if data are collected and processed by profit-oriented companies. In this paper, I focus on two problems that are at the root of the logic of algorithmic decision-making: (1) (in)tolerance for ambiguity, and (2) instantiations of Campbell\u2019s law, i. e. of indicators that are used for \u201csocial decision-making\u201d being subject to \u201ccorruption pressures\u201d and tending to \u201cdistort and corrupt\u201d the underlying social processes. As a result, algorithmic decision-making can risk missing the point of the social practice in question. These problems are intertwined with problems of structural injustice; hence, if algorithms are to deliver on their promises of efficiency and objectivity, accountability and critical scrutiny are needed.", "venue": "", "year": 2021, "referenceCount": 17, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "122609116", "name": "L. Herzog"}]}}, {"contexts": ["note that even the designers of the technology cannot provide a human comprehensible representation of them [45]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "69db71c679bd63f44e7d02a4ce0c8959c08f9b61", "externalIds": {"PubMedCentral": "8321978", "DOI": "10.1007/s10728-021-00430-4", "PubMed": "33745121"}, "url": "https://www.semanticscholar.org/paper/69db71c679bd63f44e7d02a4ce0c8959c08f9b61", "title": "A New Argument for No-Fault Compensation in Health Care: The Introduction of Artificial Intelligence Systems", "abstract": "Artificial intelligence (AI) systems advising healthcare professionals will be widely introduced into healthcare settings within the next 5\u201310 years. This paper considers how this will sit with tort/negligence based legal approaches to compensation for medical error. It argues that the introduction of AI systems will provide an additional argument pointing towards no-fault compensation as the better legal solution to compensation for medical error in modern health care systems. The paper falls into four parts. The first part rehearses the main arguments for and against no-fault compensation. The second explains why it is likely that AI systems will be widely introduced. The third part analyses why it is difficult to fit AI systems into fault-based compensation systems while the final part suggests how no-fault compensation could provide a possible solution to such challenges.", "venue": "Health Care Analysis", "year": 2021, "referenceCount": 76, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "12364167", "name": "S. Holm"}, {"authorId": "144839325", "name": "C. Stanton"}, {"authorId": "2058249325", "name": "Benjamin Bartlett"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "6a064be5664922575a2464c1a7940db93844511f", "externalIds": {"MAG": "3139261288", "DOI": "10.1007/S00146-021-01170-8"}, "url": "https://www.semanticscholar.org/paper/6a064be5664922575a2464c1a7940db93844511f", "title": "Algorithmic augmentation of democracy: considering whether technology can enhance the concepts of democracy and the rule of law through four hypotheticals", "abstract": "The potential use, relevance, and application of AI and other technologies in the democratic process may be obvious to some. However, technological innovation and, even, its consideration may face an intuitive push-back in the form of algorithm aversion (Dietvorst et al. J Exp Psychol 144(1):114\u2013126, 2015). In this paper, I confront this intuition and suggest that a more \u2018extreme\u2019 form of technological change in the democratic process does not necessarily result in a worse outcome in terms of the fundamental concepts of democracy and the Rule of Law. To provoke further consideration and illustrate that initial intuitions regarding democratic innovation may not always be accurate, I pose and explore four ways that AI and other forms of technology could be used to augment the representative democratic process. The augmentations range from voting online to the wholesale replacement of the legislature\u2019s human representatives with algorithms. After first noting the intuition that less invasive forms of augmented democracy may be less objectionable than more extreme forms, I go on to critically assess whether the augmentation of existing systems satisfies or enhances ideas associated with democracy and the Rule of Law (provided by Dahl and Fuller). By imagining a (not too far-fetched) future in a (not too far-removed) democratic society, my conclusion is that, when it comes to democracy and the Rule of Law, intuitions regarding technology may lead us astray.", "venue": "", "year": 2021, "referenceCount": 67, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "145097878", "name": "P. Burgess"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "6d126fc28c99a69f8e2c9873ecd59ffcddfc678c", "externalIds": {"DOI": "10.1177/01622439211053661"}, "url": "https://www.semanticscholar.org/paper/6d126fc28c99a69f8e2c9873ecd59ffcddfc678c", "title": "Ethics as Discursive Work: The Role of Ethical Framing in the Promissory Future of Data-driven Healthcare Technologies", "abstract": "The allure of a \u201cdata-driven\u201d future healthcare system continues to seduce many. Increasingly, work in Science & Technology Studies and related fields started to interrogate the saliency of this promissory rhetoric by raising ethical questions concerning epistemology, bias, surveillance, security, and opacity. Less visible is how ethical arguments are used as part of discursive work by various practitioners engaged in data-driven initiatives in healthcare. This article argues for more explicit attention to such discursive work in shaping the promissory future of data-driven healthcare technologies. Bringing together the hitherto separated themes of promissory futures and an emic approach to ethics as discursive work, we study how actors engaged various data-driven healthcare initiatives discursively conduct such ethics work, implicitly or explicitly assigning tasks and roles for stakeholders. We conceptualize this with the notion of \u201cethical framing\u201d and identify three widely recurring types: ethics as \u201cbalancing act,\u201d the technical \u201cfix,\u201d and ethics as \u201ccollective thought process.\u201d We outline the characteristics of these acts of framing and discuss their implications for the envisaged roles and responsibilities of various actors. In the Discussion section, we outline the added value of bringing the distinct bodies of literature on promissory futures and ethical framing together and outline themes for new research.", "venue": "Science, Technology, & Human Values", "year": 2021, "referenceCount": 54, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "34963471", "name": "R. Wehrens"}, {"authorId": null, "name": "Marthe Stevens"}, {"authorId": "73771413", "name": "J. Kostenzer"}, {"authorId": "7027938", "name": "A. Weggelaar"}, {"authorId": "3049048", "name": "A. D. de Bont"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "6d1d9bd6e7832cc633b74c87bb1e47d492c903cf", "externalIds": {"MAG": "3201593348", "DOI": "10.1108/jices-12-2020-0127"}, "url": "https://www.semanticscholar.org/paper/6d1d9bd6e7832cc633b74c87bb1e47d492c903cf", "title": "Perspectives on computing ethics: a multi-stakeholder analysis", "abstract": "\nPurpose\nComputing ethics represents a long established, yet rapidly evolving, discipline that grows in complexity and scope on a near-daily basis. Therefore, to help understand some of that scope it is essential to incorporate a range of perspectives, from a range of stakeholders, on current and emerging ethical challenges associated with computer technology. This study aims to achieve this by using, a three-pronged, stakeholder analysis of Computer Science academics, ICT industry professionals, and citizen groups was undertaken to explore what they consider to be crucial computing ethics concerns. The overlap between these stakeholder groups are explored, as well as whether their concerns are reflected in the existing literature.\n\n\nDesign/methodology/approach\nData collection was performed using focus groups, and the data was analysed using a thematic analysis. The data was also analysed to determine if there were overlaps between the literature and the stakeholders\u2019 concerns and attitudes towards computing ethics.\n\n\nFindings\nThe results of the focus group analysis show a mixture of overlapping concerns between the different groups, as well as some concerns that are unique to each of the specific groups. All groups stressed the importance of data as a key topic in computing ethics. This includes concerns around the accuracy, completeness and representativeness of data sets used to develop computing applications. Academics were concerned with the best ways to teach computing ethics to university students. Industry professionals believed that a lack of diversity in software teams resulted in important questions not being asked during design and development. Citizens discussed at length the negative and unexpected impacts of social media applications. These are all topics that have gained broad coverage in the literature.\n\n\nSocial implications\nIn recent years, the impact of ICT on society and the environment at large has grown tremendously. From this fast-paced growth, a myriad of ethical concerns have arisen. The analysis aims to shed light on what a diverse group of stakeholders consider the most important social impacts of technology and whether these concerns are reflected in the literature on computing ethics. The outcomes of this analysis will form the basis for new teaching content that will be developed in future to help illuminate and address these concerns.\n\n\nOriginality/value\nThe multi-stakeholder analysis provides individual and differing perspectives on the issues related to the rapidly evolving discipline of computing ethics.\n", "venue": "Journal of Information, Communication and Ethics in Society", "year": 2021, "referenceCount": 45, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": null, "name": "Damian Gordon"}, {"authorId": "1702769", "name": "I. Stavrakakis"}, {"authorId": null, "name": "J. Paul Gibson"}, {"authorId": "145765360", "name": "Brendan Tierney"}, {"authorId": "1659094613", "name": "Anna Becevel"}, {"authorId": "1580631311", "name": "A. Curley"}, {"authorId": null, "name": "Michael Collins"}, {"authorId": "2102936806", "name": "William O'Mahony"}, {"authorId": "7368602", "name": "D. O'Sullivan"}]}}, {"contexts": ["\u2026can surface even from conclusive, transparent, and robust evidence; algorithmic activities that remythologize the world translating into \u201ctransformative effects,\u201d and the difficulties in detecting the harm, finding its cause and identifying who is responsible (Mittelstadt et al., 2016, pp. 4\u20125).", "Mittelstadt et al. (2016) identify six types of ethical concerns connected with digital computing.", "\u201cOperational parameters are specified by developers and configured by users (clients) with desired outcomes in mind that privilege some values and interests over others\u201d (Mittelstadt et al., 2016, p. 1)."], "isInfluential": true, "intents": ["background"], "citingPaper": {"paperId": "6e5070fa54bd659be67abec1b177da95e6e00b72", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/6e5070fa54bd659be67abec1b177da95e6e00b72", "title": "Algorithmic Gender Bias and Audiovisual Data: A Research Agenda", "abstract": "Algorithms are increasingly used to offer jobs, loans, medical care, and other services, as well as to influence behavior. Decisions that create the algorithms, the data sets that feed them, and the outputs that result from algorithmic decision making, can be biased, potentially harming women and perpetuating discrimination. Audiovisual data are especially challenging because of online growth and their impact on women\u2019s lives. While scholarship has acknowledged data divides and cases of algorithmic bias mostly in online texts, it has yet to explore the relevance of audiovisual content for gender algorithmic bias. Based on previous guidelines and literature on algorithmic bias, this article (a) connects different types of bias with factors and harmful outcomes for women; (b) examines challenges around the lack of clarity about which data are necessary for fair algorithmic decision making, the lack of understanding of how machine learning algorithms work, and the lack of incentives for corporations to correct bias; and (c) offers a research agenda to address algorithmic gender discrimination prevalent in audiovisual data.", "venue": "", "year": 2021, "referenceCount": 104, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "48033896", "name": "Miren Guti\u00e9rrez"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "70119192d3f5883ec62e1fd4d9e9d957cf180ca9", "externalIds": {"MAG": "3163534827", "DOI": "10.22201/FCPYS.2448492XE.2021.242.79322"}, "url": "https://www.semanticscholar.org/paper/70119192d3f5883ec62e1fd4d9e9d957cf180ca9", "title": "Nuevos desaf\u00edos para la rendici\u00f3n de cuentas en tiempos de pandemia: populismo y algoritmocracia", "abstract": "This article looks at how populism and artificial intelligence systems challenge the functioning of the institutional network of accountability in representative democracies. For the former, populist governments erode accountability; as for the latter, the institutional network fails to make artificial intelligence systems accountable. Currently, the available evidence indicates that both processes are independent. However, an eventual overlap would lead to accountability in hitherto unknown places.", "venue": "", "year": 2021, "referenceCount": 72, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "104850793", "name": "D. Avaro"}, {"authorId": "2123875928", "name": "Carlos Luis S\u00e1nchez y S\u00e1nchez"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7139f914b916f5bc95a3e5abc48830361c51de18", "externalIds": {"DOI": "10.1007/978-3-030-81907-1_10"}, "url": "https://www.semanticscholar.org/paper/7139f914b916f5bc95a3e5abc48830361c51de18", "title": "From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices", "abstract": null, "venue": "Philosophical Studies Series", "year": 2021, "referenceCount": 63, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1751614630", "name": "J. Morley"}, {"authorId": "2134633922", "name": "Luciano Floridi"}, {"authorId": "40901846", "name": "Libby Kinsey"}, {"authorId": "2600242", "name": "Anat Elhalal"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7279fd41f2c57b49e795255f413bea254475c6e2", "externalIds": {"DOI": "10.1007/s00146-021-01282-1"}, "url": "https://www.semanticscholar.org/paper/7279fd41f2c57b49e795255f413bea254475c6e2", "title": "Principle-based recommendations for big data and machine learning in food safety: the P-SAFETY model", "abstract": "Big data and Machine learning Techniques are reshaping the way in which food safety risk assessment is conducted. The ongoing \u2018datafication\u2019 of food safety risk assessment activities and the progressive deployment of probabilistic models in their practices requires a discussion on the advantages and disadvantages of these advances. In particular, the low level of trust in EU food safety risk assessment framework highlighted in 2019 by an EU-funded survey could be exacerbated by novel methods of analysis. The variety of processed data raises unique questions regarding the interplay of multiple regulatory systems alongside food safety legislation. Provisions aiming to preserve the confidentiality of data and protect personal information are juxtaposed to norms prescribing the public disclosure of scientific information. This research is intended to provide guidance for data governance and data ownership issues that unfold from the ongoing transformation of the technical and legal domains of food safety risk assessment. Following the reconstruction of technological advances in data collection and analysis and the description of recent amendments to food safety legislation, emerging concerns are discussed in light of the individual, collective and social implications of the deployment of cutting-edge Big Data collection and analysis techniques. Then, a set of principle-based recommendations is proposed by adapting high-level principles enshrined in institutional documents about Artificial Intelligence to the realm of food safety risk assessment. The proposed set of recommendations adopts Safety, Accountability, Fairness, Explainability, Transparency as core principles (SAFETY), whereas Privacy and data protection are used as a meta-principle.", "venue": "AI & SOCIETY", "year": 2021, "referenceCount": 71, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "51204499", "name": "Salvatore Sapienza"}, {"authorId": "2079711467", "name": "A. Vedder"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "72b952a54b27e8081acac086d314feeb13d85355", "externalIds": {"DBLP": "journals/clsr/Nowik21", "MAG": "3194797697", "DOI": "10.1016/J.CLSR.2021.105584"}, "url": "https://www.semanticscholar.org/paper/72b952a54b27e8081acac086d314feeb13d85355", "title": "Electronic personhood for artificial intelligence in the workplace", "abstract": "Abstract There are several legal and ethical problems associated with the far-reaching integration of man with Artificial Intelligence (AI) within the framework of algorithmic management. One of these problems is the question of the legal subjectivity of the parties to a contractual obligation within the framework of crowdworking, which includes the service provider, the Internet platform with AI, and the applicant's client. Crowdworking is an excellent example of a laboratory of interdependence and collaboration between humans and artificial intelligence as part of the algorithmic management process. Referring to the example of crowdworking platforms, we should ask whether, in the face of the rapid development of AI and algorithmic management, AI can be an employer equipped with electronic personhood? What characteristics does a work environment in which AI and algorithmic governance mechanisms play a dominant role? What kind of ethical implications are associated with the practical application of the concept of electronic subjectivity of AI in employment relations? This paper analyses the legal and ethical implications of electronic AI subjectivity in the work environment. The legal construction of electronic personhood is examined. The legal entity that uses AI, which manages the work process through algorithmic subordination, bears the risks resulting from such use (economic, personal, technical, and social) and full material responsibility (individual liability regime with the application of the presumption of guilt rule) in case of damage to an employee. Liability provisions can be complemented by a mandatory insurance scheme for AI users and a compensation fund that can offer support when none of the insurance policies covers the risk. A compensation fund can be paid for by the manufacturer, owner, user, or trainer of the AI and can compensate all those who suffer damage as a result of its operations. This is the direction proposed by the European Parliament, which has progressively called for robots to be given an electronic personality. The personalistic concept of work excludes the possibility of AI becoming a legal entity. Alongside legal arguments, ethical questions are of fundamental importance. The final part of the article presents the ethical implications of AI as an employer endowed with a legal entity (electronic personhood).", "venue": "Comput. Law Secur. Rev.", "year": 2021, "referenceCount": 52, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Business"], "authors": [{"authorId": "2132217063", "name": "Pawe\u0142 Nowik"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "733b5620c01b132a600bec730c3732369b0930c8", "externalIds": {"DOI": "10.1007/s43681-021-00114-8"}, "url": "https://www.semanticscholar.org/paper/733b5620c01b132a600bec730c3732369b0930c8", "title": "Artificial intelligence in research and development for sustainability: the centrality of explicability and research data management", "abstract": "Sustainability constitutes a focal challenge and objective of our time and requires collaborative efforts. As artificial intelligence brings forth substantial opportunities for innovations across industry and social contexts, so it provides innovation potential for pursuing sustainability. We argue that (chemical) research and development driven by artificial intelligence can substantially contribute to sustainability if it is leveraged in an ethical way. Therefore, we propose that the ethical principle explicability combined with (open) research data management systems should accompany artificial intelligence in research and development to foster sustainability in an equitable and collaborative way.", "venue": "AI and Ethics", "year": 2021, "referenceCount": 44, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2068775683", "name": "Erik Hermann"}, {"authorId": "2138091321", "name": "Gunter Hermann"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "73d60bc636be21204282a6ef851d16196d75b46f", "externalIds": {"MAG": "3190013839", "DOI": "10.1007/s10676-021-09606-x"}, "url": "https://www.semanticscholar.org/paper/73d60bc636be21204282a6ef851d16196d75b46f", "title": "Predictive privacy: towards an applied ethics of data analytics", "abstract": "Data analytics and data-driven approaches in Machine Learning are now among the most hailed computing technologies in many industrial domains. One major application is predictive analytics, which is used to predict sensitive attributes, future behavior, or cost, risk and utility functions associated with target groups or individuals based on large sets of behavioral and usage data. This paper stresses the severe ethical and data protection implications of predictive analytics if it is used to predict sensitive information about single individuals or treat individuals differently based on the data many unrelated individuals provided. To tackle these concerns in an applied ethics, first, the paper introduces the concept of \u201cpredictive privacy\u201d to formulate an ethical principle protecting individuals and groups against differential treatment based on Machine Learning and Big Data analytics. Secondly, it analyses the typical data processing cycle of predictive systems to provide a step-by-step discussion of ethical implications, locating occurrences of predictive privacy violations. Thirdly, the paper sheds light on what is qualitatively new in the way predictive analytics challenges ethical principles such as human dignity and the (liberal) notion of individual privacy. These new challenges arise when predictive systems transform statistical inferences, which provide knowledge about the cohort of training data donors, into individual predictions, thereby crossing what I call the \u201cprediction gap\u201d. Finally, the paper summarizes that data protection in the age of predictive analytics is a collective matter as we face situations where an individual\u2019s (or group\u2019s) privacy is violated using data other individuals provide about themselves, possibly even anonymously.", "venue": "Ethics and Information Technology", "year": 2021, "referenceCount": 45, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "101646061", "name": "Rainer M\u00fchlhoff"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "743a42c3899efa2d26b80d8746d7fb013081da85", "externalIds": {"PubMedCentral": "7973859", "DOI": "10.1016/j.isci.2021.102249", "PubMed": "33763636"}, "url": "https://www.semanticscholar.org/paper/743a42c3899efa2d26b80d8746d7fb013081da85", "title": "Ethical machines: The human-centric use of artificial intelligence", "abstract": "Summary Today's increased availability of large amounts of human behavioral data and advances in artificial intelligence (AI) are contributing to a growing reliance on algorithms to make consequential decisions for humans, including those related to access to credit or medical treatments, hiring, etc. Algorithmic decision-making processes might lead to more objective decisions than those made by humans who may be influenced by prejudice, conflicts of interest, or fatigue. However, algorithmic decision-making has been criticized for its potential to lead to privacy invasion, information asymmetry, opacity, and discrimination. In this paper, we describe available technical solutions in three large areas that we consider to be of critical importance to achieve a human-centric AI: (1) privacy and data ownership; (2) accountability and transparency; and (3) fairness. We also highlight the criticality and urgency to engage multi-disciplinary teams of researchers, practitioners, policy makers, and citizens to co-develop and evaluate in the real-world algorithmic decision-making processes designed to maximize fairness, accountability, and transparency while respecting privacy.", "venue": "iScience", "year": 2021, "referenceCount": 390, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "1776476", "name": "B. Lepri"}, {"authorId": "2060131247", "name": "N. Oliver"}, {"authorId": "1682773", "name": "A. Pentland"}]}}, {"contexts": ["This array of technologies prompts considerable hopes for enhanced decision-making, informed planning, and responsive services, but these hopes are accompanied by risks such as privacy intrusion, loss of autonomy, bias, and opaque decision-making (Mittelstadt et al., 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "746dd0ed2004e8ac6d7084e1e671eccefec3fb85", "externalIds": {"MAG": "3169783368", "DOI": "10.1177/17470161211022790"}, "url": "https://www.semanticscholar.org/paper/746dd0ed2004e8ac6d7084e1e671eccefec3fb85", "title": "Evaluating the prospects for university-based ethical governance in artificial intelligence and data-driven innovation", "abstract": "There has been considerable debate around the ethical issues raised by data-driven technologies such as artificial intelligence. Ethical principles for the field have focused on the need to ensure that such technologies are used for good rather than harm, that they enshrine principles of social justice and fairness, that they protect privacy, respect human autonomy and are open to scrutiny. While development of such principles is well advanced, there is as yet little consensus on the mechanisms appropriate for ethical governance in this field. This paper examines the prospects for the university ethics committee to undertake effective review of research conducted on data-driven technologies in the university context. Challenges identified include: the relatively narrow focus of university-based ethical review on the human subjects research process and lack of capacity to anticipate downstream impacts; the difficulties of accommodating the complex interplay of academic and commercial interests in the field; and the need to ensure appropriate expertise from both specialists and lay voices. Overall, the challenges identified sharpen appreciation of the need to encourage a joined-up and effective system of ethical governance that fosters an ethical culture rather than replacing ethical reflection with bureaucracy.", "venue": "Research Ethics", "year": 2021, "referenceCount": 49, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "145202971", "name": "C. Hine"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "758289be073e7d4e6be6e6e4a7f937ef48ae81bd", "externalIds": {"MAG": "3173496715", "DOI": "10.1007/s10796-021-10146-4"}, "url": "https://www.semanticscholar.org/paper/758289be073e7d4e6be6e6e4a7f937ef48ae81bd", "title": "Responsible AI for Digital Health: a Synthesis and a Research Agenda", "abstract": "Responsible AI is concerned with the design, implementation and use of ethical, transparent, and accountable AI technology in order to reduce biases, promote fairness, equality, and to help facilitate interpretability and explainability of outcomes, which are particularly pertinent in a healthcare context. However, the extant literature on health AI reveals significant issues regarding each of the areas of responsible AI, posing moral and ethical consequences. This is particularly concerning in a health context where lives are at stake and where there are significant sensitivities that are not as pertinent in other domains outside of health. This calls for a comprehensive analysis of health AI using responsible AI concepts as a structural lens. A systematic literature review supported our data collection and sampling procedure, the corresponding analysis, and extraction of research themes helped us provide an evidence-based foundation. We contribute with\u00a0a systematic description and explanation of the intellectual structure of Responsible AI in digital health and develop an agenda for future research.", "venue": "Information Systems Frontiers", "year": 2021, "referenceCount": 85, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "52168559", "name": "Cristina Trocin"}, {"authorId": "2424691", "name": "Patrick Mikalef"}, {"authorId": "1995857", "name": "Z. Papamitsiou"}, {"authorId": "2032848", "name": "K. Conboy"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "76a5c61513264be2630a337f3d1251d4ba08b1da", "externalIds": {"MAG": "3135493340", "DOI": "10.1007/978-3-030-64590-8_1"}, "url": "https://www.semanticscholar.org/paper/76a5c61513264be2630a337f3d1251d4ba08b1da", "title": "Introduction: Ethical Issues of Neurotechnologies and Artificial Intelligence", "abstract": null, "venue": "", "year": 2021, "referenceCount": 54, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "48685400", "name": "O. Friedrich"}, {"authorId": "145331865", "name": "Andreas Wolkenstein"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "76b2d2f4924ba6d701c94cc96fc962ecb25c746c", "externalIds": {"MAG": "3172320947", "DOI": "10.1007/S10790-021-09835-9"}, "url": "https://www.semanticscholar.org/paper/76b2d2f4924ba6d701c94cc96fc962ecb25c746c", "title": "Sentencing Disparity and Artificial Intelligence", "abstract": "The idea of using artificial intelligence as a support system in the sentencing process has attracted increasing attention. For instance, it has been suggested that machine learning algorithms may help in curbing problems concerning inter-judge sentencing disparity. The purpose of the present article is to examine the merits of this possibility. It is argued that, insofar as the unfairness of sentencing disparity is held to reflect a retributivist view of proportionality, it is not necessarily the case that increasing inter-judge uniformity in sentencing is desirable. More generally, it is shown that the idea of introducing machine learning algorithms, that produce sentencing predictions on the ground of a dataset that is built of previous sentencing decisions, faces serious problems if there exists a discrepancy between actual sentencing practice and the sentences that are ideally desirable.", "venue": "The Journal of Value Inquiry", "year": 2021, "referenceCount": 47, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "14041373", "name": "J. Ryberg"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "77f95fa63d99d3125523639f2a11d0b4aa649343", "externalIds": {"DBLP": "journals/corr/abs-2102-04256", "ArXiv": "2102.04256"}, "url": "https://www.semanticscholar.org/paper/77f95fa63d99d3125523639f2a11d0b4aa649343", "title": "Problematic Machine Behavior: A Systematic Literature Review of Algorithm Audits", "abstract": "While algorithm audits are growing rapidly in commonality and public importance, relatively little scholarly work has gone toward synthesizing prior work and strategizing future research in the area. This systematic literature review aims to do just that, following PRISMAguidelines in a review of over 500 English articles that yielded 62 algorithm audit studies. The studies are synthesized and organized primarily by behavior (discrimination, distortion, exploitation, and misjudgement), with codes also provided for domain (e.g. search, vision, advertising, etc.), organization (e.g. Google, Facebook, Amazon, etc.), and audit method (e.g. sock puppet, direct scrape, crowdsourcing, etc.). The review shows how previous audit studies have exposed public-facing algorithms exhibiting problematic behavior, such as search algorithms culpable of distortion and advertising algorithms culpable of discrimination. Based on the studies reviewed, it also suggests some behaviors (e.g. discrimination on the basis of intersectional identities), domains (e.g. advertising algorithms), methods (e.g. code auditing), and organizations (e.g. Twitter, TikTok, LinkedIn) that call for future audit attention. The paper concludes by offering the common ingredients of successful audits, and discussing algorithm auditing in the context of broader research working toward algorithmic justice.", "venue": "ArXiv", "year": 2021, "referenceCount": 164, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3363483", "name": "Jack Bandy"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "797ee549169316ac62f79a85b4537a0fca11b60c", "externalIds": {"DOI": "10.1515/iwp-2021-2178"}, "url": "https://www.semanticscholar.org/paper/797ee549169316ac62f79a85b4537a0fca11b60c", "title": "Data Ethics Frameworks", "abstract": "Zusammenfassung Zuletzt ver\u00f6ffentlichten viele Organisationen ethische Richtlinien, um ihre Haltung gegen Diskriminierung durch Algorithmen zu betonen. Vier dieser Frameworks werden mithilfe der kritischen Diskursanalyse untersucht. Ziel ist es, die darin vermittelten Werte und Wertkonflikte zu identifizieren. Die Ergebnisse weisen darauf hin, dass etablierte Werte aus der Computer- und Informationsethik aufgegriffen und bestehende Machtstrukturen zwischen Akteuren verst\u00e4rkt werden.", "venue": "Information - Wissenschaft & Praxis", "year": 2021, "referenceCount": 18, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2123032827", "name": "Helena H\u00e4u\u00dfler"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7c4ddc2435b33b7452529cebc46637a86bbc9434", "externalIds": {"ArXiv": "1907.08228", "DOI": "10.1093/poq/nfab018"}, "url": "https://www.semanticscholar.org/paper/7c4ddc2435b33b7452529cebc46637a86bbc9434", "title": "A Total Error Framework for Digital Traces of Human Behavior on Online Platforms", "abstract": "\n People\u2019s activities and opinions recorded as digital traces online, especially on social media and other web-based platforms, offer increasingly informative pictures of the public. They promise to allow inferences about populations beyond the users of the platforms on which the traces are recorded, representing real potential for the social sciences and a complement to survey-based research. But the use of digital traces brings its own complexities and new error sources to the research enterprise. Recently, researchers have begun to discuss the errors that can occur when digital traces are used to learn about humans and social phenomena. This article synthesizes this discussion and proposes a systematic way to categorize potential errors, inspired by the Total Survey Error (TSE) framework developed for survey methodology. We introduce a conceptual framework to diagnose, understand, and document errors that may occur in studies based on such digital traces. While there are clear parallels to the well-known error sources in the TSE framework, the new \u201cTotal Error Framework for Digital Traces of Human Behavior on Online Platforms\u201d (TED-On) identifies several types of error that are specific to the use of digital traces. By providing a standard vocabulary to describe these errors, the proposed framework is intended to advance communication and research about using digital traces in scientific social research.", "venue": "Public Opinion Quarterly", "year": 2019, "referenceCount": 76, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "33770417", "name": "Indira Sen"}, {"authorId": "1724463", "name": "Fabian Fl\u00f6ck"}, {"authorId": "46417808", "name": "Katrin Weller"}, {"authorId": "2054519101", "name": "Bernd Weiss"}, {"authorId": "144065562", "name": "Claudia Wagner"}]}}, {"contexts": ["These dynamics raise major ethical questions, including notably the broader shift from human to algorithmic decision-making (Mittelstadt et al., 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "7de53d8ed81a7f7cac280a93cddc5b0a985aea95", "externalIds": {"MAG": "3157830328", "DOI": "10.1177/20539517211013051"}, "url": "https://www.semanticscholar.org/paper/7de53d8ed81a7f7cac280a93cddc5b0a985aea95", "title": "Big Data in the workplace: Privacy Due Diligence as a human rights-based approach to employee privacy protection", "abstract": "Data-driven technologies have come to pervade almost every aspect of business life, extending to employee monitoring and algorithmic management. How can employee privacy be protected in the age of datafication? This article surveys the potential and shortcomings of a number of legal and technical solutions to show the advantages of human rights-based approaches in addressing corporate responsibility to respect privacy and strengthen human agency. Based on this notion, we develop a process-oriented model of Privacy Due Diligence to complement existing frameworks for safeguarding employee privacy in an era of Big Data surveillance.", "venue": "", "year": 2021, "referenceCount": 122, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Business"], "authors": [{"authorId": "101852064", "name": "Isabel Ebert"}, {"authorId": "2060097150", "name": "Isabelle Wildhaber"}, {"authorId": "1411026500", "name": "Jeremias Adams-Prassl"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7f867c7f4b67961c21507e959d13ecd5af949179", "externalIds": {"MAG": "3161181201", "DOI": "10.1016/J.HRMR.2021.100838"}, "url": "https://www.semanticscholar.org/paper/7f867c7f4b67961c21507e959d13ecd5af949179", "title": "Algorithms as work designers: How algorithmic management influences the design of jobs", "abstract": "Abstract We review the literature on algorithmic management (AM) to bridge the gap between this emerging research area and the well-established theory and research on work design. First, we identify six management functions that algorithms are currently able to perform: monitoring, goal setting, performance management, scheduling, compensation, and job termination. Second, we show how each AM function affects key job resources (e.g., job autonomy, job complexity) and key job demands (e.g., workload, physical demands); with each of these resources and demands being important drivers of worker motivation and their well-being. Third, rejecting a deterministic perspective and drawing on sociotechnical systems theory, we outline key categories of variables that moderate the link between AM on work design, namely transparency, fairness and human influence (e.g., whether workers can control the system). We summarize our review in the form of a model to help guide research on AM, and to support practitioners and designers in the creation and maintenance of meaningful jobs in the era of algorithms.", "venue": "", "year": 2021, "referenceCount": 223, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1413517130", "name": "Xavier Parent-Rocheleau"}, {"authorId": "2109889", "name": "S. Parker"}]}}, {"contexts": ["Given the substantial impact of AI on the individual, economic, and societal level, AI development and use are accompanied by intensive discussions of guiding ethical principles by public and private institutions (Cowls et al., 2021; Floridi et al., 2018, 2020; Hagendorff, 2020; Jobin et al., 2019; Mittelstadt, 2019; Mittelstadt et al., 2016; Morley et al., 2020).", "From an external perspective, the intelligibility principle is regularly limited or diluted by proprietary boundaries and intellectual property right restrictions in case of commercial product development (e.g., Ananny & Crawford, 2018; Mittelstadt et\u00a0 al., 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "7fb7721cabd6dfa7d7f0b263efa1cb83acf7ce87", "externalIds": {"PubMedCentral": "8260511", "DBLP": "journals/see/HermannHT21", "DOI": "10.1007/s11948-021-00325-6", "PubMed": "34231042"}, "url": "https://www.semanticscholar.org/paper/7fb7721cabd6dfa7d7f0b263efa1cb83acf7ce87", "title": "Ethical Artificial Intelligence in Chemical Research and Development: A Dual Advantage for Sustainability", "abstract": "Artificial intelligence can be a game changer to address the global challenge of humanity-threatening climate change by fostering sustainable development. Since chemical research and development lay the foundation for innovative products and solutions, this study presents a novel chemical research and development process backed with artificial intelligence and guiding ethical principles to account for both process- and outcome-related sustainability. Particularly in ethically salient contexts, ethical principles have to accompany research and development powered by artificial intelligence to promote social and environmental good and sustainability (beneficence) while preventing any harm (non-maleficence) for all stakeholders (i.e., companies, individuals, society at large) affected.", "venue": "Sci. Eng. Ethics", "year": 2021, "referenceCount": 101, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2068775683", "name": "Erik Hermann"}, {"authorId": "12274887", "name": "G. Hermann"}, {"authorId": "3408159", "name": "J. Tremblay"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "800a6a2bbcdde5c635a1adf9e5fc7e2b359e4e5d", "externalIds": {"DOI": "10.1016/j.chbr.2021.100144"}, "url": "https://www.semanticscholar.org/paper/800a6a2bbcdde5c635a1adf9e5fc7e2b359e4e5d", "title": "When is personalized advertising crossing personal boundaries? How type of information, data sharing, and personalized pricing influence consumer perceptions of personalized advertising", "abstract": null, "venue": "Computers in Human Behavior Reports", "year": 2021, "referenceCount": 64, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "3423820", "name": "S. C. Boerman"}, {"authorId": "3235987", "name": "S. Kruikemeier"}, {"authorId": "2582137", "name": "N. Bol"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "8118898bc9e25a86a6e9f9ed7fe59fe4dbde7102", "externalIds": {"MAG": "3123693488", "DOI": "10.1093/BJC/AZAA099"}, "url": "https://www.semanticscholar.org/paper/8118898bc9e25a86a6e9f9ed7fe59fe4dbde7102", "title": "When Politicization Stops Algorithms in Criminal Justice", "abstract": "\n The present paper sheds light on the conditions for successfully implementing pretrial risk assessment tools by looking at barriers to their implementation and, specifically, political factors that can derail this process. Drawing on theoretical assumptions about the role of agenda-setting and public discourse in policy change, we examine three US states where the implementation of pretrial tools was halted or heavily restricted. Our findings suggest that a politicization of these tools plays a central role for thwarting their implementation. This has less to do with their technical properties or performance. Rather, through reaching high-level politics, those tools are publicly linked to concerns about opacity, fairness and public safety such that it simply becomes politically safer for politicians to stick with the status quo.", "venue": "", "year": 2021, "referenceCount": 89, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "90468386", "name": "Pascal D. K\u00f6nig"}, {"authorId": "14198009", "name": "Georg Wenzelburger"}]}}, {"contexts": ["The problem affects any algorithmic application that supports decision-making and it is well known and debated in the ethic, social and legal communities [109]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "8142c7a852eed9c5df99c33915acd5601e6e221b", "externalIds": {"DBLP": "journals/corr/abs-2109-09658", "ArXiv": "2109.09658"}, "url": "https://www.semanticscholar.org/paper/8142c7a852eed9c5df99c33915acd5601e6e221b", "title": "FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging", "abstract": "The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today\u2019s clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices from five large European projects on AI in Health Imaging. These guiding principles are named FUTURE-AI and its building blocks consist of (i) Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness and (vi) Explainability. In a step-by-step approach, these guidelines are further translated into a framework of concrete recommendations for specifying, developing, evaluating, and deploying technically, clinically and ethically trustworthy AI solutions into clinical practice.", "venue": "ArXiv", "year": 2021, "referenceCount": 199, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2127471605", "name": "Karim Lekadira"}, {"authorId": "10417336", "name": "Richard Osuala"}, {"authorId": "108139887", "name": "C. Gallin"}, {"authorId": "98019659", "name": "Noussair Lazrak"}, {"authorId": "26355925", "name": "Kaisar Kushibar"}, {"authorId": "2209674", "name": "Gianna Tsakou"}, {"authorId": "2127472933", "name": "Susanna Auss'o"}, {"authorId": "2127474682", "name": "Leonor Cerd'a Alberich"}, {"authorId": "32314596", "name": "K. Marias"}, {"authorId": "2127472016", "name": "Manolis Tskinakis"}, {"authorId": "2522681", "name": "S. Colantonio"}, {"authorId": "37830154", "name": "Nickolas Papanikolaou"}, {"authorId": "1983771933", "name": "Zohaib Salahuddin"}, {"authorId": "48000039", "name": "H. Woodruff"}, {"authorId": "1693233", "name": "P. Lambin"}, {"authorId": "2127472303", "name": "Luis Mart'i-Bonmat'i"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "81b1dacec791a6412fdaa05406dc3d65256da553", "externalIds": {"MAG": "3196654640", "DOI": "10.1007/s40593-021-00270-2"}, "url": "https://www.semanticscholar.org/paper/81b1dacec791a6412fdaa05406dc3d65256da553", "title": "Education for AI, not AI for Education: The Role of Education and Ethics in National AI Policy Strategies", "abstract": null, "venue": "International Journal of Artificial Intelligence in Education", "year": 2021, "referenceCount": 127, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "47388653", "name": "D. Schiff"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "81e396a870ef43ac6fd07eeb461df5dced218c53", "externalIds": {"DOI": "10.1044/2021_persp-21-00126"}, "url": "https://www.semanticscholar.org/paper/81e396a870ef43ac6fd07eeb461df5dced218c53", "title": "Using Nominal Group Technique to Identify Key Ethical Concerns Regarding Hearing Aids With Machine Learning", "abstract": "\n \n Machine learning (ML) in new-generation hearing aid technology presents a beneficial opportunity for development in audiology. It is, however, important to balance new applications against the audiologist's professional ethics to protect the client from any harm. This study aimed to identify the key ethical concerns related to the latest digital hearing aid technology that incorporates ML that could potentially impact the client and/or the audiologist.\n \n \n \n A nominal group technique was conducted with the audiologist to generate and prioritize a list of key ethical concerns related to human data interaction, with specific focus on hearing aids.\n \n \n \n Five categories were identified in relation to the potential impact on the client: (a) privacy and confidentiality, (b) relationship and trust, (c) nonmaleficence, (d) informed decision making, and (e) financial gain. An additional five categories were identified in relation to the potential impact on the audiologist: (a) privacy and confidentiality, (b) professional responsibility, (c) relationship and trust, (d) financial gain, and (e) trust technology. Privacy and confidentiality were ranked as the highest priority that should be considered when supplying clients with the latest hearing aid technology.\n \n \n \n Hearing aid technology has evolved considerably, and audiologists need to keep abreast of and master the general technological developments and the associated ethical challenges that may arise. Discussions on the ethics related to ML and hearing aid fittings will help identify the key ethical concerns involved and, thereby, enhance the ethical sensitivity of the profession.\n", "venue": "Perspectives of the ASHA Special Interest Groups", "year": 2021, "referenceCount": 17, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "144732127", "name": "A. Naud\u00e9"}, {"authorId": "6984322", "name": "J. Bornman"}]}}, {"contexts": ["rithms and our understanding of their ethical implications can have severe consequences affecting individuals as well as groups and whole societies\u201d [58]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "8afe359953194a5daea47ce9dc44ba51031592db", "externalIds": {"ArXiv": "2109.05658", "DBLP": "journals/corr/abs-2109-05658"}, "url": "https://www.semanticscholar.org/paper/8afe359953194a5daea47ce9dc44ba51031592db", "title": "Measurement as governance in and for responsible AI", "abstract": "Measurement of social phenomena is everywhere, unavoidably, in sociotechnical systems. This is not (only) an academic point: Fairness-related harms emerge when there is a mismatch in the measurement process between the thing we purport to be measuring and the thing we actually measure. However, the measurement process\u2014where social, cultural, and political values are implicitly encoded in sociotechnical systems\u2014is almost always obscured. Furthermore, this obscured process is where important governance decisions are encoded: governance about which systems are fair, which individuals belong in which categories, and so on. We can then use the language of measurement, and the tools of construct validity and reliability, to uncover hidden governance decisions. In particular, we highlight two types of construct validity, content validity and consequential validity, that are useful to elicit and characterize the feedback loops between the measurement, social construction, and enforcement of social categories. We then explore the constructs of fairness, robustness, and responsibility in the context of governance in and for responsible AI. Together, these perspectives help us unpack how measurement acts as a hidden governance process in sociotechnical systems. Understanding measurement as governance supports a richer understanding of the governance processes already happening in AI\u2014responsible or otherwise\u2014revealing paths to more effective interventions.", "venue": "ArXiv", "year": 2021, "referenceCount": 86, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "47786922", "name": "Abigail Z. Jacobs"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "8b3c67299d9a69fec6eef681648a7b8c560958fb", "externalIds": {"MAG": "3129445530", "DOI": "10.1111/SOC4.12851"}, "url": "https://www.semanticscholar.org/paper/8b3c67299d9a69fec6eef681648a7b8c560958fb", "title": "Sociological perspectives on artificial intelligence: A typological reading", "abstract": null, "venue": "", "year": 2021, "referenceCount": 58, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": null, "name": "Zheng Liu"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "9168c5a5671e68a3542280f9b8ee9edfbc31c4e5", "externalIds": {"DOI": "10.1080/00346764.2021.1979241"}, "url": "https://www.semanticscholar.org/paper/9168c5a5671e68a3542280f9b8ee9edfbc31c4e5", "title": "How intelligent neurotechnology can be epistemically unjust. An exploration into the ethics of algorithms", "abstract": null, "venue": "Review of Social Economy", "year": 2021, "referenceCount": 54, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "6466790", "name": "Sebastian Schleidgen"}, {"authorId": "48685400", "name": "O. Friedrich"}, {"authorId": "145331865", "name": "Andreas Wolkenstein"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "92db7c39a1bed82afd66922012d530d79acf18ce", "externalIds": {"DBLP": "conf/fat/BarbosaBSL21", "DOI": "10.1145/3442188.3445900"}, "url": "https://www.semanticscholar.org/paper/92db7c39a1bed82afd66922012d530d79acf18ce", "title": "A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development", "abstract": "One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.", "venue": "FAccT", "year": 2021, "referenceCount": 101, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145337089", "name": "S. Barbosa"}, {"authorId": "153788111", "name": "G. D. J. Barbosa"}, {"authorId": "1698433", "name": "C. D. Souza"}, {"authorId": "1859174", "name": "C. Leit\u00e3o"}]}}, {"contexts": ["prevent regulatory arbitrage, remove human bias, and reduce inconsistency in decision-making (Halachmi, & Greiling, 2013; Mittelstadt et al., 2016; Crawford & Calo, 2016; Wieringa, 2020, Bolander 2020; Chiao, 2019).", "Algorithms are linked to increase unfair outcomes and to introduce biases in\nthe decision making process (O\u2019Neill, 2016; Mittelstadt et al., 2016; Crawford & Schultz, 2014).", "However, sigma-type value gains might come at a cost of theta-type (unfair, biased and discriminatory practices) and lambda-type values (less resilience and robustness) (O\u2019Neill, 2016; Mittelstadt et al., 2016; Fry, 2018; Parnas, 1994).", "\u2026more accountable and rule out adverse effects of human decision making e.g. prevent regulatory arbitrage, remove human bias, and reduce inconsistency in decision-making (Halachmi, & Greiling, 2013; Mittelstadt et al., 2016; Crawford & Calo, 2016; Wieringa, 2020, Bolander 2020; Chiao, 2019)."], "isInfluential": true, "intents": ["background"], "citingPaper": {"paperId": "942572b63d93b026178e00f23b0e3432185a9e57", "externalIds": {"MAG": "3187554972", "DOI": "10.4018/ijpada.20210101.oa9"}, "url": "https://www.semanticscholar.org/paper/942572b63d93b026178e00f23b0e3432185a9e57", "title": "Managing Algorithms for Public Value", "abstract": "Public organisations increasingly rely on machine learning algorithms in performing many of their core activities. It is therefore important to consider how algorithms are transforming the public sector. This article aims to clarify this by assessing algorithms from a public value perspective. Based on a discussion of the literature, it is demonstrated that algorithms are generally expected to strengthen organisational performance on a first cluster of values related to the ability to be effective and efficient (sigma values). At the same time, the use of algorithms is linked to negatively affect a second cluster of values that involves fairness and transparency (theta values). In the current academic debate little attention is given to an important third cluster of values; the ability of organisations to be adaptive and robust (lambda values). This discussion highlights that algorithms invoke public value opportunities, but also public value risks and trade-offs. This article therefore presents five principles for managing algorithms from a public value perspective.", "venue": "International Journal of Public Administration in the Digital Age", "year": 2021, "referenceCount": 109, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1381831714", "name": "Friso Selten"}, {"authorId": "46679956", "name": "A. Meijer"}]}}, {"contexts": ["Explicability (particularly, intelligibility) gains in importance against the backdrop of the black box nature and opacity of AI systems and applications (e.g., Ananny & Crawford, 2018; Milano et al., 2020; Mittelstadt et al., 2016; Rudin, 2019; Thiebes et al., 2020), since black box AI could thwart evaluations of beneficence, non-maleficence, justice, and autonomy.", "That can be the case when causality is not established prior to actions, and actions are then directed to individuals, although the knowledge generated concerns populations (Mittelstadt et al., 2016).", "Explicability might be the most prevalent and controversial principle of AI ethics due to the black box nature of AI systems, their opacity, and lack of accountability (Ananny & Crawford, 2018; Milano et al., 2020; Mittelstadt et al., 2016; Rai, 2020; Rudin, 2019; Thiebes et al., 2020)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "94c4a4a8d8629e6b789c2e5c7744a2f1ec5fdd0b", "externalIds": {"PubMedCentral": "8150633", "DOI": "10.1007/s10551-021-04843-y", "PubMed": "34054170"}, "url": "https://www.semanticscholar.org/paper/94c4a4a8d8629e6b789c2e5c7744a2f1ec5fdd0b", "title": "Leveraging Artificial Intelligence in Marketing for Social Good\u2014An Ethical Perspective", "abstract": "Artificial intelligence (AI) is (re)shaping strategy, activities, interactions, and relationships in business and specifically in marketing. The drawback of the substantial opportunities AI systems and applications (will) provide in marketing are ethical controversies. Building on the literature on AI ethics, the authors systematically scrutinize the ethical challenges of deploying AI in marketing from a multi-stakeholder perspective. By revealing interdependencies and tensions between ethical principles, the authors shed light on the applicability of a purely principled, deontological approach to AI ethics in marketing. To reconcile some of these tensions and account for the AI-for-social-good perspective, the authors make suggestions of how AI in marketing can be leveraged to promote societal and environmental well-being.", "venue": "Journal of business ethics : JBE", "year": 2021, "referenceCount": 191, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "2068775683", "name": "Erik Hermann"}]}}]}