{"offset": 0, "next": 100, "data": [{"contexts": ["The search for causal links is difficult, as correlations established in large, proprietary datasets are frequently not reproducible or falsifiable (cf. Ioannidis, 2005; Lazer et al., 2014)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "00d681cfff7f0224ace607e515f4ed510e792df9", "externalIds": {"MAG": "2068181924", "DOI": "10.1126/science.1248506", "PubMed": "24626916"}, "url": "https://www.semanticscholar.org/paper/00d681cfff7f0224ace607e515f4ed510e792df9", "title": "The Parable of Google Flu: Traps in Big Data Analysis", "abstract": "Large errors in flu prediction were largely avoidable, which offers lessons for the use of big data. In February 2013, Google Flu Trends (GFT) made headlines but not for a reason that Google executives or the creators of the flu tracking system would have hoped. Nature reported that GFT was predicting more than double the proportion of doctor visits for influenza-like illness (ILI) than the Centers for Disease Control and Prevention (CDC), which bases its estimates on surveillance reports from laboratories across the United States (1, 2). This happened despite the fact that GFT was built to predict CDC reports. Given that GFT is often held up as an exemplary use of big data (3, 4), what lessons can we draw from this error?", "venue": "Science", "year": 2014, "referenceCount": 77, "citationCount": 1798, "influentialCitationCount": 56, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "3185333", "name": "D. Lazer"}, {"authorId": "40379784", "name": "Ryan P. Kennedy"}, {"authorId": "2105789236", "name": "Gary King"}, {"authorId": "80273596", "name": "A. Vespignani"}]}}, {"contexts": ["We hear, in the news, of \u2018algorithms\u2019 that suggest potential mates for single people and algorithms that detect trends of financial benefit to marketers, with the implication that these algorithms may be right or wrong. . .\u2019\u2019 (Hill, 2015: 36)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "02c5e6d5fc315aac39e7391ae22c67b4e9251cc9", "externalIds": {"MAG": "2013371377", "DOI": "10.1007/S13347-014-0184-5"}, "url": "https://www.semanticscholar.org/paper/02c5e6d5fc315aac39e7391ae22c67b4e9251cc9", "title": "What an Algorithm Is", "abstract": "The algorithm, a building block of computer science, is defined from an intuitive and pragmatic point of view, through a methodological lens of philosophy rather than that of formal computation. The treatment extracts properties of abstraction, control, structure, finiteness, effective mechanism, and imperativity, and intentional aspects of goal and preconditions. The focus on the algorithm as a robust conceptual object obviates issues of correctness and minimality. Neither the articulation of an algorithm nor the dynamic process constitute the algorithm itself. Analysis for implications in computer science and philosophy reveals unexpected results, new questions, and new perspectives on current questions, including the relationship between our informally construed algorithms and Turing machines. Exploration in terms of current computational and philosophical thinking invites further developments.", "venue": "", "year": 2016, "referenceCount": 24, "citationCount": 52, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "35036096", "name": "Robin K. Hill"}]}}, {"contexts": ["However, trust can also exist among artificial agents exclusively, seen for instance in the agents of a distributed system working cooperatively to achieve a given goal (Grodzinsky et al., 2010; Simon, 2010; Taddeo, 2010).", "Trust implies the trustor\u2019s (the agent who trusts) expectations for the trustee (the agent who is trusted) to perform a task (Taddeo, 2010), and acceptance of the risk that the trustee will betray these expectations (Wiegel and Berg, 2009)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "0304f661e8b09b83585a36b3c4571e85352dd8e6", "externalIds": {"DBLP": "journals/mima/Taddeo10", "MAG": "2044592362", "DOI": "10.1007/s11023-010-9201-3"}, "url": "https://www.semanticscholar.org/paper/0304f661e8b09b83585a36b3c4571e85352dd8e6", "title": "Modelling Trust in Artificial Agents, A First Step Toward the Analysis of e-Trust", "abstract": "This paper provides a new analysis of e-trust, trust occurring in digital contexts, among the artificial agents of a distributed artificial system. The analysis endorses a non-psychological approach and rests on a Kantian regulative ideal of a rational agent, able to choose the best option for itself, given a specific scenario and a goal to achieve. The paper first introduces e-trust describing its relevance for the contemporary society and then presents a new theoretical analysis of this phenomenon. The analysis first focuses on an agent\u2019s trustworthiness, this one is presented as the necessary requirement for e-trust to occur. Then, a new definition of e-trust as a second-order-property of first-order relations is presented. It is shown that the second-order-property of e-trust has the effect of minimising an agent\u2019s effort and commitment in the achievement of a given goal. On this basis, a method is provided for the objective assessment of the levels of e-trust occurring among the artificial agents of a distributed artificial system.", "venue": "Minds and Machines", "year": 2010, "referenceCount": 23, "citationCount": 69, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2084659", "name": "M. Taddeo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "05c08de1bf91cd52ea1d22e7238e33958b574a23", "externalIds": {"MAG": "2087710120", "DOI": "10.1007/s10676-004-3422-1"}, "url": "https://www.semanticscholar.org/paper/05c08de1bf91cd52ea1d22e7238e33958b574a23", "title": "The responsibility gap: Ascribing responsibility for the actions of learning automata", "abstract": "Traditionally, the manufacturer/operator of a machine is held (morally and legally) responsible for the consequences of its operation. Autonomous, learning machines, based on neural networks, genetic algorithms and agent architectures, create a new situation, where the manufacturer/operator of the machine is in principle not capable of predicting the future machine behaviour any more, and thus cannot be held morally responsible or liable for it. The society must decide between not using this kind of machine any more (which is not a realistic option), or facing a responsibility gap, which cannot be bridged by traditional concepts of responsibility ascription.", "venue": "Ethics and Information Technology", "year": 2004, "referenceCount": 21, "citationCount": 320, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "40558676", "name": "A. Matthias"}]}}, {"contexts": ["\u2026of responsibilities, this highlights the complex relation between good design (the re-use philosophy promoted in Structured Programming) and the absence of malfunction, and reveals that even the designers of softwareartefacts regularly treat part of their work as black\nboxes (Sametinger, 1997)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "071e7c5701b73563117316880df4a31fa141f9ab", "externalIds": {"MAG": "1715846772", "DBLP": "books/daglib/0088244", "DOI": "10.1007/978-3-662-03345-6"}, "url": "https://www.semanticscholar.org/paper/071e7c5701b73563117316880df4a31fa141f9ab", "title": "Software Engineering with Reusable Components", "abstract": "The book provides a clear understanding of what software reuse is, where the problems are, what benefits to expect, the activities, and its different forms. The reader is also given an overview of what sofware components are, different kinds of components and compositions, a taxonomy thereof, and examples of successful component reuse. An introduction to software engineering and software process models is also provided.", "venue": "Springer Berlin Heidelberg", "year": 1997, "referenceCount": 86, "citationCount": 536, "influentialCitationCount": 41, "isOpenAccess": false, "fieldsOfStudy": ["Engineering", "Computer Science"], "authors": [{"authorId": "1684014", "name": "J. Sametinger"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "07f7a8cbcbf96d6e7764016d3304f88888b97beb", "externalIds": {"MAG": "1670552626", "DOI": "10.1007/978-3-540-75829-7_8"}, "url": "https://www.semanticscholar.org/paper/07f7a8cbcbf96d6e7764016d3304f88888b97beb", "title": "Search Engine Bias and the Demise of Search Engine Utopianism", "abstract": "Due to search engines\u2019 automated operations, people often assume that search engines display search results neutrally and without bias. However, this perception is mistaken. Like any other media company, search engines affirmatively control their users\u2019 experiences, which has the consequence of skewing search results (a phenomenon called \u201csearch engine bias\u201d). Some commentators believe that search engine bias is a defect requiring legislative correction. Instead, this chapter argues that search engine bias is the beneficial consequence of search engines optimizing content for their users. The chapter further argues that the most problematic aspect of search engine bias, the \u201cwinner-take-all\u201d effect caused by top placement in search results, will be mooted by emerging personalized search technology.", "venue": "", "year": 2006, "referenceCount": 34, "citationCount": 128, "influentialCitationCount": 12, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144084067", "name": "E. Goldman"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "081fb2e8162feef4e4234432f0168c05e7951d7b", "externalIds": {"MAG": "1552199320", "DOI": "10.2139/SSRN.2156058"}, "url": "https://www.semanticscholar.org/paper/081fb2e8162feef4e4234432f0168c05e7951d7b", "title": "Ethics of Data Mining and Predictive Analytics in Higher Education", "abstract": null, "venue": "", "year": 2013, "referenceCount": 27, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "2115353555", "name": "J. Johnson"}]}}, {"contexts": ["Proprietary algorithms are kept secret for the sake of competitive advantage (Glenn and Monteith, 2014; Kitchin, 2016; Stark and Fins, 2013), national security (Leese, 2014), or privacy.", "The subject\u2019s autonomy in decision-making is disrespected when the desired choice reflects third party interests above the individual\u2019s (Applin and Fischer, 2015; Stark and Fins, 2013)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "0b277c7e33fa47d189858e8e391ee0b470142473", "externalIds": {"MAG": "1986407612", "DOI": "10.1017/S0963180113000224", "PubMed": "23916394"}, "url": "https://www.semanticscholar.org/paper/0b277c7e33fa47d189858e8e391ee0b470142473", "title": "Engineering Medical Decisions", "abstract": null, "venue": "Cambridge Quarterly of Healthcare Ethics", "year": 2013, "referenceCount": 41, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2729226", "name": "M. Stark"}, {"authorId": "113298661", "name": "J. J. Fins"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "0bf1ddf50621558647cee60cd2709eb2a9744a65", "externalIds": {"MAG": "2703592991", "DOI": "10.2966/SCRIP.140117.131"}, "url": "https://www.semanticscholar.org/paper/0bf1ddf50621558647cee60cd2709eb2a9744a65", "title": "Book review: Group Privacy: New Challenges of Data Technologies", "abstract": null, "venue": "", "year": 2017, "referenceCount": 0, "citationCount": 33, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "2108784903", "name": "Wenlong Li"}]}}, {"contexts": ["\u2026regulators (Pasquale, 2015; Tutt, 2016; Zarsky, 2016), or empirical researchers (Kitchin, 2016; Neyland, 2016), using ex post audit studies (Adler et al., 2016; Diakopoulos, 2015; Kitchin, 2016; Romei and Ruggieri, 2014; Sandvig et al., 2014), reflexive ethnographic studies in development\u2026", "Further work is required to design broadly applicable, low impact auditing mechanisms for algorithms (cf. Adler et al., 2016; Sandvig et al., 2014) that build upon current work in transparency and interpretability of machine learning (e.g. Kim et al., 2015; Lou et al., 2013)."], "isInfluential": false, "intents": ["background", "methodology"], "citedPaper": {"paperId": "0c8c3c8a28e7e90e64085d9a7b8af6ff265461ac", "externalIds": {"DBLP": "journals/corr/AdlerFFRSSV16", "MAG": "2279943773"}, "url": "https://www.semanticscholar.org/paper/0c8c3c8a28e7e90e64085d9a7b8af6ff265461ac", "title": "Auditing Black-box Models by Obscuring Features", "abstract": "Data-trained predictive models are widely used to assist in decision making. But they are used as black boxes that output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior: and in particular how different attributes influence the model prediction. This is very important when trying to interpret the behavior of complex models, or ensure that certain problematic attributes (like race or gender) are not unduly influencing decisions. \nIn this paper, we present a technique for auditing black-box models: we can study the extent to which existing models take advantage of particular features in the dataset without knowing how the models work. We show how a class of techniques originally developed for the detection and repair of disparate impact in classification models can be used to study the sensitivity of any model with respect to any feature subsets. \nOur approach does not require the black-box model to be retrained. This is important if (for example) the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence like feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available datasets and models. We also validate our procedure using techniques from interpretable learning and feature selection.", "venue": "ArXiv", "year": 2016, "referenceCount": 37, "citationCount": 24, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2058592864", "name": "Philip Adler"}, {"authorId": "38601860", "name": "Casey Falk"}, {"authorId": "34597147", "name": "Sorelle A. Friedler"}, {"authorId": "3384554", "name": "Gabriel Rybeck"}, {"authorId": "1786183", "name": "C. Scheidegger"}, {"authorId": "2111295003", "name": "Brandon Smith"}, {"authorId": "1747652", "name": "Suresh Venkatasubramanian"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "0cd10aabcbcc672e1490aae108b44be9d245b652", "externalIds": {"DBLP": "journals/tis/Granka10", "MAG": "2135182104", "DOI": "10.1080/01972243.2010.511560"}, "url": "https://www.semanticscholar.org/paper/0cd10aabcbcc672e1490aae108b44be9d245b652", "title": "The Politics of Search: A Decade Retrospective", "abstract": "In \u201cShaping the Web: Why the Politics of Search Engines Matters,\u201d Introna and Nissenbaum (2000) introduced scholars to the political, as well as technical, issues central to the development of online search engines. Since that time, scholars have critically evaluated the role that search engines play in structuring the scope of online information access for the rest of society, with an emphasis on the implications for a democratic and diverse Web. This article describes the thought behind search engine regulation, online diversity, and information bias, and it places these issues within the context of the technical and societal changes that have occurred in the online search industry. The author assesses which of the initial concerns expressed about online search engines remain relevant today and discusses how technical changes demand a new approach to measuring online diversity and democracy. The author concludes with a proposal to direct the research and thought in online search going forward.", "venue": "Inf. Soc.", "year": 2010, "referenceCount": 87, "citationCount": 126, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2709281", "name": "Laura A. Granka"}]}}, {"contexts": ["Different metrics \u2018\u2018make visible aspects of individuals and groups that are not otherwise perceptible\u2019\u2019 (Lupton, 2014: 859)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "0f4e804fbcfa83f011b9452c62c398315505d75c", "externalIds": {"DBLP": "journals/tis/LomborgB14", "MAG": "2102861279", "DOI": "10.1080/01972243.2014.915276"}, "url": "https://www.semanticscholar.org/paper/0f4e804fbcfa83f011b9452c62c398315505d75c", "title": "Using APIs for Data Collection on Social Media", "abstract": "This article discusses how social media research may benefit from social media companies making data available to researchers through their application programming interfaces (APIs). An API is a back-end interface through which third-party developers may connect new add-ons to an existing service. The API is also an interface for researchers to collect data off a given social media service for empirical analysis. Presenting a critical methodological discussion of the opportunities and challenges associated with quantitative and qualitative social media research based on APIs, this article highlights a number of general methodological issues to be dealt with when collecting and assessing data through APIs. The article further discusses the legal and ethical implications of empirical research using APIs for data collection.", "venue": "Inf. Soc.", "year": 2014, "referenceCount": 62, "citationCount": 143, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1834033", "name": "Stine Lomborg"}, {"authorId": "30827227", "name": "Anja Bechmann"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "10ddb646feddc12337b5a755c72e153e37088c02", "externalIds": {"MAG": "2019363670", "DBLP": "conf/stoc/Valiant84", "DOI": "10.1145/800057.808710"}, "url": "https://www.semanticscholar.org/paper/10ddb646feddc12337b5a755c72e153e37088c02", "title": "A theory of the learnable", "abstract": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.", "venue": "STOC '84", "year": 1984, "referenceCount": 18, "citationCount": 3982, "influentialCitationCount": 196, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1741124", "name": "L. Valiant"}]}}, {"contexts": ["A machine learning algorithm applied to classification tasks, for example, typically consists of two components, a learner which produces a classifier, with the intention to develop classes that can generalise beyond the training data (Domingos, 2012)."], "isInfluential": false, "intents": ["methodology"], "citedPaper": {"paperId": "1696cbf7da0ee845c50591843993e6605adec177", "externalIds": {"DBLP": "journals/cacm/Domingos12", "MAG": "2161336914", "DOI": "10.1145/2347736.2347755"}, "url": "https://www.semanticscholar.org/paper/1696cbf7da0ee845c50591843993e6605adec177", "title": "A few useful things to know about machine learning", "abstract": "Tapping into the \"folk knowledge\" needed to advance machine learning applications.", "venue": "Commun. ACM", "year": 2012, "referenceCount": 35, "citationCount": 2055, "influentialCitationCount": 97, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1740213", "name": "Pedro M. Domingos"}]}}, {"contexts": ["Statistical learning theory (James et al., 2013) and computational learning theory (Valiant, 1984) are both concerned with the characterisation and quantification of this uncertainty."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "16b3c8f6f1dffd31271c59c11e17241e51377d68", "externalIds": {"MAG": "2487770199", "DOI": "10.1007/978-1-4614-7138-7"}, "url": "https://www.semanticscholar.org/paper/16b3c8f6f1dffd31271c59c11e17241e51377d68", "title": "An introduction to statistical learning", "abstract": "Statistics An Intduction to Stistical Lerning with Applications in R An Introduction to Statistical Learning provides an accessible overview of the fi eld of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fi elds ranging from biology to fi nance to marketing to astrophysics in the past twenty years. Th is book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classifi cation, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fi elds, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical soft ware platform. Two of the authors co-wrote Th e Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. Th is book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. Th e text assumes only a previous course in linear regression and no knowledge of matrix algebra.", "venue": "", "year": 2013, "referenceCount": 0, "citationCount": 5664, "influentialCitationCount": 451, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144026427", "name": "Gareth M. James"}, {"authorId": "2113693105", "name": "D. Witten"}, {"authorId": "1784682", "name": "T. Hastie"}, {"authorId": "1761784", "name": "R. Tibshirani"}]}}, {"contexts": ["One possible path to explainability is algorithmic auditing carried out by data processors (Zarsky, 2016), external regulators (Pasquale, 2015; Tutt, 2016; Zarsky, 2016), or empirical researchers (Kitchin, 2016; Neyland, 2016), using ex post audit studies (Adler et al., 2016; Diakopoulos, 2015;\u2026"], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "16d48c78afb6a9880486ce1b2111a611b4007557", "externalIds": {"MAG": "202799293", "DOI": "10.5860/choice.190706"}, "url": "https://www.semanticscholar.org/paper/16d48c78afb6a9880486ce1b2111a611b4007557", "title": "The Black Box Society: The Secret Algorithms That Control Money and Information", "abstract": "Every day, corporations are connecting the dots about our personal behaviorsilently scrutinizing clues left behind by our work habits and Internet use. The data compiled and portraits created are incredibly detailed, to the point of being invasive. But who connects the dots about what firms are doing with this information? The Black Box Society argues that we all need to be able to do soand to set limits on how big data affects our lives. Hidden algorithms can make (or ruin) reputations, decide the destiny of entrepreneurs, or even devastate an entire economy. Shrouded in secrecy and complexity, decisions at major Silicon Valley and Wall Street firms were long assumed to be neutral and technical. But leaks, whistleblowers, and legal disputes have shed new light on automated judgment. Self-serving and reckless behavior is surprisingly common, and easy to hide in code protected by legal and real secrecy. Even after billions of dollars of fines have been levied, underfunded regulators may have only scratched the surface of this troubling behavior. Frank Pasquale exposes how powerful interests abuse secrecy for profit and explains ways to rein them in. Demanding transparency is only the first step. An intelligible society would assure that key decisions of its most important firms are fair, nondiscriminatory, and open to criticism. Silicon Valley and Wall Street need to accept as much accountability as they impose on others.", "venue": "", "year": 2015, "referenceCount": 526, "citationCount": 1026, "influentialCitationCount": 46, "isOpenAccess": true, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "24272314", "name": "Frank A. Pasquale"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "184a62a6006cb95cc959e672569188861bf7923d", "externalIds": {"MAG": "1999447203", "DOI": "10.1007/s10676-006-0010-6"}, "url": "https://www.semanticscholar.org/paper/184a62a6006cb95cc959e672569188861bf7923d", "title": "Data Mining to Combat Terrorism and the Roots of Privacy Concerns", "abstract": "Recently, there has been a heavy debate in the US about the government\u2019s use of data mining in its fight against terrorism. Privacy concerns in fact led the Congress to terminate the funding of TIA, a program for advanced information technology to be used in the combat of terrorism. The arguments put forward in this debate, more specifically those found in the main report and minority report by the TAPAC established by the Secretary of Defense to examine the TIA issue, will be analysed to trace the deeper roots of this controversy. This analysis will in turn be used as a test case to examine the adequacy of the usual theoretical frameworks for these kinds of issues, in particular the notion of privacy. Whereas the dominant theoretical framing of the notion of privacy turns around access to information, most of the core arguments in the debate do not fit in this kind of framework. The basic disagreements in the controversy are not about mere access, they involve both access and use. Furthermore, whereas the issue of access by itself refers to a more or less static situation, the real disagreements much more concern the organisational dynamics of the use of information, the mechanisms in the organisation that control these dynamics, and the awareness present within the organisation of the \u2018social risks\u2019 these dynamics represent. The bottom line question is whether the assessment of these gives sufficient reason for trust.", "venue": "Ethics and Information Technology", "year": 2005, "referenceCount": 15, "citationCount": 15, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "3385523", "name": "F. Birrer"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "1c775c0eae07f6d716a4f0da42f3e2c1ab4d7f82", "externalIds": {"DBLP": "books/daglib/p/Hildebrandt08", "MAG": "162565672", "DOI": "10.1007/978-1-4020-6914-7_2"}, "url": "https://www.semanticscholar.org/paper/1c775c0eae07f6d716a4f0da42f3e2c1ab4d7f82", "title": "Defining Profiling: A New Type of Knowledge?", "abstract": null, "venue": "Profiling the European Citizen", "year": 2008, "referenceCount": 26, "citationCount": 133, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Sociology", "Computer Science"], "authors": [{"authorId": "40652172", "name": "M. Hildebrandt"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "1d174f0e3c391368d0f3384a144a6c7487f2a143", "externalIds": {"MAG": "1819662813", "DOI": "10.2139/SSRN.2477899"}, "url": "https://www.semanticscholar.org/paper/1d174f0e3c391368d0f3384a144a6c7487f2a143", "title": "Big Data's Disparate Impact", "abstract": "Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm\u2019s use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.This Essay examines these concerns through the lens of American antidiscrimination law \u2014 more particularly, through Title VII\u2019s prohibition of discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining\u2019s victims would seem to lie in disparate impact doctrine. Case law and the Equal Employment Opportunity Commission\u2019s Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way to demonstrate that these discoveries are spurious, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others\u2019 discrimination against members of protected groups, or flaws in the underlying dataAddressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, when discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying antidiscrimination law: anticlassification and antisubordination. Finding a solution to big data\u2019s disparate impact will require more than best efforts to stamp out prejudice and bias; it will require a wholesale reexamination of the meanings of \u201cdiscrimination\u201d and \u201cfairness.\u201d", "venue": "", "year": 2016, "referenceCount": 15, "citationCount": 1350, "influentialCitationCount": 67, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "2881033", "name": "Solon Barocas"}, {"authorId": "46432110", "name": "Andrew D. Selbst"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "201d763bbffc1797ad407c0ab60925750180a67c", "externalIds": {"MAG": "2057118538", "DBLP": "conf/icdm/KamishimaAAS12", "DOI": "10.1109/ICDMW.2012.101"}, "url": "https://www.semanticscholar.org/paper/201d763bbffc1797ad407c0ab60925750180a67c", "title": "Considerations on Fairness-Aware Data Mining", "abstract": "With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals' lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair regarding sensitive features such as race, gender, religion, and so on. Several researchers have recently begun to develop fairness-aware or discrimination-aware data mining techniques that take into account issues of social fairness, discrimination, and neutrality. In this paper, after demonstrating the applications of these techniques, we explore the formal concepts of fairness and techniques for handling fairness in data mining. We then provide an integrated view of these concepts based on statistical independence. Finally, we discuss the relations between fairness-aware data mining and other research topics, such as privacy-preserving data mining or causal inference.", "venue": "2012 IEEE 12th International Conference on Data Mining Workshops", "year": 2012, "referenceCount": 29, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "34865486", "name": "Toshihiro Kamishima"}, {"authorId": "143683998", "name": "S. Akaho"}, {"authorId": "7142317", "name": "H. Asoh"}, {"authorId": "1733719", "name": "Jun Sakuma"}]}}, {"contexts": ["Personalisation algorithms reduce the diversity of information users encounter by excluding content deemed irrelevant or contradictory to the user\u2019s beliefs (Barnet, 2009; Pariser, 2011).", "\u2026(Raymond, 2014; Shackelford and Raymond, 2014); recommendation and filtering systems that compare and group users to provide personalised content (Barnet, 2009); clinical decision support systems (CDSS) that recommend diagnoses and treatments to physicians (Diamond et al., 1987; Mazoue\u0301, 1990);\u2026"], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "201e2ba8917eb1b1d0915ba6e1b2e4edeb1144e8", "externalIds": {"MAG": "1987282744", "DOI": "10.1080/10304310802570890"}, "url": "https://www.semanticscholar.org/paper/201e2ba8917eb1b1d0915ba6e1b2e4edeb1144e8", "title": "Idiomedia: The rise of personalized, aggregated content", "abstract": "We are very early in the total information we have within Google. The algorithms will get better and we will get better at personalization. (Eric Schmidt, CEO Google, cited in Daniel and Palmer 200...", "venue": "", "year": 2009, "referenceCount": 30, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "13814727", "name": "Belinda Barnet"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "2302c56eb40f3339e910fa8197d7429e6ba8b032", "externalIds": {"MAG": "320587423", "DBLP": "journals/see/MittelstadtF16", "DOI": "10.1007/s11948-015-9652-2", "PubMed": "26002496"}, "url": "https://www.semanticscholar.org/paper/2302c56eb40f3339e910fa8197d7429e6ba8b032", "title": "The Ethics of Big Data: Current and Foreseeable Issues in Biomedical Contexts", "abstract": "The capacity to collect and analyse data is growing exponentially. Referred to as \u2018Big Data\u2019, this scientific, social and technological trend has helped create destabilising amounts of information, which can challenge accepted social and ethical norms. Big Data remains a fuzzy idea, emerging across social, scientific, and business contexts sometimes seemingly related only by the gigantic size of the datasets being considered. As is often the case with the cutting edge of scientific and technological progress, understanding of the ethical implications of Big Data lags behind. In order to bridge such a gap, this article systematically and comprehensively analyses academic literature concerning the ethical implications of Big Data, providing a watershed for future ethical investigations and regulations. Particular attention is paid to biomedical Big Data due to the inherent sensitivity of medical information. By means of a meta-analysis of the literature, a thematic narrative is provided to guide ethicists, data scientists, regulators and other stakeholders through what is already known or hypothesised about the ethical risks of this emerging and innovative phenomenon. Five key areas of concern are identified: (1) informed consent, (2) privacy (including anonymisation and data protection), (3) ownership, (4) epistemology and objectivity, and (5) \u2018Big Data Divides\u2019 created between those who have or lack the necessary resources to analyse increasingly large datasets. Critical gaps in the treatment of these themes are identified with suggestions for future research. Six additional areas of concern are then suggested which, although related have not yet attracted extensive debate in the existing literature. It is argued that they will require much closer scrutiny in the immediate future: (6) the dangers of ignoring group-level ethical harms; (7) the importance of epistemology in assessing the ethics of Big Data; (8) the changing nature of fiduciary relationships that become increasingly data saturated; (9) the need to distinguish between \u2018academic\u2019 and \u2018commercial\u2019 Big Data practices in terms of potential harm to data subjects; (10) future problems with ownership of intellectual property generated from analysis of aggregated datasets; and (11) the difficulty of providing meaningful access rights to individual data subjects that lack necessary resources. Considered together, these eleven themes provide a thorough critical framework to guide ethical assessment and governance of emerging Big Data practices.", "venue": "Sci. Eng. Ethics", "year": 2016, "referenceCount": 139, "citationCount": 329, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Sociology", "Medicine", "Computer Science"], "authors": [{"authorId": "3127701", "name": "B. Mittelstadt"}, {"authorId": "1982425", "name": "L. Floridi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "236b0ce63ba4ef65980d1a7f977f630bceaf15b1", "externalIds": {"MAG": "2038782498", "DOI": "10.1007/s10676-011-9278-2"}, "url": "https://www.semanticscholar.org/paper/236b0ce63ba4ef65980d1a7f977f630bceaf15b1", "title": "Robots: ethical by design", "abstract": "Among ethicists and engineers within robotics there is an ongoing discussion as to whether ethical robots are possible or even desirable. We answer both of these questions in the positive, based on an extensive literature study of existing arguments. Our contribution consists in bringing together and reinterpreting pieces of information from a variety of sources. One of the conclusions drawn is that artifactual morality must come in degrees and depend on the level of agency, autonomy and intelligence of the machine. Moral concerns for agents such as intelligent search machines are relatively simple, while highly intelligent and autonomous artifacts with significant impact and complex modes of agency must be equipped with more advanced ethical capabilities. Systems like cognitive robots are being developed that are expected to become part of our everyday lives in future decades. Thus, it is necessary to ensure that their behaviour is adequate. In an analogy with artificial intelligence, which is the ability of a machine to perform activities that would require intelligence in humans, artificial morality is considered to be the ability of a machine to perform activities that would require morality in humans. The capacity for artificial (artifactual) morality, such as artifactual agency, artifactual responsibility, artificial intentions, artificial (synthetic) emotions, etc., come in varying degrees and depend on the type of agent. As an illustration, we address the assurance of safety in modern High Reliability Organizations through responsibility distribution. In the same way that the concept of agency is generalized in the case of artificial agents, the concept of moral agency, including responsibility, is generalized too. We propose to look at artificial moral agents as having functional responsibilities within a network of distributed responsibilities in a socio-technological system. This does not take away the responsibilities of the other stakeholders in the system, but facilitates an understanding and regulation of such networks. It should be pointed out that the process of development must assume an evolutionary form with a number of iterations because the emergent properties of artifacts must be tested in real world situations with agents of increasing intelligence and moral competence. We see this paper as a contribution to the macro-level Requirement Engineering through discussion and analysis of general requirements for design of ethical robots.", "venue": "Ethics and Information Technology", "year": 2012, "referenceCount": 155, "citationCount": 54, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1391837542", "name": "Gordana Dodig Crnkovic"}, {"authorId": "2794313", "name": "Baran \u00c7\u00fcr\u00fckl\u00fc"}]}}, {"contexts": ["Professionals have implicit knowledge and subtle skills (cf. Coeckelbergh, 2013; MacIntyre, 2007) that are difficult to make explicit and perhaps impossible to make computable (Morek, 2006)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "2a0457ba98cc901cf7c44244c064dcac05a0703d", "externalIds": {"MAG": "2156217188", "DOI": "10.1007/s11019-013-9463-7", "PubMed": "23338289"}, "url": "https://www.semanticscholar.org/paper/2a0457ba98cc901cf7c44244c064dcac05a0703d", "title": "E-care as craftsmanship: virtuous work, skilled engagement, and information technology in health care", "abstract": "Contemporary health care relies on electronic devices. These technologies are not ethically neutral but change the practice of care. In light of Sennett\u2019s work and that of other thinkers (Dewey, Dreyfus, Borgmann) one worry is that \u201ce-care\u201d\u2014care by means of new information and communication technologies\u2014does not promote skilful and careful engagement with patients and hence is neither conducive to the quality of care nor to the virtues of the care worker. Attending to the kinds of knowledge involved in care work and their moral significance, this paper explores what \u201ccraftsmanship\u201d means in the context of medicine and health care and discusses whether today the care giver\u2019s craftsmanship is eroded. It is argued that this is a real danger, especially under modern conditions and in the case of telecare, but that whether it happens, and to what extent it happens, depends on whether in a specific practice and given a specific technology e-carers can develop the know-how and skill to engage more intensely with those under their care and to cooperate with their co-workers.", "venue": "Medicine, health care, and philosophy", "year": 2013, "referenceCount": 29, "citationCount": 28, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Sociology", "Medicine"], "authors": [{"authorId": "46450294", "name": "M. Coeckelbergh"}]}}, {"contexts": ["Machine learning algorithms are particularly challenging in this respect (Burrell, 2016; Matthias, 2004; Zarsky, 2016), seen for instance in genetic algorithms that program themselves.", "The longstanding problem of interpretability in machine learning algorithms indicates the challenge of opacity in algorithms (Burrell, 2016; Hildebrandt, 2011;\nLeese, 2014; Tutt, 2016).", "algorithms is a product of the high-dimensionality of data, complex code and changeable decision-making logic (Burrell, 2016).", "Comparable restrictions already exist in the US Credit Reporting Act, which effectively prohibits machine learning in credit scoring because reasons for the denial of credit must be made available to consumers on demand (Burrell, 2016).", "This alteration of how the algorithm classifies new inputs is how it learns (Burrell, 2016: 5).", "Learning algorithms, often quoted as the \u2018future\u2019 of algorithms and analytics (Tutt, 2016), introduce uncertainty over how and why decisions are made due to their capacity to tweak operational parameters and decision-making rules \u2018in the wild\u2019 (Burrell, 2016).", "Machine learning in particular raises unique challenges, because achieving the intended or \u2018\u2018correct\u2019\u2019 behaviour does not imply the absence of errors20 (cf. Burrell, 2016) or harmful actions and feedback loops.", "Many scholarly critiques also fail to specify technical categories or a formal definition of \u2018algorithm\u2019 (Burrell, 2016; Kitchin, 2016).", "Algorithms \u2018\u2018are opaque in the sense that if one is a recipient of the output of the algorithm (the classification decision), rarely does one have any concrete sense of how or why a particular classification has been arrived at from inputs\u2019\u2019 (Burrell, 2016: 1).", "Opacity in machine learning algorithms is a product of the highdimensionality of data, complex code and changeable decision-making logic (Burrell, 2016).", "Learning algorithms, often quoted as the 'future' of algorithms and analytics (Tutt, 2016), introduce uncertainty over how and why decisions are made due to their capacity to tweak operational parameters and decision-making rules 'in the wild' (Burrell, 2016).", "Burrell (2016) and Schermer (2011) argue that the opacity of machine learning algorithms inhibits oversight."], "isInfluential": true, "intents": ["background"], "citedPaper": {"paperId": "2b931e0636b54a7592a14de37d6a2320c372fc53", "externalIds": {"MAG": "2793798487"}, "url": "https://www.semanticscholar.org/paper/2b931e0636b54a7592a14de37d6a2320c372fc53", "title": "This Machine Thinks", "abstract": null, "venue": "", "year": 2012, "referenceCount": 0, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Philosophy"], "authors": [{"authorId": "2076400455", "name": "T. Vaughn"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "2bbd3efbb00fb57824b8880ae2dde076e15a80e7", "externalIds": {"MAG": "2047370889", "DBLP": "journals/joc/LindellP02", "DOI": "10.1007/s00145-001-0019-2"}, "url": "https://www.semanticscholar.org/paper/2bbd3efbb00fb57824b8880ae2dde076e15a80e7", "title": "Privacy Preserving Data Mining\n", "abstract": "Abstract. In this paper we address the issue of privacy preserving data mining. Specifically, we consider a scenario in which two parties owning confidential databases wish to run a data mining algorithm on the union of their databases, without revealing any unnecessary information. Our work is motivated by the need both to protect privileged information and to enable its use for research or other purposes. The above problem is a specific example of secure multi-party computation and, as such, can be solved using known generic protocols. However, data mining algorithms are typically complex and, furthermore, the input usually consists of massive data sets. The generic protocols in such a case are of no practical use and therefore more efficient protocols are required. We focus on the problem of decision tree learning with the popular ID3 algorithm. Our protocol is considerably more efficient than generic solutions and demands both very few rounds of communication and reasonable bandwidth.", "venue": "Journal of Cryptology", "year": 2002, "referenceCount": 61, "citationCount": 2235, "influentialCitationCount": 113, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1682750", "name": "Yehuda Lindell"}, {"authorId": "1689531", "name": "Benny Pinkas"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "2c3ee5917db4edab543ced87af55df59895ad6ff", "externalIds": {"DBLP": "conf/acsc/FuleR04", "MAG": "1613845839"}, "url": "https://www.semanticscholar.org/paper/2c3ee5917db4edab543ced87af55df59895ad6ff", "title": "Detecting Privacy and Ethical Sensitivity in Data Mining Results", "abstract": "Knowledge discovery allows considerable insight into data. This brings with it the inherent risk that what is inferred may be private or ethically sensitive. The process of generating rules through a mining operation becomes an ethical issue when the results are used in decision making processes that effect people, or when mining customer data unwittingly compromises the privacy of those customers.Significantly, the sensitivity of a rule not be apparent to the miner, particularly since the volume and diversity of rules can often be large. However, given the subjective nature of such sensitivity, rather than prohibit the production of ethically and privacy sensitive rules, we present here an alerting process that detects and highlights the sensitivity of the discovered rules. The process caters for differing sensitivities at the attribute value level and allows a variety of sensitivity combination functions to be employed. These functions have been tested empirically and the results of these tests are reported.", "venue": "ACSC", "year": 2004, "referenceCount": 43, "citationCount": 69, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Engineering", "Political Science"], "authors": [{"authorId": "143897475", "name": "P. Fule"}, {"authorId": "1707518", "name": "J. Roddick"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "2c7c6ded2304b1a1aac6a7de38d5b81f24e5de73", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/2c7c6ded2304b1a1aac6a7de38d5b81f24e5de73", "title": "Classification with No Discrimination by Preferential Sampling", "abstract": "The concept of classification without discrimination is a new area of research. (Kamiran & Calders, ) introduced the idea of Classification with No Discrimination (CND)and proposed a solution based on \u201cmassaging\u201d the data to remove the discrimination from it with the least possible changes. In this paper, we propose a new solution to theCNDproblem by introducing a sampling scheme for making the data discrimination free instead of relabeling the dataset. On the resulting non-discriminatory dataset we then learn a classifier. This new method is not only less intrusive as compared to the \u201cmassaging\u201d but also outperforms the \u201creweighing\u201d approach of (Calders et al., 2009). The proposed method has been implemented and experimental results on the Census Income dataset show promising results: in all experiments our method performs onpar with the state-of-the art non-discriminatory techniques.", "venue": "", "year": 2010, "referenceCount": 6, "citationCount": 85, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1391279582", "name": "Married"}, {"authorId": "2072256384", "name": "Young"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "2cbc1cea80cdf9e0bd819040f480841ea09ab406", "externalIds": {"MAG": "2317166805", "DOI": "10.1177/0162243915589635"}, "url": "https://www.semanticscholar.org/paper/2cbc1cea80cdf9e0bd819040f480841ea09ab406", "title": "Can an Algorithm be Agonistic? Ten Scenes from Life in Calculated Publics", "abstract": "This paper explores how political theory may help us map algorithmic logics against different visions of the political. Drawing on Chantal Mouffe\u2019s theories of agonistic pluralism, this paper depicts algorithms in public life in ten distinct scenes, in order to ask the question, what kinds of politics do they instantiate? Algorithms are working within highly contested online spaces of public discourse, such as YouTube and Facebook, where incompatible perspectives coexist. Yet algorithms are designed to produce clear \u201cwinners\u201d from information contests, often with little visibility or accountability for how those contests are designed. In isolation, many of these algorithms seem the opposite of agonistic: much of the complexity of search, ranking, and recommendation algorithms is nonnegotiable and kept far from view, inside an algorithmic \u201cblack box.\u201d But what if we widen our perspective? This paper suggests agonistic pluralism as both a design ideal for engineers and a provocation to understand algorithms in a broader social context: rather than focusing on the calculations in isolation, we need to account for the spaces of contestation where they operate.", "venue": "", "year": 2016, "referenceCount": 47, "citationCount": 153, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "48024382", "name": "K. Crawford"}]}}, {"contexts": ["The party that sets confidence intervals for an algorithm\u2019s decision-making structure shares responsibility for the effects of the resultant false positives, false negatives and spurious correlations (Birrer, 2005; Johnson, 2013; Kraemer et al., 2011).", "In other words, designers make value-judgments that express views \u2018\u2018on how things ought to be or not to be, or what is good or bad, or desirable or undesirable\u2019\u2019 (Kraemer et al., 2011: 252).", "\u2026is unsustainable, as shown by prior work demonstrating the normativity of information technologies in general and algorithm development in particular14 (e.g. Bozdag, 2013; Friedman and Nissenbaum, 1996; Kraemer et al., 2011; Macnish, 2012; Newell and Marabelli, 2015: 6; Tene and Polonetsky, 2013b).", "Kraemer et al. (2011) provide a counterargument from the reviewed literature.", "Designers and users of algorithms are typically blamed when problems arise (Kraemer et al., 2011: 251).", "For Kraemer et al. (2011), algorithms that produce hypothetical value-judgments or recommended courses of action, such as clinical decision support systems, can be value-neutral because the judgments produced are hypothetical.", "Operational parameters are specified by developers and configured by users with desired outcomes in mind that privilege some values and interests over others (cf. Friedman and Nissenbaum, 1996; Johnson, 2006; Kraemer et al., 2011; Nakamura, 2013).", "Denying agency to artificial agents makes designers responsible for the unethical behaviour of their semi-autonomous creations; bad consequences reflect bad design (Anderson and Anderson, 2014; Kraemer et al., 2011; Turilli, 2007)."], "isInfluential": true, "intents": ["methodology", "background"], "citedPaper": {"paperId": "2ddc5fdab06d1d78d41c39c64c1b9c6ec4fcc6c8", "externalIds": {"MAG": "2131044459", "DOI": "10.1007/s10676-010-9233-7"}, "url": "https://www.semanticscholar.org/paper/2ddc5fdab06d1d78d41c39c64c1b9c6ec4fcc6c8", "title": "Is there an ethics of algorithms?", "abstract": "We argue that some algorithms are value-laden, and that two or more persons who accept different value-judgments may have a rational reason to design such algorithms differently. We exemplify our claim by discussing a set of algorithms used in medical image analysis: In these algorithms it is often necessary to set certain thresholds for whether e.g. a cell should count as diseased or not, and the chosen threshold will partly depend on the software designer\u2019s preference between avoiding false positives and false negatives. This preference ultimately depends on a number of value-judgments. In the last section of the paper we discuss some general principles for dealing with ethical issues in algorithm-design.", "venue": "Ethics and Information Technology", "year": 2011, "referenceCount": 10, "citationCount": 105, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "137499529", "name": "Felicitas Kraemer"}, {"authorId": "21720427", "name": "K. Overveld"}, {"authorId": "34024409", "name": "M. Peterson"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "2e76fa107c50485985487b03a601eb0c11894193", "externalIds": {"MAG": "2465777981"}, "url": "https://www.semanticscholar.org/paper/2e76fa107c50485985487b03a601eb0c11894193", "title": "Big Data: A Revolution That Will Transform How We Live, Work, and Think", "abstract": "Amazon Exclusive: Q&A with Kenneth Cukier and Viktor Mayer-Schonberger Q. What did it take to write Big Data? A. Kenn has written about technology and business from Europe, Asia, and the US for The Economist, and is well-connected to the data community. Viktor had researched the information economy as a professor at Harvard and now at Oxford, and his book Delete had been well received. So we thought we had a good basis to make a contribution in the area. As we wrote the book, we had to dig deep to find unheard stories about big data pioneers and interview them. We wanted Big Data to be about a big idea, but also to be full of examples and success stories -- and be engrossing to read. Q. Are you big datas cheerleaders? A. Absolutely not. We are the messengers of big data, not its evangelists. The big data age is happening, and in the book we take a look at the drivers, and big datas likely trajectory: how it will change how we work and live. We emphasize that the fundamental shift is not in the machines that calculate data, but in the data itself and how we use it. Q. In discovering big data applications, what was your biggest surprise? A. It is tempting to say that it was predicting exploding manholes, tracking inflation in real time, or how big data saves the lives of premature babies. But the biggest surprise for us perhaps was the very diversity of the uses of big data, and how it already is changing peoples everyday world. Many people see big data through the lens of the Internet economy, since Google and Facebook have so much data. But that misses the point: big data is everywhere. Q. Is Big Data then primarily a story about economic efficiency? A. Big data improves economic efficiency, but thats only a very small part of the story. We realized when talking to dozens and dozens of big data pioneers that it improves health care, advances better education, and helps predict societal changefrom urban sprawl to the spread of the flu. Big data is roaring through all sectors of the economy and all areas of life. Q. So big data offers only upside? A. Not at all. We are very concerned about what we call in our book the dark side of big data. However the real challenge is that the problem is not necessarily where we initially tend to think it is, such as surveillance and privacy. After looking into the potential misuses of big data, we became much more troubled by propensity -- that is, big data predictions being used to police and punish. And by the fetishization of data that may occur, whereby organizations may blindly defer to what the data says without understanding its limitations. Q. What can we do about this dark side? A. Knowing about it is the first step. We thought hard to suggest concrete steps that can be taken to minimize and mitigate big datas risk, and came up with a few ways to ensure transparency, guarantee human free will, and strike a better balance on privacy and the use of personal information. These are deeply serious issues. If we do not take action soon, it might be too late.", "venue": "", "year": 2013, "referenceCount": 7, "citationCount": 2069, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "1412534293", "name": "Viktor Mayer-Schnberger"}, {"authorId": "50603623", "name": "K. Cukier"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "2f29b39e9de8ef36591ff86d66a72b16029481ec", "externalIds": {"MAG": "2042137662", "DOI": "10.2307/302234"}, "url": "https://www.semanticscholar.org/paper/2f29b39e9de8ef36591ff86d66a72b16029481ec", "title": "Truth and Method", "abstract": "Translator's Preface \\ Introduction \\ Foreword \\ Part I: The Question of Truth as it Emerges in the Experience of Art \\ 1. Transcending the Aesthetic Dimension \\ 2. The Ontology of the Work of Art and its Hermeneutic Significance \\ Part II: The Extension of the Question of Truth to Understanding in the Human Sciences \\ 3. Historical Preparation \\ 4. Elements of a Theory of Hermeneutic Experience \\ Part III: The Ontological Shift of Hermeneutics Guided by Language \\ 5. Language and Hermeneutics \\ Appendices and Supplements \\ Afterword \\ Subject Index \\ Author Index.", "venue": "", "year": 1960, "referenceCount": 0, "citationCount": 5608, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Philosophy"], "authors": [{"authorId": "103547638", "name": "H. Gadamer"}, {"authorId": "112939557", "name": "J. Weinsheimer"}, {"authorId": "41175318", "name": "D. Marshall"}]}}, {"contexts": ["\u2026and software-artefacts; from the customary use of existing libraries, to the repurposing of existing tools and methods for different purposes (e.g. the use of seismological models of aftershocks in predictive policing (Mohler et al., 2011), and the tailoring of general tools for specific methods."], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "32b246099484de62f623cf4f017395cbe76b1a21", "externalIds": {"MAG": "2123906784", "DOI": "10.1198/jasa.2011.ap09546"}, "url": "https://www.semanticscholar.org/paper/32b246099484de62f623cf4f017395cbe76b1a21", "title": "Self-Exciting Point Process Modeling of Crime", "abstract": "Highly clustered event sequences are observed in certain types of crime data, such as burglary and gang violence, due to crime-specific patterns of criminal behavior. Similar clustering patterns are observed by seismologists, as earthquakes are well known to increase the risk of subsequent earthquakes, or aftershocks, near the location of an initial event. Space\u2013time clustering is modeled in seismology by self-exciting point processes and the focus of this article is to show that these methods are well suited for criminological applications. We first review self-exciting point processes in the context of seismology. Next, using residential burglary data provided by the Los Angeles Police Department, we illustrate the implementation of self-exciting point process models in the context of urban crime. For this purpose we use a fully nonparametric estimation methodology to gain insight into the form of the space\u2013time triggering function and temporal trends in the background rate of burglary.", "venue": "", "year": 2011, "referenceCount": 37, "citationCount": 654, "influentialCitationCount": 38, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1850196", "name": "G. Mohler"}, {"authorId": "3261371", "name": "M. Short"}, {"authorId": "1970636", "name": "P. Brantingham"}, {"authorId": "101976286", "name": "F. Schoenberg"}, {"authorId": "2539926", "name": "G. Tita"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "367c69b27e79bb8727cf36b98d0785da907d5aa5", "externalIds": {"MAG": "418325907", "DBLP": "journals/japll/TurnerE08", "DOI": "10.1016/j.jal.2008.09.006"}, "url": "https://www.semanticscholar.org/paper/367c69b27e79bb8727cf36b98d0785da907d5aa5", "title": "The Philosophy of Computer Science", "abstract": "The Philosophy of Computer Science is concerned with those philo-sophical issues that surround and underpin the academic discipline of computer science. In this paper we provide an introduction to some itsphilosophical concerns.", "venue": "J. Appl. Log.", "year": 2008, "referenceCount": 394, "citationCount": 79, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Sociology"], "authors": [{"authorId": "145425029", "name": "R. Turner"}, {"authorId": "2445597", "name": "Amnon H. Eden"}]}}, {"contexts": ["Actionable insights (more on this later) are sought rather than causal relationships (Grindrod, 2014; Hildebrandt, 2011; Johnson, 2013).", "Analytics informs immediate responses to the needs and preferences of the users of a system, as well as longer term strategic planning and development by a platform or service provider (Grindrod, 2014).", "Actionable insights (more on\nthis later) are sought rather than causal relationships (Grindrod, 2014; Hildebrandt, 2011; Johnson, 2013)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "396a383891e4417a14de27c9c5576e81140fa063", "externalIds": {"MAG": "2503641548", "DOI": "10.1093/acprof:oso/9780198725091.001.0001"}, "url": "https://www.semanticscholar.org/paper/396a383891e4417a14de27c9c5576e81140fa063", "title": "Mathematical Underpinnings of Analytics: Theory and Applications", "abstract": null, "venue": "", "year": 2015, "referenceCount": 0, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3029780", "name": "P. Grindrod"}]}}, {"contexts": ["Data mining algorithms are said to show promise in helping make sense of emerging streams of behavioural data generated by the \u2018Internet of Things\u2019 (Portmess and Tower, 2014: 1)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "3aa00d107b64c96a19e7dbf450b4640c8a55f741", "externalIds": {"MAG": "2151384869", "DBLP": "journals/ethicsit/PortmessT15", "DOI": "10.1007/s10676-014-9357-2"}, "url": "https://www.semanticscholar.org/paper/3aa00d107b64c96a19e7dbf450b4640c8a55f741", "title": "Data barns, ambient intelligence and cloud computing: the tacit epistemology and linguistic representation of Big Data", "abstract": "The explosion of data grows at a rate of roughly five trillion bits a second, giving rise to greater urgency in conceptualizing the infosphere (Floridi 2011) and understanding its implications for knowledge and public policy. Philosophers of technology and information technologists alike who wrestle with ontological and epistemological questions of digital information tend to emphasize, as Floridi does, information as our new ecosystem and human beings as interconnected informational organisms, inforgs at home in ambient intelligence. But the linguistic and conceptual representations of Big Data\u2014the massive volume of both structured and unstructured data\u2014and the real world practice of data-mining for patterns and meaningful interpretation of evidence reveal tension and ambiguity in the bold promise of data analytics. This paper explores the tacit epistemology of the rhetoric and representation of Big Data and suggests a richer account of its ambiguities and the paradox of its real world materiality. We argue that Big Data should be recognized as manifesting multiple and conflicting trajectories that reflect human intentionality and particular patterns of power and authority. Such patterns require attentive exploration and moral appraisal if we are to resist simplistic informationist ontologies of Big Data, and the subtle forms of control in the political ecology of Big Data that undermine its promise as transformational knowledge.", "venue": "Ethics and Information Technology", "year": 2015, "referenceCount": 58, "citationCount": 22, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology", "Computer Science"], "authors": [{"authorId": "4607553", "name": "Lisa Portmess"}, {"authorId": "6249299", "name": "Sara Tower"}]}}, {"contexts": ["Artificial ethical agents can \u2018\u2018calculate the best action in ethical dilemmas using ethical principles\u2019\u2019 (Moor, 2006) or frameworks derived thereof.", "In contrast, artificial morality requires only that machines act \u2018as if\u2019 they are moral agents, and thus make ethically justified decisions according to pre-defined criteria (Moor, 2006)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "3b146bd60789b7ca9a4b36191d1b53ed2dad55b8", "externalIds": {"MAG": "3006625706"}, "url": "https://www.semanticscholar.org/paper/3b146bd60789b7ca9a4b36191d1b53ed2dad55b8", "title": "The Nature, Importance, and Difficulty of Machine Ethics", "abstract": "Machine ethics has a broad range of possible implementations in computer technology--from maintaining detailed records in hospital databases to overseeing emergency team movements after a disaster....", "venue": "", "year": 2006, "referenceCount": 0, "citationCount": 27, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1643789098", "name": "H. MoorJames"}]}}, {"contexts": ["The moral standing and capacity for ethical decision-making of algorithms remains a\nstandout question in machine ethics (e.g. Allen et al., 2006; Anderson, 2008; Floridi and Sanders, 2004a)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "3e3e302bcf0b97aecd4b66c47d19a1118f497435", "externalIds": {"DBLP": "journals/ais/Anderson08", "MAG": "2031307582", "DOI": "10.1007/s00146-007-0094-5"}, "url": "https://www.semanticscholar.org/paper/3e3e302bcf0b97aecd4b66c47d19a1118f497435", "title": "Asimov\u2019s \u201cthree laws of robotics\u201d and machine metaethics", "abstract": "Using Asimov\u2019s \u201cBicentennial Man\u201d as a springboard, a number of metaethical issues concerning the emerging field of machine ethics are discussed. Although the ultimate goal of machine ethics is to create autonomous ethical machines, this presents a number of challenges. A good way to begin the task of making ethics computable is to create a program that enables a machine to act an ethical advisor to human beings. This project, unlike creating an autonomous ethical machine, will not require that we make a judgment about the ethical status of the machine itself, a judgment that will be particularly difficult to make. Finally, it is argued that Asimov\u2019s \u201cthree laws of robotics\u201d are an unsatisfactory basis for machine ethics, regardless of the status of the machine.", "venue": "AI & SOCIETY", "year": 2008, "referenceCount": 24, "citationCount": 92, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2120952", "name": "S. Anderson"}]}}, {"contexts": ["Algorithms that make decisions can be considered blameworthy agents (Floridi and Sanders, 2004a; Wiltshire, 2015).", "Ethical decisions require agents to evaluate the desirability of different courses of actions which present conflicts between the interests of involved parties (Allen et al., 2006; Wiltshire, 2015).", "An ideal model for artificial moral agents based on heroic virtues is suggested by Wiltshire (2015), wherein algorithms are trained to be heroic and thus, moral.19\nOther approaches do not require ethical principles to serve as pillars of algorithmic decision-making frameworks."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "3fc249f82116676de841e94c7cb65fb68bad3bc5", "externalIds": {"DBLP": "journals/mima/Wiltshire15", "MAG": "2026271577", "DOI": "10.1007/s11023-015-9361-2"}, "url": "https://www.semanticscholar.org/paper/3fc249f82116676de841e94c7cb65fb68bad3bc5", "title": "A Prospective Framework for the Design of Ideal Artificial Moral Agents: Insights from the Science of Heroism in Humans", "abstract": "The growing field of machine morality has becoming increasingly concerned with how to develop artificial moral agents. However, there is little consensus on what constitutes an ideal moral agent let alone an artificial one. Leveraging a recent account of heroism in humans, the aim of this paper is to provide a prospective framework for conceptualizing, and in turn designing ideal artificial moral agents, namely those that would be considered heroic robots. First, an overview of what it means to be an artificial moral agent is provided. Then, an overview of a recent account of heroism that seeks to define the construct as the dynamic and interactive integration of character strengths (e.g., bravery and integrity) and situational constraints that afford the opportunity for moral behavior (i.e., moral affordances). With this as a foundation, a discussion is provided for what it might mean for a robot to be an ideal moral agent by proposing a dynamic and interactive connectionist model of robotic heroism. Given the limited accounts of robots engaging in moral behavior, a case for extending robotic moral capacities beyond just being a moral agent to the level of heroism is supported by drawing from exemplar situations where robots demonstrate heroism in popular film and fiction.", "venue": "Minds and Machines", "year": 2015, "referenceCount": 55, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Psychology"], "authors": [{"authorId": "2375973", "name": "Travis J. Wiltshire"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "3fecdc3b29be90366e21fadb9fec1981be55354d", "externalIds": {"MAG": "2051878417", "DOI": "10.1007/s10676-010-9255-1"}, "url": "https://www.semanticscholar.org/paper/3fecdc3b29be90366e21fadb9fec1981be55354d", "title": "Developing artificial agents worthy of trust: \u201cWould you buy a used car from this artificial agent?\u201d", "abstract": "There is a growing literature on the concept of e-trust and on the feasibility and advisability of \u201ctrusting\u201d artificial agents. In this paper we present an object-oriented model for thinking about trust in both face-to-face and digitally mediated environments. We review important recent contributions to this literature regarding e-trust in conjunction with presenting our model. We identify three important types of trust interactions and examine trust from the perspective of a software developer. Too often, the primary focus of research in this area has been on the artificial agents and the humans they may encounter after they are deployed. We contend that the humans who design, implement, and deploy the artificial agents are crucial to any discussion of e-trust and to understanding the distinctions among the concepts of trust, e-trust and face-to-face trust.", "venue": "Ethics and Information Technology", "year": 2011, "referenceCount": 39, "citationCount": 40, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2491009", "name": "F. Grodzinsky"}, {"authorId": "1754832", "name": "K. Miller"}, {"authorId": "49306743", "name": "M. J. Wolf"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "41f50057a2e67abefdf39b41d885138d870b0453", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/41f50057a2e67abefdf39b41d885138d870b0453", "title": "Transparent Predictions", "abstract": "Can human behavior be predicted? A broad variety of governmental initiatives are using computerized processes to try. Vast datasets of personal information enhance the ability to engage in these ventures and the appetite to push them forward. Governments have a distinct interest in automated individualized predictions to foresee unlawful actions. Novel technological tools, especially data-mining applications, are making governmental predictions possible. The growing use of predictive practices is generating serious concerns regarding the lack of transparency. Although echoed across the policy, legal, and academic debate, the nature of transparency, in this context, is unclear. Transparency flows from different, even competing, rationales, as well as very different legal and philosophical backgrounds. This Article sets forth a unique and comprehensive conceptual framework for understanding the role transparency must play as a regulatory concept in the crucial and innovative realm of automated predictive modeling. Part II begins by briefly describing the predictive modeling process while focusing on initiatives carried out in the context of federal income tax collection and law enforcement. It then draws out the process\u2019s fundamental elements, while distinguishing between the role of technology and humans. Recognizing these elements is crucial for understanding the importance and challenges of transparency. Part III moves to address the flow of information the prediction process generates. In doing so, it addresses various strategies to achieve transparency in this process \u2014 some addressed by law, while others are ignored. In doing so, the Article introduces a helpful taxonomy that will be relied upon throughout the analysis. It also establishes the need for an overall theoretical analysis and policy blueprint for transparency in prediction. Part IV shifts to a theoretical analysis seeking the sources of calls for transparency. Here, the analysis addresses transparency as a tool to enhance government efficiency, facilitate crowdsourcing, and promote both privacy and autonomy. Part V turns to examine counterarguments which call for limiting transparency. It explains how disclosure can undermine government policy and authority, as well as generate problematic stereotypes. After mapping out the justifications and counterclaims, Part VI moves to provide an innovative and unique policy framework for achieving transparency. The Article concludes, in Part VII, by explaining which concerns and risks of the predictive modeling process transparency cannot mitigate, and calling for other regulatory responses.", "venue": "", "year": 2013, "referenceCount": 4, "citationCount": 38, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1938823", "name": "Tal Z. Zarsky"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "4556f3f9463166aa3e27b2bec798c0ca7316bd65", "externalIds": {"MAG": "2097246321", "DBLP": "journals/datamine/CaldersV10", "DOI": "10.1007/s10618-010-0190-x"}, "url": "https://www.semanticscholar.org/paper/4556f3f9463166aa3e27b2bec798c0ca7316bd65", "title": "Three naive Bayes approaches for discrimination-free classification", "abstract": "In this paper, we investigate how to modify the naive Bayes classifier in order to perform classification that is restricted to be independent with respect to a given sensitive attribute. Such independency restrictions occur naturally when the decision process leading to the labels in the data-set was biased; e.g., due to gender or racial discrimination. This setting is motivated by many cases in which there exist laws that disallow a decision that is partly based on discrimination. Naive application of machine learning techniques would result in huge fines for companies. We present three approaches for making the naive Bayes classifier discrimination-free: (i) modifying the probability of the decision being positive, (ii) training one model for every sensitive attribute value and balancing them, and (iii) adding a latent variable to the Bayesian model that represents the unbiased label and optimizing the model parameters for likelihood using expectation maximization. We present experiments for the three approaches on both artificial and real-life data.", "venue": "Data Mining and Knowledge Discovery", "year": 2010, "referenceCount": 15, "citationCount": 574, "influentialCitationCount": 53, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1709830", "name": "T. Calders"}, {"authorId": "1682757", "name": "S. Verwer"}]}}, {"contexts": ["Online service providers continue to mediate how information is accessed with personalisation and filtering algorithms (Newell and Marabelli, 2015; Taddeo and Floridi, 2015)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "458fb4f0319e62e193f3ae1e3a111ad8c532814b", "externalIds": {"MAG": "101716117", "DOI": "10.1007/978-3-319-04093-6_17"}, "url": "https://www.semanticscholar.org/paper/458fb4f0319e62e193f3ae1e3a111ad8c532814b", "title": "Distributed Epistemic Responsibility in a Hyperconnected Era", "abstract": "The challenge to locate responsibility in ever more entangled and dynamic socio-technical environments is a key concern of the ONLIFE Manifesto. This contribution focuses specifically on responsibilities in processes of knowing, a topic which is discussed under the heading of epistemic responsibility in philosophy. I argue that two perspectives regarding epistemic responsibility should be distinguished: (1) the individualistic perspective, focusing on individuals as knowers within increasingly complex and dynamic socio-technical epistemic systems and (2) the governance perspective, focusing on the question how systems and environments should be designed so that individuals can act responsibly. Different fields of research have offered valuable insights for the development of a notion of epistemic responsibility in a hyperconnected era, most notably the fields of (social) epistemology, philosophy of computing as well as feminist theory of science and technology. From those insights, two major challenges can be deduced: (1) To acknowledge the socio-technical entanglement of knowers while at the same time striving to support responsibility assumption and attribution and (2) to be attentive to power asymmetries within entangled socio-technical environments.", "venue": "", "year": 2015, "referenceCount": 34, "citationCount": 33, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Geography"], "authors": [{"authorId": "17736848", "name": "Judith Simon"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "4794714ef64a90457409e3cc6782bed5f70b6e61", "externalIds": {"MAG": "1602070487"}, "url": "https://www.semanticscholar.org/paper/4794714ef64a90457409e3cc6782bed5f70b6e61", "title": "Cybertypes: Race, Ethnicity, and Identity on the Internet", "abstract": "From the Publisher: \nCyberspace entices us with the promise of an online utopia\u0097a web of fluid identities and infinite possibilities. When we look for signs of freedom online\u0097anywhere from chat room conversations to cyberpunk fiction\u0097we are almost inevitably urged toward \"liberation\" from our bodies and their \"restrictive\" attributes like race, gender, and age. But cyberculture critic Lisa Nakamura insists that the Internet is a place where race matters. \nRace itself may not be fixed or finite, but Nakamura argues that racial stereotypes-or \"cybertypes\"-are hardwired into our online interactions: Identity tourists masquerade in virtual roles like Asian_Geisha and Alatinolover. Web directories sharply narrow racial categories. Anonymous computer users are assumed to be white. \nIn Cybertypes, Nakamura looks at what happened to race when it went online, and how our ideas about race continue to be shaped and reshaped every time we log on. Examining all facets of our everyday online experience from Internet advertising to email jokes, Nakamura shows that the postmodern ideal of fluid selves made possible by network technology is not necessarily subversive, progressive, or liberating. The harder race is pushed off-line, the greater the consequences in real life for people of color. \nA lively and provocative discussion Cybertypes offers a valuable new way of thinking about race and identity in the information age.", "venue": "", "year": 2002, "referenceCount": 0, "citationCount": 698, "influentialCitationCount": 54, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "89364752", "name": "Lisa Nakamura"}]}}, {"contexts": ["Algorithms, particularly those embedded in robotics, can for instance be made safely interruptible insofar as harmful actions can be discouraged without the algorithm being encouraged to deceive human users to avoid further interruptions (Orseau and Armstrong, 2016)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "496934919e514784a1ee7236b025ffd1a2be0876", "externalIds": {"MAG": "1676609437", "DOI": "10.1177/0162243915598056"}, "url": "https://www.semanticscholar.org/paper/496934919e514784a1ee7236b025ffd1a2be0876", "title": "Bearing Account-able Witness to the Ethical Algorithmic System", "abstract": "This paper explores how accountability might make otherwise obscure and inaccessible algorithms available for governance. The potential import and difficulty of accountability is made clear in the compelling narrative reproduced across recent popular and academic reports. Through this narrative we are told that algorithms trap us and control our lives, undermine our privacy, have power and an independent agential impact, at the same time as being inaccessible, reducing our opportunities for critical engagement. The paper suggests that STS sensibilities can provide a basis for scrutinizing the terms of the compelling narrative, disturbing the notion that algorithms have a single, essential characteristic and a predictable power or agency. In place of taking for granted the terms of the compelling narrative, ethnomethodological work on sense-making accounts is drawn together with more conventional approaches to accountability focused on openness and transparency. The paper uses empirical material from a study of the development of an \u201cethical,\u201d \u201csmart\u201d algorithmic videosurveillance system. The paper introduces the \u201cethical\u201d algorithmic surveillance system, the approach to accountability developed, and some of the challenges of attempting algorithmic accountability in action. The paper concludes with reflections on future questions of algorithms and accountability.", "venue": "", "year": 2016, "referenceCount": 100, "citationCount": 82, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "2949821", "name": "D. Neyland"}]}}, {"contexts": ["\u2026from informational privacy interests suggests that opaque or secretive profiling is problematic.17 Opaque decision-making by algorithms (see \u2018Inconclusive evidence leading to unjustified actions\u2019 section) inhibits oversight and informed decision-making concerning data sharing (Kim et al., 2014).", "Notable contemporary examples include online software agents used by online service providers to carry out operations on the behalf of users (Kim et al., 2014); online dispute resolution algorithms that replace human decisionmakers in dispute mediation (Raymond, 2014; Shackelford and Raymond,\u2026"], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "499967b0dc1946ad42cd6274577f1dded3ffbdb1", "externalIds": {"DBLP": "journals/ijhci/KimGM14", "DOI": "10.1080/10447318.2014.925383"}, "url": "https://www.semanticscholar.org/paper/499967b0dc1946ad42cd6274577f1dded3ffbdb1", "title": "A Qualitative Study of Stakeholders\u2019 Perspectives on the Social Network Service Environment", "abstract": "More than 2 billion people are using the Internet at present, assisted by the mediating activities of software agents that deal with the diversity and complexity of information. There are, however, ethical issues due to the monitoring-and-surveillance, data-mining, and autonomous nature of software agents. Considering the context, this study aims to comprehend stakeholders\u2019 perspectives on the social network service environment to identify the main considerations for the design of software agents in social network services in the near future. Twenty-one stakeholders, belonging to 3 key stakeholder groups, were recruited using a purposive sampling strategy for unstandardized semistructured e-mail interviews. The interview data were analyzed using a qualitative content analysis method. It was possible to identify 3 main considerations for the design of software agents in social network services, which were classified into the following categories: comprehensive understanding of users\u2019 perception of privacy, user type recognition algorithms for software agent development, and existing software agents enhancement.", "venue": "Int. J. Hum. Comput. Interact.", "year": 2014, "referenceCount": 132, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "52330529", "name": "Hojung Kim"}, {"authorId": "29897317", "name": "J. Giacomin"}, {"authorId": "1752902", "name": "R. Macredie"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "4a57fb370b83ac3d1a886a3fae479e45e5fe156d", "externalIds": {"DBLP": "journals/tois/FriedmanN96", "MAG": "1607120026", "DOI": "10.1145/230538.230561"}, "url": "https://www.semanticscholar.org/paper/4a57fb370b83ac3d1a886a3fae479e45e5fe156d", "title": "Bias in computer systems", "abstract": "From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints of considerations. Emergent bias arises in a context of use. Although others have pointed to bias inparticular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should by counted amoung the select set of criteria\u2014including reliability, accuracy, and efficiency\u2014according to which the quality of systems in use in society should be judged.", "venue": "TOIS", "year": 1996, "referenceCount": 32, "citationCount": 621, "influentialCitationCount": 24, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Sociology"], "authors": [{"authorId": "144029598", "name": "Batya Friedman"}, {"authorId": "2994505", "name": "H. Nissenbaum"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "4b46d0a649c337c8190bb28cfb5acbd9490ab45b", "externalIds": {"DBLP": "journals/ijsr/WiegelB09", "MAG": "1993244199", "DOI": "10.1007/s12369-009-0023-5"}, "url": "https://www.semanticscholar.org/paper/4b46d0a649c337c8190bb28cfb5acbd9490ab45b", "title": "Combining Moral Theory, Modal Logic and Mas to Create Well-Behaving Artificial Agents", "abstract": "Witnessing a growing number of increasingly autonomous software agents we interact with or that operate on our behalf under circumstances that are not fully known in advance, we argue that there is a need to provide these agents with moral reasoning capabilities. Looking at the current literature on behaviour constraints and multi-agent (software) systems (MAS), one can distinguish various topics. The first topic concerns the analysis of various forms of restraint and their basis. This topic is at the core of moral philosophy. The second topic concerns the formalized specification of, and the reasoning about the constraints. The research on this topic focuses predominantly on the use of logic, mostly modal logic, and defeasible logic. The last topic is the MAS and implementation related topic of designing a working system in which there are rules that can be enforced and deviant behaviour be detected.Here we argue that all three topics need addressing and strong integration. The moral philosophical analysis is needed to provide a detailed conceptualization of the various forms of behaviour constraint and direction. This analysis goes beyond what is usual in the more technical/design focus. The (modal) logic provides the rigour required to ultimately allow implementation. The implementation itself is the ultimate objective. We outline the three components and demonstrate how they can be integrated. We observe here that we do not intend, or claim, that this moral reasoning is on par with human moral reasoning. Our claim is that the analysis of human moral reasoning may provide a useful model for constraining software agent behaviour. And, as equally important, it is recognizable by humans which is an important characteristic when it comes to \u2018human\u2013artificial agent\u2019 interaction. Recognizing and understanding the precise basis for the behaviour constraint in the artificial entity will make the agent more trustful which, in its turn, will facilitate the acceptance of the use of and the interaction with artificial agents.", "venue": "Int. J. Soc. Robotics", "year": 2009, "referenceCount": 21, "citationCount": 20, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3219770", "name": "V. Wiegel"}, {"authorId": "143617123", "name": "J. Berg"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "4eadd77be64cdb4709cbe20cf2772ca60655159e", "externalIds": {"MAG": "1966600096", "DOI": "10.1007/s10676-006-9128-9"}, "url": "https://www.semanticscholar.org/paper/4eadd77be64cdb4709cbe20cf2772ca60655159e", "title": "Ethical protocols design", "abstract": "The paper offers a solution to the problem of specifying computational systems that behave in accordance with a given set of ethical principles. The proposed solution is based on the concepts of ethical requirements and ethical protocols. A new conceptual tool, called the Control Closure of an operation, is defined and used to translate ethical principles into ethical requirements and protocols. The concept of Generalised Informational Privacy (GIP) is used as a paradigmatic example of an ethical principle. GIP is defined in such a way as to (i) discriminate specific cases in which an individual\u2019s GIP can be infringed without accessing the individual\u2019s data; (ii) separate unauthorised accesses to data that do not respect the right to GIP from access that do; and (iii) distinguish different degrees of GIP. Finally a camera phone is used to illustrate the proposed solution.", "venue": "Ethics and Information Technology", "year": 2007, "referenceCount": 25, "citationCount": 42, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "1840916", "name": "M. Turilli"}]}}, {"contexts": ["Profiling and classification algorithms determine how individuals and groups are shaped and managed (Floridi, 2012).", "Profiling seeks to assemble individuals into meaningful groups, for which identity is irrelevant (Floridi, 2012; Hildebrandt, 2011; Leese, 2014).", "Analytics identifies relationships and small patterns across vast and distributed datasets (Floridi, 2012)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "4ebeeda4f692e16e7f0588193b4c3dbe9f823cf2", "externalIds": {"MAG": "2024355436", "DOI": "10.1007/S13347-012-0093-4"}, "url": "https://www.semanticscholar.org/paper/4ebeeda4f692e16e7f0588193b4c3dbe9f823cf2", "title": "Big Data and Their Epistemological Challenge", "abstract": "It is estimated that humanity accumulated 180 EB of data between the invention of writing and 2006. Between 2006 and 2011, the total grew ten times and reached 1,600 EB. This figure is now expected to grow fourfold approximately every 3 years. Every day, enough new data are being generated to fill all US libraries eight times over. As a result, there is much talk about \u201cbig data\u201d. This special issue on \u201cEvolution, Genetic Engineering and Human Enhancement\u201d, for example, would have been inconceivable in an age of \u201csmall data\u201d, simply because genetics is one of the datagreediest sciences around. This is why, in the USA, the National Institutes of Health (NIH) and the National Science Foundation (NSF) have identified big data as a programme focus. One of the main NSF\u2013NIH interagency initiatives addresses the need for core techniques and technologies for advancing big data science and engineering (see NSF-12-499). Despite the importance of the phenomenon, it is unclear what exactly the term \u201cbig data\u201d means and hence refers to. The aforementioned document specifies that: \u201cThe phrase \u2018big data\u2019 in this solicitation refers to large, diverse, complex, longitudinal, and/or distributed data sets generated from instruments, sensors, Internet transactions, email, video, click streams, and/or all other digital sources available today and in the future.\u201d You do not need to be an analytic philosopher to find this both obscure and vague. Wikipedia, for once, is also unhelpful. Not because the relevant entry is unreliable, but because it reports the common definition, which is unsatisfactory: \u201cdata sets so large and complex that they become awkward to work with using onhand database management tools\u201d. Apart from the circular problem of defining \u201cbig\u201d with \u201clarge\u201d, the definition suggests that data are too big or large only in relation to our current computational power. This is misleading. Of course, \u201cbig\u201d, as many other terms, is a relational predicate: a pair of shoes is too big for you, but fine for me. It is also trivial to acknowledge that we tend to evaluate things non-relationally, in this case as absolutely big, whenever the frame of reference is obvious enough to be left Philos. Technol. (2012) 25:435\u2013437 DOI 10.1007/s13347-012-0093-4", "venue": "", "year": 2012, "referenceCount": 0, "citationCount": 138, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1982425", "name": "L. Floridi"}]}}, {"contexts": ["However, trust can also exist among artificial agents exclusively, seen for instance in the agents of a distributed system working cooperatively to achieve a given goal (Grodzinsky et al., 2010; Simon, 2010; Taddeo, 2010)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "4f67f1d0b1836ffcba066659a896a5bb413ff3bc", "externalIds": {"MAG": "2080205018", "DOI": "10.1007/s10676-010-9243-5"}, "url": "https://www.semanticscholar.org/paper/4f67f1d0b1836ffcba066659a896a5bb413ff3bc", "title": "The entanglement of trust and knowledge on the Web", "abstract": "In this paper I use philosophical accounts on the relationship between trust and knowledge in science to apprehend this relationship on the Web. I argue that trust and knowledge are fundamentally entangled in our epistemic practices. Yet despite this fundamental entanglement, we do not trust blindly. Instead we make use of knowledge to rationally place or withdraw trust. We use knowledge about the sources of epistemic content as well as general background knowledge to assess epistemic claims. Hence, although we may have a default to trust, we remain and should remain epistemically vigilant; we look out and need to look out for signs of insincerity and dishonesty in our attempts to know. A fundamental requirement for such vigilance is transparency: in order to critically assess epistemic agents, content and processes, we need to be able to access and address them. On the Web, this request for transparency becomes particularly pressing if (a) trust is placed in unknown human epistemic agents and (b) if it is placed in non-human agents, such as algorithms. I give examples of the entanglement between knowledge and trust on the Web and draw conclusions about the forms of transparency needed in such systems to support epistemically vigilant behaviour, which empowers users to become responsible and accountable knowers.", "venue": "Ethics and Information Technology", "year": 2010, "referenceCount": 33, "citationCount": 46, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "17736848", "name": "Judith Simon"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "51de0f43ce5dd44117fd631ad0eb8fa8e95e8434", "externalIds": {"DBLP": "conf/icdm/HajianMPDG12", "MAG": "2051727062", "DOI": "10.1109/ICDMW.2012.51"}, "url": "https://www.semanticscholar.org/paper/51de0f43ce5dd44117fd631ad0eb8fa8e95e8434", "title": "Injecting Discrimination and Privacy Awareness Into Pattern Discovery", "abstract": "Data mining is gaining societal momentum due to the ever increasing availability of large amounts of human data, easily collected by a variety of sensing technologies. Data mining comes with unprecedented opportunities and risks: a deeper understanding of human behavior and how our society works is darkened by a greater chance of privacy intrusion and unfair discrimination based on the extracted patterns and profiles. Although methods independently addressing privacy or discrimination in data mining have been proposed in the literature, in this context we argue that privacy and discrimination risks should be tackled together, and we present a methodology for doing so while publishing frequent pattern mining results. We describe a combined pattern sanitization framework that yields both privacy and discrimination-protected patterns, while introducing reasonable (controlled) pattern distortion.", "venue": "2012 IEEE 12th International Conference on Data Mining Workshops", "year": 2012, "referenceCount": 28, "citationCount": 26, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2075582", "name": "S. Hajian"}, {"authorId": "2296962", "name": "A. Monreale"}, {"authorId": "1693341", "name": "D. Pedreschi"}, {"authorId": "1393591007", "name": "J. Domingo-Ferrer"}, {"authorId": "1685102", "name": "F. Giannotti"}]}}, {"contexts": ["Professionals have implicit knowledge and subtle skills (cf. Coeckelbergh, 2013; MacIntyre, 2007) that are difficult to make explicit and perhaps impossible to make computable (Morek, 2006)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "53092b7c052cd9bb623039ff244181f13b464dea", "externalIds": {"MAG": "350296131"}, "url": "https://www.semanticscholar.org/paper/53092b7c052cd9bb623039ff244181f13b464dea", "title": "After virtue, a study in moral theory", "abstract": null, "venue": "", "year": 1983, "referenceCount": 0, "citationCount": 1909, "influentialCitationCount": 151, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "101188484", "name": "A. Edel"}, {"authorId": "145686346", "name": "E. Flower"}]}}, {"contexts": ["\u2026may be possible to direct algorithms not to consider sensitive attributes that contribute to discrimination (Barocas and Selbst, 2015), such as gender or ethnicity (Calders et al., 2009; Kamiran and Calders, 2010; Schermer, 2011), based upon the emergence of discrimination in a particular context."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "595538decb1228a9fbeb0a1df3581c64dea95dd7", "externalIds": {"DBLP": "conf/icdm/CaldersKP09", "MAG": "2116666691", "DOI": "10.1109/ICDMW.2009.83"}, "url": "https://www.semanticscholar.org/paper/595538decb1228a9fbeb0a1df3581c64dea95dd7", "title": "Building Classifiers with Independency Constraints", "abstract": "In this paper we study the problem of classifier learning where the input data contains unjustified dependencies between some data attributes and the class label. Such cases arise for example when the training data is collected from different sources with different labeling criteria or when the data is generated by a biased decision process. When a classifier is trained directly on such data, these undesirable dependencies will carry over to the classifier\u2019s predictions. In order to tackle this problem, we study the classification with independency constraints problem: find an accurate model for which the predictions are independent from a given binary attribute. We propose two solutions for this problem and present an empirical validation.", "venue": "2009 IEEE International Conference on Data Mining Workshops", "year": 2009, "referenceCount": 15, "citationCount": 273, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1709830", "name": "T. Calders"}, {"authorId": "2054808", "name": "F. Kamiran"}, {"authorId": "1691997", "name": "Mykola Pechenizkiy"}]}}, {"contexts": ["Individuals\u2019 choices are structured according to information about the group (Danna and Gandy, 2002: 382).", "Questions of the fairness and equitability of such practices are often raised (e.g. Cohen et al., 2014; Danna and Gandy, 2002; Rubel and Jones, 2014).", "Danna and Gandy (2002) provide a demonstrative exam-\nple in the Royal Bank of Canada which \u2018nudged\u2019 customers on fee-for-service to flat-fee service packages after discovering (through mining in-house data) that customers on the latter offered greater lifetime value to the bank."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "59cb9baed12410bd989e627f5795670d73d5eaf7", "externalIds": {"MAG": "1554197503", "DOI": "10.1023/A:1020845814009"}, "url": "https://www.semanticscholar.org/paper/59cb9baed12410bd989e627f5795670d73d5eaf7", "title": "All That Glitters is Not Gold: Digging Beneath the Surface of Data Mining", "abstract": "This article develops a more comprehensive understanding of data mining by examining the application of this technology in the marketplace. In addition to exploring the technological issues that arise from the use of these applications, we address some of the social concerns that are too often ignored.As more firms shift more of their business activities to the Web, increasingly more information about consumers and potential customers is being captured in Web server logs. Sophisticated analytic and data mining software tools enable firms to use the data contained in these logs to develop and implement a complex relationship management strategy. Although this new trend in marketing strategy is based on the old idea of relating to customers as individuals, customer relationship management actually rests on segmenting consumers into groups based on profiles developed through a firm's data mining activities. Individuals whose profiles suggest that they are likely to provide a high lifetime value to the firm are served content that will vary from that which is served to consumers with less attractive profiles.Social costs may be imposed on society when objectively rational business decisions involving data mining and consumer profiles are made. The ensuing discussion examines the ways in which data mining and the use of consumer profiles may exclude classes of consumers from full participation in the marketplace, and may limit their access to information essential to their full participation as citizens in the public sphere. We suggest more ethically sensitive alternatives to the unfettered use of data mining.", "venue": "", "year": 2002, "referenceCount": 60, "citationCount": 133, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "150898006", "name": "A. Danna"}, {"authorId": "1486680764", "name": "O. Gandy"}]}}, {"contexts": ["2015; Birrer, 2005), as seen in delivery of online advertisements according to perceived ethnicity (Sweeney, 2013)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "5ab4fa95ed0097e72ff9a94dee5e45bcec30395c", "externalIds": {"DBLP": "journals/cacm/Sweeney13", "MAG": "2953001331", "ArXiv": "1301.6822", "DOI": "10.1145/2447976.2447990"}, "url": "https://www.semanticscholar.org/paper/5ab4fa95ed0097e72ff9a94dee5e45bcec30395c", "title": "Discrimination in online ad delivery", "abstract": "Google ads, black names and white names, racial discrimination, and click advertising.", "venue": "CACM", "year": 2013, "referenceCount": 28, "citationCount": 538, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Psychology"], "authors": [{"authorId": "144851214", "name": "L. Sweeney"}]}}, {"contexts": ["Further work is required to design broadly applicable, low impact auditing mechanisms for algorithms (cf. Adler et al., 2016; Sandvig et al., 2014) that build upon current work in transparency and interpretability of machine learning (e.g. Kim et al., 2015; Lou et al., 2013)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "5b3791047274b9fb7a89bc28f69c70bbd4c4843e", "externalIds": {"DBLP": "conf/kdd/LouCGH13", "MAG": "2046945713", "DOI": "10.1145/2487575.2487579"}, "url": "https://www.semanticscholar.org/paper/5b3791047274b9fb7a89bc28f69c70bbd4c4843e", "title": "Accurate intelligible models with pairwise interactions", "abstract": "Standard generalized additive models (GAMs) usually model the dependent variable as a sum of univariate models. Although previous studies have shown that standard GAMs can be interpreted by users, their accuracy is significantly less than more complex models that permit interactions. In this paper, we suggest adding selected terms of interacting pairs of features to standard GAMs. The resulting models, which we call GA2{M}$-models, for Generalized Additive Models plus Interactions, consist of univariate terms and a small number of pairwise interaction terms. Since these models only include one- and two-dimensional components, the components of GA2M-models can be visualized and interpreted by users. To explore the huge (quadratic) number of pairs of features, we develop a novel, computationally efficient method called FAST for ranking all possible pairs of features as candidates for inclusion into the model. In a large-scale empirical study, we show the effectiveness of FAST in ranking candidate pairs of features. In addition, we show the surprising result that GA2M-models have almost the same performance as the best full-complexity models on a number of real datasets. Thus this paper postulates that for many problems, GA2M-models can yield models that are both intelligible and accurate.", "venue": "KDD", "year": 2013, "referenceCount": 19, "citationCount": 203, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "91993942", "name": "Yin Lou"}, {"authorId": "145727186", "name": "R. Caruana"}, {"authorId": "143614516", "name": "J. Gehrke"}, {"authorId": "145403451", "name": "G. Hooker"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "5c5f5607168e7508db6f3d98a3bd9a22bdd98dee", "externalIds": {"MAG": "2149394571", "DOI": "10.1023/B:ETIN.0000047476.05912.3d"}, "url": "https://www.semanticscholar.org/paper/5c5f5607168e7508db6f3d98a3bd9a22bdd98dee", "title": "Ethical issues in web data mining", "abstract": "Web mining refers to the whole of data miningand related techniques that are used toautomatically discover and extract informationfrom web documents and services. When used in abusiness context and applied to some type ofpersonal data, it helps companies to builddetailed customer profiles, and gain marketingintelligence. Web mining does, however, pose athreat to some important ethical values likeprivacy and individuality. Web mining makes itdifficult for an individual to autonomouslycontrol the unveiling and dissemination of dataabout his/her private life. To study thesethreats, we distinguish between `content andstructure mining' and `usage mining.' Webcontent and structure mining is a cause forconcern when data published on the web in acertain context is mined and combined withother data for use in a totally differentcontext. Web usage mining raises privacyconcerns when web users are traced, and theiractions are analysed without their knowledge.Furthermore, both types of web mining are oftenused to create customer files with a strongtendency of judging and treating people on thebasis of group characteristics instead of ontheir own individual characteristics and merits(referred to as de-individualisation). Althoughthere are a variety of solutions toprivacy-problems, none of these solutionsoffers sufficient protection. Only a combinedsolution package consisting of solutions at anindividual as well as a collective level cancontribute to release some of the tensionbetween the advantages and the disadvantages ofweb mining. The values of privacy andindividuality should be respected and protectedto make sure that people are judged and treatedfairly. People should be aware of theseriousness of the dangers and continuouslydiscuss these ethical issues. This should be ajoint responsibility shared by web miners (bothadopters and developers), web users, andgovernments.", "venue": "Ethics and Information Technology", "year": 2004, "referenceCount": 27, "citationCount": 123, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "147379905", "name": "Lita van Wel"}, {"authorId": "1788365", "name": "Lamb\u00e8r M. M. Royakkers"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "6092ccf125ecf0d1f89f015a5d6ca1491f5c7c49", "externalIds": {"DBLP": "journals/synthese/FloridiFP15", "MAG": "2006692266", "DOI": "10.1007/s11229-014-0610-3"}, "url": "https://www.semanticscholar.org/paper/6092ccf125ecf0d1f89f015a5d6ca1491f5c7c49", "title": "On malfunctioning software", "abstract": "Artefacts do not always do what they are supposed to, due to a variety of reasons, including manufacturing problems, poor maintenance, and normal wear-and-tear. Since software is an artefact, it should be subject to malfunctioning in the same sense in which other artefacts can malfunction. Yet, whether software is on a par with other artefacts when it comes to malfunctioning crucially depends on the abstraction used in the analysis. We distinguish between \u201cnegative\u201d and \u201cpositive\u201d notions of malfunction. A negative malfunction, or dysfunction, occurs when an artefact token either does not (sometimes) or cannot (ever) do what it is supposed to. A positive malfunction, or misfunction, occurs when an artefact token may do what is supposed to but, at least occasionally, it also yields some unintended and undesirable effects. We argue that software, understood as type, may misfunction in some limited sense, but cannot dysfunction. Accordingly, one should distinguish software from other technical artefacts, in view of their design that makes dysfunction impossible for the former, while possible for the latter.", "venue": "Synthese", "year": 2015, "referenceCount": 57, "citationCount": 33, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1982425", "name": "L. Floridi"}, {"authorId": "3263628", "name": "Nir Fresco"}, {"authorId": "2614511", "name": "G. Primiero"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "6333194385a86ed4f78a92f8e5e85450a4597ad5", "externalIds": {"MAG": "2405892821", "DOI": "10.4037/AJCC1994.3.2.87", "PubMed": "8167780"}, "url": "https://www.semanticscholar.org/paper/6333194385a86ed4f78a92f8e5e85450a4597ad5", "title": "Controversial decisions regarding treatment and DNR: an algorithmic Guide for the Uncertain in Decision-Making Ethics (GUIDE).", "abstract": "We describe an algorithm to aid clinicians in making ethical decisions regarding interventions. Logically sequenced questions about competency, advance directives, treatment benefit, and patient and family preferences guide the decision maker to nine specific scenarios. Each scenario includes guidelines for action based on legal and ethical consensus.", "venue": "American journal of critical care : an official publication, American Association of Critical-Care Nurses", "year": 1994, "referenceCount": 0, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "2086629545", "name": "J. Levenson"}, {"authorId": "14592388", "name": "L. Pettrey"}]}}, {"contexts": ["The algorithm modifies its behavioural structure during operation (Markowetz et al., 2014)."], "isInfluential": false, "intents": ["methodology"], "citedPaper": {"paperId": "63ec3a3a26a483c9472c7704e17670c8251d4228", "externalIds": {"MAG": "2062808044", "DOI": "10.1016/j.mehy.2013.11.030", "PubMed": "24529915"}, "url": "https://www.semanticscholar.org/paper/63ec3a3a26a483c9472c7704e17670c8251d4228", "title": "Psycho-informatics: Big Data shaping modern psychometrics.", "abstract": "For the first time in history, it is possible to study human behavior on great scale and in fine detail simultaneously. Online services and ubiquitous computational devices, such as smartphones and modern cars, record our everyday activity. The resulting Big Data offers unprecedented opportunities for tracking and analyzing behavior. This paper hypothesizes the applicability and impact of Big Data technologies in the context of psychometrics both for research and clinical applications. It first outlines the state of the art, including the severe shortcomings with respect to quality and quantity of the resulting data. It then presents a technological vision, comprised of (i) numerous data sources such as mobile devices and sensors, (ii) a central data store, and (iii) an analytical platform, employing techniques from data mining and machine learning. To further illustrate the dramatic benefits of the proposed methodologies, the paper then outlines two current projects, logging and analyzing smartphone usage. One such study attempts to thereby quantify severity of major depression dynamically; the other investigates (mobile) Internet Addiction. Finally, the paper addresses some of the ethical issues inherent to Big Data technologies. In summary, the proposed approach is about to induce the single biggest methodological shift since the beginning of psychology or psychiatry. The resulting range of applications will dramatically shape the daily routines of researches and medical practitioners alike. Indeed, transferring techniques from computer science to psychiatry and psychology is about to establish Psycho-Informatics, an entire research direction of its own.", "venue": "Medical hypotheses", "year": 2014, "referenceCount": 24, "citationCount": 123, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1747393", "name": "A. Markowetz"}, {"authorId": "3455314", "name": "Konrad Blaszkiewicz"}, {"authorId": "2720403", "name": "C. Montag"}, {"authorId": "9582588", "name": "C. Switala"}, {"authorId": "2721378", "name": "T. Schlaepfer"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "64b0e0fb809c231abced4231d31f2c65c7b1a009", "externalIds": {"MAG": "2312738617", "DOI": "10.1177/0967010614544204"}, "url": "https://www.semanticscholar.org/paper/64b0e0fb809c231abced4231d31f2c65c7b1a009", "title": "The new profiling: Algorithms, black boxes, and the failure of anti-discriminatory safeguards in the European Union", "abstract": "This article argues that with increasingly large databases and computational power, profiling as a key part of security governance is experiencing major changes. Targeting mobile populations in order to enact security via controlling and sifting the good from the bad, profiling techniques accumulate and process personal data. However, as advanced algorithmic analytics enable authorities to make sense of unprecedented amounts of information and derive patterns in a data-driven fashion, the procedures that bring risk into being increasingly differ from those of traditional profiling. While several scholars have dealt with the consequences of black-boxed and invisible algorithmic analytics in terms of privacy and data protection, this article engages the effects of knowledge-generating algorithms on anti-discriminatory safeguards. Using the European-level efforts for the establishment of a Passenger Name Record (PNR) system as an example, and on the theoretical level connecting distinct modes of profiling with Foucauldian thought on governing, the article finds that with pattern-based categorizations in data-driven profiling, safeguards such as the Charter of Fundamental Rights of the European Union or the EU data-protection framework essentially lose their applicability, leading to a diminishing role of the tools of the anti-discrimination framework.", "venue": "", "year": 2014, "referenceCount": 89, "citationCount": 113, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "12365785", "name": "M. Leese"}]}}, {"contexts": ["The moral standing and capacity for ethical decision-making of algorithms remains a\nstandout question in machine ethics (e.g. Allen et al., 2006; Anderson, 2008; Floridi and Sanders, 2004a).", "The construction of artificial morality is seen as the immediate and imminently achievable challenge for machine ethics, as it does not first require artificial intelligence (Allen et al., 2006).", "Ethical decisions require agents to evaluate the desirability of different courses of actions which present conflicts between the interests of involved parties (Allen et al., 2006; Wiltshire, 2015)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "6609ffd2e42fa9552b613d5ceeaee39ee8515b9a", "externalIds": {"DBLP": "journals/expert/AllenWS06", "MAG": "2125906904", "DOI": "10.1109/MIS.2006.83"}, "url": "https://www.semanticscholar.org/paper/6609ffd2e42fa9552b613d5ceeaee39ee8515b9a", "title": "Why Machine Ethics?", "abstract": "Machine ethics isn't merely science fiction; it's a topic that requires serious consideration, given the rapid emergence of increasingly complex autonomous software agents and robots. Machine ethics is an emerging field that seeks to implement moral decision-making faculties in computers and robots. We already have semiautonomous robots and software agents that violate ethical standards as a matter of course. In the case of AI and robotics, fearful scenarios range from the future takeover of humanity by a superior form of AI to the havoc created by endlessly reproducing nanobots", "venue": "IEEE Intelligent Systems", "year": 2006, "referenceCount": 15, "citationCount": 202, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "30308222", "name": "C. Allen"}, {"authorId": "143836888", "name": "Wendell Wallach"}, {"authorId": "7935872", "name": "I. \u0160mit"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "682de6c921966c3a677fb2ce86d50f6c7e313f1f", "externalIds": {"MAG": "2127439640", "DOI": "10.1007/s10676-009-9215-9"}, "url": "https://www.semanticscholar.org/paper/682de6c921966c3a677fb2ce86d50f6c7e313f1f", "title": "Identity, profiling algorithms and a world of ambient intelligence", "abstract": "The tendency towards an increasing integration of the informational web into our daily physical world (in particular in so-called Ambient Intelligent technologies which combine ideas derived from the field of Ubiquitous Computing, Intelligent User Interfaces and Ubiquitous Communication) is likely to make the development of successful profiling and personalization algorithms, like the ones currently used by internet companies such as Amazon, even more important than it is today. I argue that the way in which we experience ourselves necessarily goes through a moment of technical mediation. Because of this algorithmic profiling that thrives on continuous reconfiguration of identification should not be understood as a supplementary process which maps a pre-established identity that exists independently from the profiling practice. In order to clarify how the experience of one\u2019s identity can become affected by such machine-profiling a theoretical exploration of identity is made (including Agamben\u2019s understanding of an apparatus, Ricoeur\u2019s distinction between idem- and ipse-identity, and Stiegler\u2019s notion of a conjunctive\u2013disjunctive relationship towards retentional apparatuses). Although it is clear that no specific predictions about the impact of Ambient Intelligent technologies can be made without taking more particulars into account, the theoretical concepts are used to describe three general scenarios about the way wherein the experience of identity might become affected. To conclude, I argue that the experience of one\u2019s identity may affect whether the cases of unwarranted discrimination resulting from ubiquitous differentiations and identifications within an Ambient Intelligent environment, will become a matter of societal concern.", "venue": "Ethics and Information Technology", "year": 2010, "referenceCount": 83, "citationCount": 39, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3003529", "name": "K. Vries"}]}}, {"contexts": ["While Shannon\u2019s mathematical theory of communication (Shannon and Weaver, 1998), and especially some of his information-inequalities, give a formally precise account of this fact, the informal \u2018garbage in, garbage out\u2019 principle clearly illustrates what is at stake here, namely that conclusions can\u2026"], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "6d12a1d23b21a9b170118a56386552bc5d4727de", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/6d12a1d23b21a9b170118a56386552bc5d4727de", "title": "A Mathematical Theory of Communication", "abstract": "This paper opened the new area the information theory. Before this paper, most people believed that the only way to make the error probability of transmission as small as desired is to reduce the data rate (such as a long repetition scheme). However, surprisingly this paper revealed that it does not need to reduce the data rate for achieving that much of small errors. It proved that we can get some positive data rate that has the same small error probability and also there is an upper bound of the data rate, which means we cannot achieve the data rate with any encoding scheme that has small enough error probability over the upper bound.", "venue": "", "year": 2006, "referenceCount": 0, "citationCount": 44583, "influentialCitationCount": 4015, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "144977509", "name": "Jin Shin"}, {"authorId": "2109828882", "name": "Sang Joon Kim"}]}}, {"contexts": ["Besides being accessible, information must be comprehensible to be considered transparent (Turilli and Floridi, 2009).", "Consistency can be confirmed between the protocol (consisting of a decisionmaking structure) and the designer\u2019s or organisation\u2019s explicit ethical principles (Turilli and Floridi, 2009).", "However, this would appear to discount advances in data visualization and sorting techniques to help humans comprehend large datasets and information flows (cf. Turilli and Floridi, 2009).", "Transparency is generally defined with respect to \u2018\u2018the availability of information, the conditions of accessibility and how the information . . . may pragmatically or epistemically support the user\u2019s decision-making process\u2019\u2019 (Turilli and Floridi, 2009: 106)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "70fbbc02fe02a755bc5915c4a6fc00da814492ad", "externalIds": {"MAG": "2136635136", "DOI": "10.1007/s10676-009-9187-9"}, "url": "https://www.semanticscholar.org/paper/70fbbc02fe02a755bc5915c4a6fc00da814492ad", "title": "The ethics of information transparency", "abstract": "The paper investigates the ethics of information transparency (henceforth transparency). It argues that transparency is not an ethical principle in itself but a pro-ethical condition for enabling or impairing other ethical practices or principles. A new definition of transparency is offered in order to take into account the dynamics of information production and the differences between data and information. It is then argued that the proposed definition provides a better understanding of what sort of information should be disclosed and what sort of information should be used in order to implement and make effective the ethical practices and principles to which an organisation is committed. The concepts of \u201cheterogeneous organisation\u201d and \u201cautonomous computational artefact\u201d are further defined in order to clarify the ethical implications of the technology used in implementing information transparency. It is argued that explicit ethical designs, which describe how ethical principles are embedded into the practice of software design, would represent valuable information that could be disclosed by organisations in order to support their ethical standing.", "venue": "Ethics and Information Technology", "year": 2009, "referenceCount": 37, "citationCount": 247, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1840916", "name": "M. Turilli"}, {"authorId": "1982425", "name": "L. Floridi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "719dbcb7c24c471c8093b66d976d1973d731af06", "externalIds": {"MAG": "1994647306", "DBLP": "journals/jsis/NewellM15", "DOI": "10.1016/j.jsis.2015.02.001"}, "url": "https://www.semanticscholar.org/paper/719dbcb7c24c471c8093b66d976d1973d731af06", "title": "Strategic opportunities (and challenges) of algorithmic decision-making: A call for action on the long-term societal effects of 'datification'", "abstract": "Today, digital data are captured through a variety of devices that have the ability to monitor the minutiae of an individual's everyday life. These data are often processed by algorithms, which support (or drive) decisions (termed 'algorithmic decision-making' in this article). While the strategic value of these data (and subsequent analysis) for businesses is unquestionable, the implications for individuals and wider society are less clear. Therefore, in this Viewpoint article we aim to shed light on the tension between businesses - that increasingly profile customers and personalize products and services - and individuals, who, as McAfee and Brynjolfsson (2012, p. 5) suggest, are 'walking data generators' but are often unaware of how the data they produce are being used, and by whom and with what consequences. Issues associated with privacy, control and dependence arise, suggesting that social and ethical concerns related to the way business is strategically exploiting digitized technologies that increasingly support our everyday activities should be brought to the fore and thoughtfully discussed. In this article we aim to lay a foundation for this discussion in the IS community and beyond.", "venue": "J. Strateg. Inf. Syst.", "year": 2015, "referenceCount": 75, "citationCount": 230, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144513918", "name": "S. Newell"}, {"authorId": "2455920", "name": "Marco Marabelli"}]}}, {"contexts": ["\u2026data (e.g. Lomborg and Bechmann, 2014: 256); tracking of finegrained behaviours and preferences (e.g. sexual orientation or political opinions; (Mahajan et al., 2012); and prediction of future behaviour (as used in predictive policing or credit, insurance and employment screening; Zarsky,\u2026"], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "7479c12f42a9831363beca0afaf8d6a20383ddc0", "externalIds": {"MAG": "1991156765", "DOI": "10.1115/IMECE2012-89339"}, "url": "https://www.semanticscholar.org/paper/7479c12f42a9831363beca0afaf8d6a20383ddc0", "title": "Cultivating Emerging and Black Swan Technologies", "abstract": "Emerging technologies, defined as contemporary cutting-edge developments in various fields of technology, are generally associated with the potential for large impact on society. In a recent op-ed, \u201cThe coming Tech-led Boom\u201d (Wall Street Journal, January 30, 2012), Mills and Ottino list three grand technological transformations \u2014 big data, smart manufacturing, and the wireless revolution \u2014 poised to transform this century as much as telephony and electricity did in the 20th century. This list is by no means comprehensive and most likely misses technologies that are not yet recognized, but may still carry an extreme impact \u2014 i.e., the so-called Black Swans, as defined by New York Times best-selling author, Nassim Nicholas Taleb, in his book, The Black Swan. Taleb cites the example of three recently implemented technologies that most impact our world today \u2014 the Internet, the computer, and the laser \u2014 and notes that all three were unplanned, unpredicted, and unappreciated upon their discovery, and remained unappreciated well after initial use.In this paper, we will examine several emerging technologies, present a methodology to create a breeding ground for potential Black Swans, and finally discuss the societal and ethical aspects of these technologies.Copyright \u00a9 2012 by ASME", "venue": "", "year": 2012, "referenceCount": 16, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "2520356", "name": "R. Mahajan"}, {"authorId": "49662409", "name": "Rolf Mueller"}, {"authorId": "144209180", "name": "Christopher B. Williams"}, {"authorId": "123857258", "name": "J. Reed"}, {"authorId": "49347859", "name": "T. Campbell"}, {"authorId": "1755938", "name": "Naren Ramakrishnan"}]}}, {"contexts": ["However, complex decisionmaking structures can quickly exceed the human and organisational resources available for oversight (Kitchin, 2016).", "Proprietary algorithms are kept secret for the sake of competitive advantage (Glenn and Monteith, 2014; Kitchin, 2016; Stark and Fins, 2013), national security (Leese, 2014), or privacy.", "\u2026auditing carried out by data processors (Zarsky, 2016), external regulators (Pasquale, 2015; Tutt, 2016; Zarsky, 2016), or empirical researchers (Kitchin, 2016; Neyland, 2016), using ex post audit studies (Adler et al., 2016; Diakopoulos, 2015; Kitchin, 2016; Romei and Ruggieri, 2014; Sandvig\u2026", "Beyond machine learning, algorithms with \u2018handwritten\u2019 decision-making rules can still be highly complex and practically inscrutable from a lay data subject\u2019s perspective (Kitchin, 2016).", "\u20262016; Zarsky, 2016), or empirical researchers (Kitchin, 2016; Neyland, 2016), using ex post audit studies (Adler et al., 2016; Diakopoulos, 2015; Kitchin, 2016; Romei and Ruggieri, 2014; Sandvig et al., 2014), reflexive ethnographic studies in development and testing (Neyland, 2016), or\u2026", "Many scholarly critiques also fail to specify technical categories or a formal definition of \u2018algorithm\u2019 (Burrell, 2016; Kitchin, 2016)."], "isInfluential": true, "intents": ["methodology", "background"], "citedPaper": {"paperId": "787c6c27675a74a45baa8d88b2a19ed377885859", "externalIds": {"MAG": "1515490832", "DOI": "10.1080/1369118X.2016.1154087"}, "url": "https://www.semanticscholar.org/paper/787c6c27675a74a45baa8d88b2a19ed377885859", "title": "Thinking critically about and researching algorithms", "abstract": "ABSTRACT More and more aspects of our everyday lives are being mediated, augmented, produced and regulated by software-enabled technologies. Software is fundamentally composed of algorithms: sets of defined steps structured to process instructions/data to produce an output. This paper synthesises and extends emerging critical thinking about algorithms and considers how best to research them in practice. Four main arguments are developed. First, there is a pressing need to focus critical and empirical attention on algorithms and the work that they do given their increasing importance in shaping social and economic life. Second, algorithms can be conceived in a number of ways \u2013 technically, computationally, mathematically, politically, culturally, economically, contextually, materially, philosophically, ethically \u2013 but are best understood as being contingent, ontogenetic and performative in nature, and embedded in wider socio-technical assemblages. Third, there are three main challenges that hinder research about algorithms (gaining access to their formulation; they are heterogeneous and embedded in wider systems; their work unfolds contextually and contingently), which require practical and epistemological attention. Fourth, the constitution and work of algorithms can be empirically studied in a number of ways, each of which has strengths and weaknesses that need to be systematically evaluated. Six methodological approaches designed to produce insights into the nature and work of algorithms are critically appraised. It is contended that these methods are best used in combination in order to help overcome epistemological and practical challenges.", "venue": "", "year": 2017, "referenceCount": 82, "citationCount": 434, "influentialCitationCount": 25, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "145878647", "name": "R. Kitchin"}]}}, {"contexts": ["Discussion of a concept as complex as \u2018algorithm\u2019 inevitably encounters problems of abstraction or \u2018talking past each other\u2019 due to a failure to specify a level of abstraction (LoA) for discussion, and thus limit the relevant set of observables (Floridi, 2008)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "79453d53700a6bca7160ecee9560d758f794500b", "externalIds": {"DBLP": "journals/mima/Floridi08", "MAG": "2079455354", "DOI": "10.1007/s11023-008-9113-7"}, "url": "https://www.semanticscholar.org/paper/79453d53700a6bca7160ecee9560d758f794500b", "title": "The Method of Levels of Abstraction", "abstract": "The use of \u201clevels of abstraction\u201d in philosophical analysis (levelism) has recently come under attack. In this paper, I argue that a refined version of epistemological levelism should be retained as a fundamental method, called the method of levels of abstraction. After a brief introduction, in section \u201cSome Definitions and Preliminary Examples\u201d the nature and applicability of the epistemological method of levels of abstraction is clarified. In section \u201cA Classic Application of the Method of Abstraction\u201d, the philosophical fruitfulness of the new method is shown by using Kant\u2019s classic discussion of the \u201cantinomies of pure reason\u201d as an example. In section \u201cThe Philosophy of the Method of Abstraction\u201d, the method is further specified and supported by distinguishing it from three other forms of \u201clevelism\u201d: (i) levels of organisation; (ii) levels of explanation and (iii) conceptual schemes. In that context, the problems of relativism and antirealism are also briefly addressed. The conclusion discusses some of the work that lies ahead, two potential limitations of the method and some results that have already been obtained by applying the method to some long-standing philosophical problems.", "venue": "Minds and Machines", "year": 2008, "referenceCount": 87, "citationCount": 235, "influentialCitationCount": 22, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1982425", "name": "L. Floridi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "7c2ecfb2feb2b8bd61232567631fe5c0f336cc14", "externalIds": {"MAG": "2023571823", "DOI": "10.1007/s10676-012-9291-0"}, "url": "https://www.semanticscholar.org/paper/7c2ecfb2feb2b8bd61232567631fe5c0f336cc14", "title": "Unblinking eyes: the ethics of automating surveillance", "abstract": "In this paper I critique the ethical implications of automating CCTV surveillance. I consider three modes of CCTV with respect to automation: manual (or non-automated), fully automated, and partially automated. In each of these I examine concerns posed by processing capacity, prejudice towards and profiling of surveilled subjects, and false positives and false negatives. While it might seem as if fully automated surveillance is an improvement over the manual alternative in these areas, I demonstrate that this is not necessarily the case. In preference to the extremes I argue in favour of partial automation in which the system integrates a human CCTV operator with some level of automation. To assess the degree to which such a system should be automated I draw on the further issues of privacy and distance. Here I argue that the privacy of the surveilled subject can benefit from automation, while the distance between the surveilled subject and the CCTV operator introduced by automation can have both positive and negative effects. I conclude that in at least the majority of cases more automation is preferable to less within a partially automated system where this does not impinge on efficacy.", "venue": "Ethics and Information Technology", "year": 2012, "referenceCount": 69, "citationCount": 42, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "4863368", "name": "K. Macnish"}]}}, {"contexts": ["Similar effects can be observed in mixed networks of human and information systems as already studied in bureaucracies, characterised by reduced feelings of personal responsibility and the execution of otherwise unjustifiable actions (Arendt, 1971)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "86c48c095f0285a0a621c0b137a3231abdb0d9ce", "externalIds": {"MAG": "2323494234", "DOI": "10.2307/3319667"}, "url": "https://www.semanticscholar.org/paper/86c48c095f0285a0a621c0b137a3231abdb0d9ce", "title": "Eichmann in Jerusalem. A Report on the Banality of Evil", "abstract": "Hannah Arendt's portrayal of the terrible consequences of blind obedience, \"Eichmann in Jerusalem: A Report on the Banality of Evil\" contains an introduction by Amos Elon in \"Penguin Classics\". Sparking a flurry of heated debate, Hannah Arendt's authoritative and stunning report on the trial of German Nazi SS leader Adolf Eichmann first appeared as a series of articles in \"The New Yorker\" in 1963. This revised edition includes material that came to light after the trial, as well as Arendt's postscript commenting on the controversy that arose over her book. A major journalistic triumph by an intellectual of singular influence, \"Eichmann in Jerusalem\" is as shocking as it is informative - a meticulous and unflinching look at one of the most unsettling (and unsettled) issues of the twentieth century. Hannah Arendt (1906-1975) was for many years University Professor of Political Philosophy in the Graduate Faculty of the New School for Social Research and a Visiting Fellow of the Committee on Social Thought at the University of Chicago. She is also the author of \"Eichmann in Jerusalem\", \"On Revolution\", and \"Between Past and Future\". If you enjoyed \"Eichmann in Jerusalem\", you might like Elie Wiesel's \"Night\", available in \"Penguin Modern Classics\". \"Deals with the greatest problem of our time ...the problem of the human being within a modern totalitarian system\". (Bruno Bettelheim, \"The New Republic\"). \"A profound and documented analysis...Bound to stir our minds and trouble our consciences\". (\"Chicago Tribune\").", "venue": "", "year": 1964, "referenceCount": 1, "citationCount": 2453, "influentialCitationCount": 113, "isOpenAccess": true, "fieldsOfStudy": ["Philosophy", "Sociology"], "authors": [{"authorId": "2326038", "name": "H. Arendt"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "875b60080148c6e6a7f10304fa9bfe30192fc399", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/875b60080148c6e6a7f10304fa9bfe30192fc399", "title": "Toward ethical intelligent autonomous healthcare agents : a case-supported principle-based behavior paradigm", "abstract": "A paradigm of case-supported principle-based behavior is proposed to help ensure ethical behavior of intelligent autonomous machines. The requirements, methods, implementation, and tests of this paradigm are detailed.", "venue": "", "year": 2014, "referenceCount": 17, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "145012477", "name": "Michael Anderson"}, {"authorId": "2120952", "name": "S. Anderson"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "89ac6d85ce79adf538451069d384fe13d45a91bd", "externalIds": {"MAG": "2161280932", "DBLP": "journals/tis/RubelJ16", "DOI": "10.1080/01972243.2016.1130502"}, "url": "https://www.semanticscholar.org/paper/89ac6d85ce79adf538451069d384fe13d45a91bd", "title": "Student privacy in learning analytics: An information ethics perspective", "abstract": "ABSTRACT Higher education institutions have started using big data analytics tools. By gathering information about students as they navigate information systems, learning analytics employs techniques to understand student behaviors and to improve instructional, curricular, and support resources and learning environments. However, learning analytics presents important moral and policy issues surrounding student privacy. We argue that there are five crucial questions about student privacy that we must address in order to ensure that whatever the laudable goals and gains of learning analytics, they are commensurate with respecting students' privacy and associated rights, including (but not limited to) autonomy interests. We address information access concerns, the intrusive nature of information-gathering practices, whether or not learning analytics is justified given the potential distribution of consequences and benefits, and issues related to student autonomy. Finally, we question whether learning analytics advances the aims of higher education or runs counter to those goals.", "venue": "Inf. Soc.", "year": 2014, "referenceCount": 80, "citationCount": 101, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Psychology", "Computer Science"], "authors": [{"authorId": "2490144", "name": "Alan Rubel"}, {"authorId": "153739387", "name": "Kyle M. L. Jones"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "8d00c1941319645c544f07c8fac2dd8bfee2010e", "externalIds": {"MAG": "2070321941", "DOI": "10.1007/s10676-012-9302-1"}, "url": "https://www.semanticscholar.org/paper/8d00c1941319645c544f07c8fac2dd8bfee2010e", "title": "Intending to err: the ethical challenge of lethal, autonomous systems", "abstract": "Current precursors in the development of lethal, autonomous systems (LAS) point to the use of biometric devices for assessing, identifying, and verifying targets. The inclusion of biometric devices entails the use of a probabilistic matching program that requires the deliberate targeting of noncombatants as a statistically necessary function of the system. While the tactical employment of the LAS may be justified on the grounds that the deliberate killing of a smaller number of noncombatants is better than the accidental killing of a larger number, it may nonetheless contravene a reemerging conception of right intention. Originally framed by Augustine of Hippo, this lesser-known formulation has served as the foundation for chivalric code, canon law, jus in bello criteria, and the law of armed conflict. Thus it serves as a viable measure to determine whether the use of lethal autonomy would accord with these other laws and principles. Specifically, examinations of the LAS through the lenses of collateral damage, the principle of double effect, and the principle of proportionality, reveal the need for more attention to be paid to the moral issues now, so that the promise of this emerging technology\u2014that it will perform better than human beings\u2014might actually come to pass.", "venue": "Ethics and Information Technology", "year": 2012, "referenceCount": 26, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "5147981", "name": "Mark S. Swiatek"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "8d474bc459ab9af34e4d2bfdd462444f8328b7ef", "externalIds": {"MAG": "2259325736"}, "url": "https://www.semanticscholar.org/paper/8d474bc459ab9af34e4d2bfdd462444f8328b7ef", "title": "The Dilemma of Private Justice Systems: Big Data Sources, the Cloud and Predictive Analytics", "abstract": "In the age of big data, demanding customer expectations, and increasingly limited access to justice for small claims arising from online sales, business organizations are moving to enhanced online customer complaint platforms and insisting upon increased online justice resolution systems. At the same time, online businesses, even websites you fail to think of as a business, are moving from traditional analytics that provide a snapshot of the past, to solutions that provide an accurate picture of the present and a prediction of future trends. For many, predictive analytics is the wave of the future. In many ways, the use of predictive analytics is a wonderful occurrence, as our packages will arrive in a more timely manner, our advertising will be more personal and our online and physical lives will be tailored, monitored and adjusted to our interests, life styles and immediate needs without so much as a hiccup. However, what will happen when the current push for private online dispute resolution systems meets the current big data gathering of a private market? Will the private online dispute resolution providers use the information gathered for good, or as a means to quickly resolve disputes without notice of the law, personal rights and/or ethical outcomes? Worse yet, what will happen when the private market of online dispute resolution faces the demands of a business environment that would prefer analytic outcomes to be skewed to favor the business? Bear in mind, these issues do not arise in a prediction, these private online dispute resolution mechanisms already exist and are growing in support and use on a daily basis.This paper will explore the emerging issue that occurs when private online dispute resolution providers are allowed, without transparency, oversight, or regulation, to create a justice system that knows a lot of personal information about you but is required to follow no legal standard or regulation to resolve your dispute with a merchant.", "venue": "", "year": 2014, "referenceCount": 40, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "2049399608", "name": "Anjanette H. Raymond"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "8eec02312eabfc3e38c2f57f85f17a86c5d6cca7", "externalIds": {"MAG": "1964102704", "DOI": "10.1007/s10676-012-9290-1"}, "url": "https://www.semanticscholar.org/paper/8eec02312eabfc3e38c2f57f85f17a86c5d6cca7", "title": "Out of character: on the creation of virtuous machines", "abstract": "The emerging discipline of Machine Ethics is concerned with creating autonomous artificial moral agents that perform ethically significant actions out in the world. Recently, Wallach and Allen (Moral machines: teaching robots right from wrong, Oxford University Press, Oxford, 2009) and others have argued that a virtue-based moral framework is a promising tool for meeting this end. However, even if we could program autonomous machines to follow a virtue-based moral framework, there are certain pressing ethical issues that need to be taken into account, prior to the implementation and development stages. Here I examine whether the creation of virtuous autonomous machines is morally permitted by the central tenets of virtue ethics. It is argued that the creation of such machines violates certain tenets of virtue ethics, and hence that the creation and use of those machines is impermissible. One upshot of this is that, although virtue ethics may have a role to play in certain near-term Machine Ethics projects (e.g. designing systems that are sensitive to ethical considerations), machine ethicists need to look elsewhere for a moral framework to implement into their autonomous artificial moral agents, Wallach and Allen\u2019s claims notwithstanding.", "venue": "Ethics and Information Technology", "year": 2012, "referenceCount": 32, "citationCount": 20, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "47075655", "name": "Ryan Tonkens"}]}}, {"contexts": ["Machine learning is \u2018\u2018any methodology and set of techniques that can employ data to come up with novel patterns and knowledge, and generate models that can be used for effective predictions about the data\u2019\u2019 (Van Otterlo, 2013).", "Data subjects cannot define privacy norms to govern all types of data generically because their value or insightfulness is only established through processing (Hildebrandt, 2011; Van Wel and Royakkers, 2004).", "Van Wel and Royakkers (2004) urge a re-conceptualisation of personal data, where equivalent privacy protections are afforded to \u2018group characteristics\u2019 when used in place of \u2018individual characteristics\u2019 in generating knowledge about or taking actions towards an individual.", "Van Wel and Royakkers (2004: 133) argue that external identity construction by algorithms is a type of de-individualisation, or a \u2018\u2018tendency of judging and treating people on the basis of group characteristics instead of on their own individual characteristics and merit.\u2019\u2019", "Informational privacy concerns the capacity of an individual to control information about herself (Van Wel\nand Royakkers, 2004), and the effort required by third parties to obtain this information.", "Personalisation through non-distributive profiling, seen for example in personalised pricing in insurance premiums (Hildebrandt and Koops, 2010; Van Wel and Royakkers, 2004), can be discriminatory by violating both ethical and legal principles of equal or fair treatment of individuals (Newell and Marabelli, 2015).", "The model can be taught to the algorithm via hand labelled inputs (supervised learning); in other cases the algorithm itself defines best-fit models to make sense of a set of inputs (unsupervised learning)5 (Schermer, 2011; Van Otterlo, 2013).", "Algorithmic decision-making structures containing \u2018\u2018hundreds of rules are very hard to inspect visually, especially when their predictions are combined probabilistically in complex ways\u2019\u2019 (Van Otterlo, 2013).", "Knowledge about the assigned classes is used to make\npredictions about the case (Van Otterlo, 2013)."], "isInfluential": true, "intents": ["methodology", "background"], "citedPaper": {"paperId": "8f73fe7bdb97e064266a3433c154e07bba11c919", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/8f73fe7bdb97e064266a3433c154e07bba11c919", "title": "AMachine Learning View on Profiling", "abstract": "\"To the right and to the left as in mirrors, to the right and to the left through the glass walls I see others like myself, other rooms like my own, other clothes like my own, movements like mine, duplicated thousands of times. This invigorates me; I see myself as a part of an enormous, vigorous, united body; and what precise beauty!\" (Zamyatin, 1924, pp.31-32) Our current information age is shifting the concept of privacy day by day. Privacy may eventually be a futile notion, drowned in an ocean of big data (Bollier and Firestone, 2010). The main eroding forces on privacy include the amount of information that is available, the data hunger of public and private institutions, the (perceived) threat of terrorist attacks and the corresponding business opportunities of (security) companies, and the rise of smart algorithms (Nilsson, 2010) to automatically gather, store, analyze, and utilize enormous amounts of data 1. When it comes to big data and privacy, we can find examples in every newspaper on an almost daily basis. In the Netherlands, current debates and plans are about i) a central database for medical records, ii) a GPS-based tracking system that will be required in every car to make differentiated use and payment possible, iii) a chip card system which will record and track every individual making use of public transportation, and iv) smart energy meters that will monitor our demand for energy. These systems can deliver useful information to optimize various services but at the same time they disclose a lot of information concerning habits, preferences and whereabouts of individuals. Other concrete systems include v) surveillance cameras extended with microphones in Amsterdam, vi) automatic face recognition in the Rotterdam metro system, vii) body scanners at Schiphol airport, and viii) the addition of our biometric data (e.g. fingerprints) to our passports. All these systems are tacitly introduced into our societies and generate huge amounts of private information of individuals. When people talk about negative aspects of privacy they often employ terms as \"Big Brother\" and \"Orwellian\", to connect to the idea of an all-seeing eye overseeing every bit of information we (have to) reveal about ourselves. Sometimes we are aware of this modern electronic version of Bentham\u2019s panopticum and consciously think about the fact that we are being watched continuously, and sometimes act accordingly. In most occasions, however, privacy is seen as attached only to the information we reveal, or have to give up, about ourselves to governments, companies and other parties. Our concerns are often solely about what personal information, i.e. attached to a specific identity, is out there, who gets to see it, who owns it, and how that information is used to control, track, monitor", "venue": "", "year": 2011, "referenceCount": 64, "citationCount": 18, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2541098", "name": "M. V. Otterlo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "8f8824858b2419823b3fd9a3ab445fda0cf57b01", "externalIds": {"MAG": "2309013802", "DBLP": "conf/istas/ApplinF15", "DOI": "10.1109/ISTAS.2015.7439436"}, "url": "https://www.semanticscholar.org/paper/8f8824858b2419823b3fd9a3ab445fda0cf57b01", "title": "New technologies and mixed-use convergence: How humans and algorithms are adapting to each other", "abstract": "Human experience with technology has shifted from technological contexts requiring occasional intervention by a fraction of people mostly in command of technologies, to technological contexts that require constant ongoing participation from most people to complete tasks. We examine the current state of `mixed-use' new technologies integration with legacy systems, and whether the human assistance required to complete tasks and processes could function as a training ground for future smart systems, or whether increasing `co-dependence with' or `training of' algorithmic systems, enhancing task completion and inadvertently educating systems in human behaviour and intelligence, will simply subsume people into the algorithmic landscape. As the Internet of Things (IoT) arises in conjunction with advancing robotics and drone technology, semi and fully automated algorithmic systems are being developed that intersect with human experience in new and heterogeneous ways. Many new technologies are not yet flexible enough to support the choices people require in their daily lives, due to limitations in the algorithmic `logics' used that restrict options to predetermined pathways conceived of by programmers. This greatly limits human agency, and presently the potential to overcome problems that arise in processes. In this mixed-use period, we have the opportunity to develop new ways to address ethical guidance as knowledge that machines can learn. We explore promoting embedding of ethically-based principles into automated contexts through: (1) developing mutually agreed automated external ethical review systems (human or otherwise) that evaluate conformance across multiple ethical codes and provide feedback to designers, agents, and users on the distribution of conformance; (2) focusing on review systems to drive distributed development of embedded ethical principles in individual services by responding to this feedback to develop ongoing correction through dynamic adaption or incremental releases; and (3) using multi-agent simulation tools to forecast scenarios in real time.", "venue": "2015 IEEE International Symposium on Technology and Society (ISTAS)", "year": 2015, "referenceCount": 30, "citationCount": 19, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1963689", "name": "Sally A. Applin"}, {"authorId": "152586454", "name": "Michael D. Fischer"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "9019fde5d617a1981399200501e8c67136cd1a3a", "externalIds": {"MAG": "2320046939", "DOI": "10.1177/1469540513480159"}, "url": "https://www.semanticscholar.org/paper/9019fde5d617a1981399200501e8c67136cd1a3a", "title": "Consumption as biopower: Governing bodies with loyalty cards", "abstract": "For more than a decade, many retail companies have been collecting large volumes of data on a daily basis through loyalty card programmes. These programmes gather, at point-of-purchase, the identity of the consumer, date and time of the transaction, and the list of products purchased. With the help of data mining techniques, companies can use this data to get a better knowledge of their customer and to address them personally with targeted advertisement. This \u201cmass customization\u201d, which is at the core of the relationship marketing paradigm, has traditionally been viewed as a means of customizing services to meet the needs of an existing market. However, it appears also to be invested in actually customizing consumers to meet market needs. To investigate this aspect of relationship marketing, a study was conducted to examine the extent to which companies in Switzerland use data-mining technologies and strategies, their data collection and analysis practices, the privacy risks posed by such practices, and the modalities of power they create. As a result, and as it will be developed in this article, I finally theorized surveillance of consumption as being a much elaborated form of biopower, which strongly relies on the use of data mining to reveal patterns in consumption. This biopower is actually growing as data collected through loyalty programmes is now becoming a prime target for other purposes than pure marketing, such as helping the fight of health policies against obesity, or to control the consumer\u2019s intake of food additive. These new kind of practices bring major ethical issues that are also discussed in this article.", "venue": "", "year": 2013, "referenceCount": 73, "citationCount": 30, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "2075209768", "name": "S. Coll"}]}}, {"contexts": ["This is because algorithms can affect how we conceptualise the world, and modify its social and political organisation (cf. Floridi, 2014)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "905a15364af895957f5c9f82781a366541392502", "externalIds": {"MAG": "582315782", "DOI": "10.5860/choice.187284"}, "url": "https://www.semanticscholar.org/paper/905a15364af895957f5c9f82781a366541392502", "title": "The Fourth Revolution: How the infosphere is reshaping human reality", "abstract": "Preface Acknowledgements List of figures 1. Hyperhistory 2. Space: Infosphere 3. Identity: Onlife 4. Self-Understanding: The Four Revolutions 5. Privacy: Informational Friction 6. Intelligence: Inscribing the World 7. Agency: Enveloping the World 8. Politics: The Rise of the Multi-Agent System 9. Environment: The Digital Gambit 10. Ethics: E-nvironmentalism Further Reading References Endnotes Index", "venue": "", "year": 2014, "referenceCount": 0, "citationCount": 464, "influentialCitationCount": 35, "isOpenAccess": true, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "1982425", "name": "L. Floridi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "91e36e1c6e3d4a5bdc97fa5ab5b89bdf9113413d", "externalIds": {"MAG": "2189632871", "DOI": "10.1177/2053951715622512"}, "url": "https://www.semanticscholar.org/paper/91e36e1c6e3d4a5bdc97fa5ab5b89bdf9113413d", "title": "How the machine \u2018thinks\u2019: Understanding opacity in machine learning algorithms", "abstract": "This article considers the issue of opacity as a problem for socially consequential mechanisms of classification and ranking, such as spam filters, credit card fraud detection, search engines, news trends, market segmentation and advertising, insurance or loan qualification, and credit scoring. These mechanisms of classification all frequently rely on computational algorithms, and in many cases on machine learning algorithms to do this work. In this article, I draw a distinction between three forms of opacity: (1) opacity as intentional corporate or state secrecy, (2) opacity as technical illiteracy, and (3) an opacity that arises from the characteristics of machine learning algorithms and the scale required to apply them usefully. The analysis in this article gets inside the algorithms themselves. I cite existing literatures in computer science, known industry practices (as they are publicly presented), and do some testing and manipulation of code as a form of lightweight code audit. I argue that recognizing the distinct forms of opacity that may be coming into play in a given application is a key to determining which of a variety of technical and non-technical solutions could help to prevent harm.", "venue": "", "year": 2016, "referenceCount": 39, "citationCount": 786, "influentialCitationCount": 38, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "48129731", "name": "J. Burrell"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "948dee1941cef465b14a95c613051477a48d2590", "externalIds": {"MAG": "2676401", "DOI": "10.2139/SSRN.2154654"}, "url": "https://www.semanticscholar.org/paper/948dee1941cef465b14a95c613051477a48d2590", "title": "Technology and Pragmatism: From Value Neutrality to Value Criticality", "abstract": null, "venue": "", "year": 2006, "referenceCount": 0, "citationCount": 9, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Philosophy"], "authors": [{"authorId": "2115353555", "name": "J. Johnson"}]}}, {"contexts": ["Algorithms are inescapably value-laden (Brey and Soraker, 2009; Wiener, 1988)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "9573e4e10c786de69fac32b80b03f164a9c27f44", "externalIds": {"MAG": "95528974", "DOI": "10.1016/B978-0-444-51667-1.50051-3"}, "url": "https://www.semanticscholar.org/paper/9573e4e10c786de69fac32b80b03f164a9c27f44", "title": "Philosophy of Computing and Information Technology", "abstract": "Philosophy has been described as having taken a \u201ccomputational turn,\u201d referring to the ways in which computers and information technology throw new light upon traditional philosophical issues, provide new tools and concepts for philosophical reasoning, and pose theoretical and practical questions that cannot readily be approached within traditional philosophical frameworks. As such, computer technology is arguably the technology that has had the most profound impact on philosophy. Philosophers have studied computer technology and its philosophical implications extensively. Philosophers have discovered computers and information technology (IT) as research topics, and a wealth of research is taking place on philosophical issues in relation to these technologies. The research agenda is broad and diverse. Issues that are studied include the nature of computational systems, the ontological status of virtual worlds, the limitations of artificial intelligence, philosophical aspects of data modeling, the political regulation of cyberspace, the epistemology of Internet information, ethical aspects of information privacy and security, and many more.\n\nPhilosophy has been described as having taken a \u2018computational turn\u2019, referring to the ways in which computers and information technology throw new light upon traditional philosophical issues, provide new tools and concepts for philosophical reasoning, and pose theoretical and practical questions that cannot readily be approached within traditional philosophical frameworks. As such, computer technology is arguably the technology that has had the most profound impact on philosophy. Philosophers have studied computer technology and its philosophical implications extensively, and this chapter gives an overview of the field. We start with definitions and historical overviews of the field and its various subfields. We then consider studies of the fundamental nature and basic principles of computing and computational systems, before moving on to philosophy of computer science, which investigates the nature, scope and methods of computer science. Under this heading, we will also address such topics as data modeling, ontology in computer science, programming languages, software engineering as an engineering discipline, management of information systems, the use of computers for simulation, and human-computer interaction. Subsequently, we will address the issue in computing that has received the most attention from philosophers, artificial intelligence (AI). The purpose of this section is to give an overview of the philosophical issues raised by the notion of creating intelligent machines. We consider philosophical critiques of different approaches within AI and pay special attention to philosophical studies of applications of AI. We then turn to a section on philosophical issues pertaining to new media and the Internet, including the convergence between media and digital computers. The theoretical and ethical issues raised by this relatively recent phenomenon are diverse. We will focus on philosophical theories of the \u2018information society\u2019, epistemological and ontological issues in relation to Internet information and virtuality, the philosophical study of social life online and cyberpolitics, and issues raised by the disappearing borders between body and artifact in cyborgs and virtual selves. The final section in this chapter is devoted to the many ethical questions raised by computers and information technology, as studied in computer ethics.", "venue": "", "year": 2009, "referenceCount": 335, "citationCount": 46, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "39979166", "name": "P. Brey"}, {"authorId": "152181934", "name": "J. S\u00f8raker"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "96374cc11d072f320095a665a6b42af5e5208402", "externalIds": {"MAG": "1980534953", "DOI": "10.1017/CBO9780511817557"}, "url": "https://www.semanticscholar.org/paper/96374cc11d072f320095a665a6b42af5e5208402", "title": "The Emergence of Probability. A Philosophical Study of Early Ideas about Probability, Induction and Statistical Inference", "abstract": "1. An absent family of ideas 2. Duality 3. Opinion 4. Evidence 5. Signs 6. The first calculations 7. The Roannez circle (1654) 8. The great decision (1658?) 9. The art of thinking (1662) 10. Probability and the law (1665) 11. Expectation (1657) 12. Political arithmetic (1662) 13. Annuities (1671) 14. Equipossibility (1678) 15. Inductive logic 16. The art of conjecturing (1692[?] published 1713) 17. The first limit theorem 18. Design 19. Induction (1737) Bibliography Index.", "venue": "", "year": 1979, "referenceCount": 0, "citationCount": 497, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Philosophy"], "authors": [{"authorId": "2971501", "name": "I. Hacking"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "96ef799148b63e2f4399cb454b1dd1b0c6836c3d", "externalIds": {"PubMedCentral": "1182327", "MAG": "2243629635", "DOI": "10.1371/journal.pmed.0020124", "PubMed": "16060722"}, "url": "https://www.semanticscholar.org/paper/96ef799148b63e2f4399cb454b1dd1b0c6836c3d", "title": "Why Most Published Research Findings Are False", "abstract": "Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.", "venue": "PLoS medicine", "year": 2005, "referenceCount": 82, "citationCount": 4401, "influentialCitationCount": 159, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Psychology"], "authors": [{"authorId": "145441750", "name": "J. Ioannidis"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "97416df836d7210f2dc8b0d91ce1851be354de54", "externalIds": {"MAG": "2613463286", "DOI": "10.1007/978-3-319-54024-5_4"}, "url": "https://www.semanticscholar.org/paper/97416df836d7210f2dc8b0d91ce1851be354de54", "title": "Algorithmic Transparency via Quantitative Input Influence", "abstract": "Algorithmic systems that employ machine learning are often opaque\u2014it is difficult to explain why a certain decision was made. We present a formal foundation to improve the transparency of such decision-making systems. Specifically, we introduce a family of Quantitative Input Influence (QII) measures that capture the degree of input influence on system outputs. These measures provide a foundation for the design of transparency reports that accompany system decisions (e.g., explaining a specific credit decision) and for testing tools useful for internal and external oversight (e.g., to detect algorithmic discrimination). Distinctively, our causal QII measures carefully account for correlated inputs while measuring influence. They support a general class of transparency queries and can, in particular, explain decisions about individuals and groups. Finally, since single inputs may not always have high influence, the QII measures also quantify the joint influence of a set of inputs (e.g., age and income) on outcomes (e.g. loan decisions) and the average marginal influence of individual inputs within such a set (e.g., income) using principled aggregation measures, such as the Shapley value, previously applied to measure influence in voting.", "venue": "", "year": 2017, "referenceCount": 41, "citationCount": 53, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "33374965", "name": "Anupam Datta"}, {"authorId": "1774073", "name": "S. Sen"}, {"authorId": "34803282", "name": "Yair Zick"}]}}, {"contexts": ["However, proxies for protected attributes are not easy to predict or detect (Romei and Ruggieri, 2014; Zarsky, 2016), particularly when algorithms access linked datasets (Barocas and Selbst, 2015).", "Second, substantial trust is already placed in algorithms, in some cases affecting a de-responsibilisation of human actors, or a tendency to \u2018hide behind the computer\u2019 and assume automated processes are correct by default (Zarsky, 2016: 121).", "Further, as described above the capacity of individuals to investigate the personal relevance of factors used in decision-making is inhibited by opacity and automation (Zarsky, 2016).", "\u2026data (e.g. Lomborg and Bechmann, 2014: 256); tracking of finegrained behaviours and preferences (e.g. sexual orientation or political opinions; (Mahajan et al., 2012); and prediction of future behaviour (as used in predictive policing or credit, insurance and employment screening; Zarsky, 2016).", "A lack of public engagement with existing transparency mechanisms reflects this uncertainty, seen for example in credit scoring (Zarsky, 2016).", "Machine learning algorithms are particularly challenging in this respect (Burrell, 2016; Matthias, 2004; Zarsky, 2016), seen for instance in genetic algorithms that program themselves.", "Despite this, correlations based on a sufficient volume of data are increasingly seen as sufficiently credible to direct action without first establishing causality (Hildebrandt, 2011; Hildebrandt and Koops, 2010; Mayer-Scho\u0308nberger and Cukier, 2013; Zarsky, 2016).", "\u2026tread a fine line between supporting and controlling decisions by filtering which information is presented to the user based upon indepth understanding of preferences, behaviours, and perhaps vulnerabilities to influence (Bozdag, 2013; Goldman, 2006; Newell and Marabelli, 2015; Zarsky, 2016).", "\u2026path to explainability is algorithmic auditing carried out by data processors (Zarsky, 2016), external regulators (Pasquale, 2015; Tutt, 2016; Zarsky, 2016), or empirical researchers (Kitchin, 2016; Neyland, 2016), using ex post audit studies (Adler et al., 2016; Diakopoulos, 2015; Kitchin,\u2026", "One possible path to explainability is algorithmic auditing carried out by data processors (Zarsky, 2016), external regulators (Pasquale, 2015; Tutt, 2016; Zarsky, 2016), or empirical researchers (Kitchin, 2016; Neyland, 2016), using ex post audit studies (Adler et al., 2016; Diakopoulos, 2015;\u2026", "Alternatively, it may be necessary to limit automation or particular analytic methods in particular contexts to meet transparency requirements specified in the GDPR (Tutt, 2016; Zarsky, 2016).", "Disclosing the structure of these algorithms would facilitate ill-intentioned manipulations of search results (or \u2018gaming the system\u2019), while not bringing any advantage to the average non-techsavvy user (Granka, 2010; Zarsky, 2016)."], "isInfluential": true, "intents": ["methodology", "background"], "citedPaper": {"paperId": "9b4dbc901010a790d88c8be2370f8c9557895956", "externalIds": {"MAG": "2340696859", "DOI": "10.1177/0162243915605575"}, "url": "https://www.semanticscholar.org/paper/9b4dbc901010a790d88c8be2370f8c9557895956", "title": "The Trouble with Algorithmic Decisions", "abstract": "We are currently witnessing a sharp rise in the use of algorithmic decision-making tools. In these instances, a new wave of policy concerns is set forth. This article strives to map out these issues, separating the wheat from the chaff. It aims to provide policy makers and scholars with a comprehensive framework for approaching these thorny issues in their various capacities. To achieve this objective, this article focuses its attention on a general analytical framework, which will be applied to a specific subset of the overall discussion. The analytical framework will reduce the discussion to two dimensions, every one of which addressing two central elements. These four factors call for a distinct discussion, which is at times absent in the existing literature. The two dimensions are (1) the specific and novel problems the process assumedly generates and (2) the specific attributes which exacerbate them. While the problems are articulated in a variety of ways, they most likely could be reduced to two broad categories: efficiency and fairness-based concerns. In the context of this discussion, such problems are usually linked to two salient attributes the algorithmic processes feature\u2014its opaque and automated nature.", "venue": "", "year": 2016, "referenceCount": 22, "citationCount": 146, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "1938823", "name": "Tal Z. Zarsky"}]}}, {"contexts": ["The latter concerns a lack of reproducibility in analytics that distinguishes it in practice from science (cf. Feynman, 1974; Ioannidis, 2005; Vasilevsky et al., 2013)\nand is better understood as an issue of epistemology.", "The search for causal links is difficult, as correlations established in large, proprietary datasets are frequently not reproducible or falsifiable (cf. Ioannidis, 2005; Lazer et al., 2014)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "a0cbd181e7c85b985d893265515ce4a3484bfae5", "externalIds": {"DOI": "10.1080/09332480.2019.1579573"}, "url": "https://www.semanticscholar.org/paper/a0cbd181e7c85b985d893265515ce4a3484bfae5", "title": "Why Most Published Research Findings Are False", "abstract": "There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; effect sizes are smaller; there is a greater number and lesser preselection of tested relationships; there is greater flexibility in designs, definitions, outcomes, and analytical modes; there is greater financial and other interest and prejudice; and more teams are involved in a scientific field in cases of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings often may be simply accurate measures of the prevailing bias. Published research findings are refuted sometimes by subsequent evidence, with ensuing confusion and disappointment. Refutation and controversy is seen across the range of research designs, from clinical trials and traditional epidemiological studies to the most modern molecular research. There is increasing concern that in modern research, false findings may be the majority, or even the vast majority, of published research claims. However, this should not be surprising. It can be proven that most claimed research findings are false.", "venue": "CHANCE", "year": 2019, "referenceCount": 47, "citationCount": 1071, "influentialCitationCount": 18, "isOpenAccess": true, "fieldsOfStudy": null, "authors": [{"authorId": "145441750", "name": "J. Ioannidis"}]}}, {"contexts": ["\u2026the nature of moral agency in machines, work in machine ethics also investigates how best to design moral reasoning and behaviours into autonomous algorithms as artificial moral and ethical agents18 (Anderson and Anderson, 2007; Crnkovic and C\u0327u\u0308ru\u0308klu\u0308, 2011; Sullins, 2006; Wiegel and Berg, 2009).", "Anderson and Anderson (2007) suggest algorithms can be designed to mimic human ethical decision-making modelled on empirical research on how intuitions, principles and reasoning interact."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "a2f8c2da5abf99e517c65efc55fd10a28cef7770", "externalIds": {"MAG": "2133460105", "DBLP": "journals/aim/AndersonA07", "DOI": "10.1609/aimag.v28i4.2065"}, "url": "https://www.semanticscholar.org/paper/a2f8c2da5abf99e517c65efc55fd10a28cef7770", "title": "Machine Ethics: Creating an Ethical Intelligent Agent", "abstract": "The newly emerging field of machine ethics (Anderson and Anderson 2006) is concerned with adding an ethical dimension to machines. Unlike computer ethics -- which has traditionally focused on ethical issues surrounding humans' use of machines -- machine ethics is concerned with ensuring that the behavior of machines toward human users, and perhaps other machines as well, is ethically acceptable. In this article we discuss the importance of machine ethics, the need for machines that represent ethical principles explicitly, and the challenges facing those working on machine ethics. We also give an example of current research in the field that shows that it is possible, at least in a limited domain, for a machine to abstract an ethical principle from examples of correct ethical judgments and use that principle to guide its own behavior.", "venue": "AI Mag.", "year": 2007, "referenceCount": 43, "citationCount": 278, "influentialCitationCount": 24, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145012477", "name": "Michael Anderson"}, {"authorId": "2120952", "name": "S. Anderson"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "a45c7f4fa3804722f444b846ae6ab89d35af7331", "externalIds": {"MAG": "1966348702", "DOI": "10.1007/S11245-012-9129-8"}, "url": "https://www.semanticscholar.org/paper/a45c7f4fa3804722f444b846ae6ab89d35af7331", "title": "On How to Build a Moral Machine", "abstract": "Herein we make a plea to machine ethicists for the inclusion of constraints on their theories consistent with empirical data on human moral cognition. As philosophers, we clearly lack widely accepted solutions to issues regarding the existence of free will, the nature of persons and firm conditions on moral agency/patienthood; all of which are indispensable concepts to be deployed by any machine able to make moral judgments. No agreement seems forthcoming on these matters, and we don\u2019t hold out hope for machines that can both always do the right thing (on some general ethic) and produce explanations for its behavior that would be understandable to a human confederate. Our tentative solution involves understanding the folk concepts associated with our moral intuitions regarding these matters, and how they might be dependent upon the nature of human cognitive architecture. It is in this spirit that we begin to explore the complexities inherent in human moral judgment via computational theories of the human cognitive architecture, rather than under the extreme constraints imposed by rational-actor models assumed throughout much of the literature on philosophical ethics. After discussing the various advantages and challenges of taking this particular perspective on the development of artificial moral agents, we computationally explore a case study of human intuitions about the self and causal responsibility. We hypothesize that a significant portion of the variance in reported intuitions for this case might be explained by appeal to an interplay between the human ability to mindread and to the way that knowledge is organized conceptually in the cognitive system. In the present paper, we build on a pre-existing computational model of mindreading (Bello et\u00a0al. 2007) by adding constraints related to psychological distance (Trope and Liberman 2010), a well-established psychological theory of conceptual organization. Our initial results suggest that studies of folk concepts involved in moral intuitions lead us to an enriched understanding of cognitive architecture and a more systematic method for interpreting the data generated by such studies.", "venue": "", "year": 2013, "referenceCount": 31, "citationCount": 29, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "144093093", "name": "Paul Bello"}, {"authorId": "1797985", "name": "S. Bringsjord"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "abb054df8076453474879fe9d25830af5715ad1b", "externalIds": {"MAG": "2037451280", "DOI": "10.1007/s10676-013-9321-6"}, "url": "https://www.semanticscholar.org/paper/abb054df8076453474879fe9d25830af5715ad1b", "title": "Bias in algorithmic filtering and personalization", "abstract": "Online information intermediaries such as Facebook and Google are slowly replacing traditional media channels thereby partly becoming the gatekeepers of our society. To deal with the growing amount of information on the social web and the burden it brings on the average user, these gatekeepers recently started to introduce personalization features, algorithms that filter information per individual. In this paper we show that these online services that filter information are not merely algorithms. Humans not only affect the design of the algorithms, but they also can manually influence the filtering process even when the algorithm is operational. We further analyze filtering processes in detail, show how personalization connects to other filtering techniques, and show that both human and technical biases are present in today\u2019s emergent gatekeepers. We use the existing literature on gatekeeping and search engine bias and provide a model of algorithmic gatekeeping.", "venue": "Ethics and Information Technology", "year": 2013, "referenceCount": 160, "citationCount": 334, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1779305", "name": "E. Bozdag"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "abbb235fcf3b163afd74e1967f7d3784252b44fa", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/abbb235fcf3b163afd74e1967f7d3784252b44fa", "title": "MINING AND THE DISCOURSE ON DISCRIMINATION", "abstract": "This paper surveys and brings some order to the broad set of charges that commentators have begun to levy against data mining, all expressed in the language of discrimination. It maps the myriad kinds of discrimination ascribed to data mining, clarifies the precise mechanisms the commentators see as giving rise to these objectionable forms of discrimination, and specifies the principles or policies that such discrimination seems to", "venue": "", "year": 2014, "referenceCount": 18, "citationCount": 38, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2881033", "name": "Solon Barocas"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "ac70bb2458f01a9e47fc1afe0dd478fb2feb8f50", "externalIds": {"DBLP": "conf/uai/OrseauA16", "MAG": "2574075983"}, "url": "https://www.semanticscholar.org/paper/ac70bb2458f01a9e47fc1afe0dd478fb2feb8f50", "title": "Safely Interruptible Agents", "abstract": "Reinforcement learning agents interacting with a complex environment like the real world are unlikely to behave optimally all the time. If such an agent is operating in real-time under human supervision, now and then it may be necessary for a human operator to press the big red button to prevent the agent from continuing a harmful sequence of actions\u2014harmful either for the agent or for the environment\u2014and lead the agent into a safer situation. However, if the learning agent expects to receive rewards from this sequence, it may learn in the long run to avoid such interruptions, for example by disabling the red button\u2014 which is an undesirable outcome. This paper explores a way to make sure a learning agent will not learn to prevent (or seek!) being interrupted by the environment or a human operator. We provide a formal definition of safe interruptibility and exploit the off-policy learning property to prove that either some agents are already safely interruptible, like Q-learning, or can easily be made so, like Sarsa. We show that even ideal, uncomputable reinforcement learning agents for (deterministic) general computable environments can be made safely interruptible.", "venue": "UAI", "year": 2016, "referenceCount": 33, "citationCount": 87, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1749270", "name": "Laurent Orseau"}, {"authorId": "2054678912", "name": "S. Armstrong"}]}}, {"contexts": ["Various metrics of fairness are possible based on statistical parity, differential privacy and\nother relations between data subjects in classification tasks (Dwork et al., 2011; Romei and Ruggieri, 2014).", "These strategies are seen in the development of privacypreserving, fairness- and discrimination-aware data mining (Dwork et al., 2011; Kamishima et al., 2012)."], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId": "adaa0523a5c9d5f92aa2009a51226391d8e62380", "externalIds": {"ArXiv": "1104.3913", "DBLP": "conf/innovations/DworkHPRZ12", "MAG": "2949135123", "DOI": "10.1145/2090236.2090255"}, "url": "https://www.semanticscholar.org/paper/adaa0523a5c9d5f92aa2009a51226391d8e62380", "title": "Fairness through awareness", "abstract": "We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of \"fair affirmative action,\" which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.", "venue": "ITCS '12", "year": 2011, "referenceCount": 42, "citationCount": 1586, "influentialCitationCount": 186, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1781565", "name": "C. Dwork"}, {"authorId": "1775622", "name": "Moritz Hardt"}, {"authorId": "1695317", "name": "T. Pitassi"}, {"authorId": "1746057", "name": "O. Reingold"}, {"authorId": "1804104", "name": "R. Zemel"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "ae79976928acbc477c49d6502fa008d815de2515", "externalIds": {"MAG": "2734695534"}, "url": "https://www.semanticscholar.org/paper/ae79976928acbc477c49d6502fa008d815de2515", "title": "Big Data for All: Privacy and User Control in the Age of Analytics", "abstract": "We live in an age of \u201cbig data.\u201d Data have become the raw material of production, a new source for immense economic and social value. Advances in data mining and analytics and the massive increase in computing power and data storage capacity have expanded by orders of magnitude the scope of information available for businesses and government. Data are now available for analysis in raw form, escaping the confines of structured databases and enhancing researchers\u2019 abilities to identify correlations and conceive of new, unanticipated uses for existing information. In addition, the increasing number of people, devices, and sensors that are now connected by digital networks has revolutionized the ability to generate, communicate, share, and access data. Data creates enormous value for the world economy, driving innovation, productivity, efficiency and growth. At the same time, the \u201cdata deluge\u201d presents privacy concerns which could stir a regulatory backlash dampening the data economy and stifling innovation. In order to craft a balance between beneficial uses of data and in individual privacy, policymakers must address some of the most fundamental concepts of privacy law, including the definition of \u201cpersonally identifiable information\u201d, the role of individual control, and the principles of data minimization and purpose limitation. This article emphasizes the importance of providing individuals with access to their data in usable format. This will let individuals share the wealth created by their information and incentivize developers to offer user-side features and applications harnessing the value of big data. Where individual access to data is impracticable, data are likely to be de-identified to an extent sufficient to diminish privacy concerns. In addition, organizations should be required to disclose their decisional criteria, since in a big data world it is often not the data but rather the inferences drawn from them that give cause for concern.", "venue": "", "year": 2012, "referenceCount": 32, "citationCount": 450, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2301316", "name": "Omer Tene"}, {"authorId": "2476781", "name": "Jules Polonetsky"}]}}, {"contexts": ["Others suggest a mechanism to \u2018optout\u2019 of profiling for a particular purpose or context would help protect data subjects\u2019 privacy interests (Hildebrandt, 2011; Rubel and Jones, 2014).", "\u2026should do based on what the algorithm indicates); for behavioural data, \u2018objective\u2019 correlations can come to reflect the interpreter\u2019s \u2018\u2018unconscious motivations, particular emotions, deliberate choices, socio-economic determinations, geographic or demographic influences\u2019\u2019 (Hildebrandt, 2011: 376).", "Data subjects cannot define privacy norms to govern all types of data generically because their value or insightfulness is only established through processing (Hildebrandt, 2011; Van Wel and Royakkers, 2004).", "The longstanding problem of interpretability in machine learning algorithms indicates the challenge of opacity in algorithms (Burrell, 2016; Hildebrandt, 2011;\nLeese, 2014; Tutt, 2016).", "Despite this, correlations based on a sufficient volume of data are increasingly seen as sufficiently credible to direct action without first establishing causality (Hildebrandt, 2011; Hildebrandt and Koops, 2010; Mayer-Scho\u0308nberger and Cukier, 2013; Zarsky, 2016).", "Actionable insights (more on\nthis later) are sought rather than causal relationships (Grindrod, 2014; Hildebrandt, 2011; Johnson, 2013).", "Profiling seeks to assemble individuals into meaningful groups, for which identity is irrelevant (Floridi, 2012; Hildebrandt, 2011; Leese, 2014).", "It is difficult to detect latent bias in algorithms and the models they produce when encountered in isolation of the algorithm\u2019s development history (Friedman and Nissenbaum, 1996; Hildebrandt, 2011; Morek, 2006)."], "isInfluential": true, "intents": ["background"], "citedPaper": {"paperId": "af4b7318ec23e4bc94dda41d0db9afe056aaf412", "externalIds": {"MAG": "2008509487", "DOI": "10.1007/S13347-011-0041-8"}, "url": "https://www.semanticscholar.org/paper/af4b7318ec23e4bc94dda41d0db9afe056aaf412", "title": "Who Needs Stories if You Can Get the Data? ISPs in the Era of Big Number Crunching", "abstract": "In this article, I will investigate to what extent democracy and the Rule of Law require that ISPs as \u2018common carriers\u2019 that provide \u2018mere conduit\u2019 pre-empt extensive monitoring of the content they carry. I will trace this duty as a moral duty that is bound up with the framework of constitutional democracy, arguing that such monitoring affords unprecedented data-mining operations that could stifle our account of ourselves as moral agents in the novel infosphere.", "venue": "", "year": 2011, "referenceCount": 69, "citationCount": 31, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "40652172", "name": "M. Hildebrandt"}]}}, {"contexts": ["However, proxies for protected attributes are not easy to predict or detect (Romei and Ruggieri, 2014; Zarsky, 2016), particularly when algorithms access linked datasets (Barocas and Selbst, 2015).", "Romei and Ruggieri (2014) observe four overlapping strategies for discrimination prevention in analytics: (1) controlled distortion of training data; (2) integration of anti-discrimination criteria into the classifier algorithm; (3) post-processing of classification models; (4) modification of\u2026", "Various metrics of fairness are possible based on statistical parity, differential privacy and\nother relations between data subjects in classification tasks (Dwork et al., 2011; Romei and Ruggieri, 2014).", "\u20262016), or empirical researchers (Kitchin, 2016; Neyland, 2016), using ex post audit studies (Adler et al., 2016; Diakopoulos, 2015; Kitchin, 2016; Romei and Ruggieri, 2014; Sandvig et al., 2014), reflexive ethnographic studies in development and testing (Neyland, 2016), or reporting mechanisms\u2026", "Flaws in the data are inadvertently adopted by the algorithm and hidden in outputs and models produced (Barocas and Selbst, 2015; Romei and Ruggieri, 2014)."], "isInfluential": true, "intents": ["background"], "citedPaper": {"paperId": "b0d3968e7a630d3661cd0878acc38c85f1828a9c", "externalIds": {"DBLP": "journals/ker/RomeiR14", "MAG": "2166454173", "DOI": "10.1017/S0269888913000039"}, "url": "https://www.semanticscholar.org/paper/b0d3968e7a630d3661cd0878acc38c85f1828a9c", "title": "A multidisciplinary survey on discrimination analysis", "abstract": "Abstract The collection and analysis of observational and experimental data represent the main tools for assessing the presence, the extent, the nature, and the trend of discrimination phenomena. Data analysis techniques have been proposed in the last 50 years in the economic, legal, statistical, and, recently, in the data mining literature. This is not surprising, since discrimination analysis is a multidisciplinary problem, involving sociological causes, legal argumentations, economic models, statistical techniques, and computational issues. The objective of this survey is to provide a guidance and a glue for researchers and anti-discrimination data analysts on concepts, problems, application areas, datasets, methods, and approaches from a multidisciplinary perspective. We organize the approaches according to their method of data collection as observational, quasi-experimental, and experimental studies. A fourth line of recently blooming research on knowledge discovery based methods is also covered. Observational methods are further categorized on the basis of their application context: labor economics, social profiling, consumer markets, and others.", "venue": "The Knowledge Engineering Review", "year": 2013, "referenceCount": 532, "citationCount": 212, "influentialCitationCount": 12, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2886801", "name": "A. Romei"}, {"authorId": "2325428", "name": "S. Ruggieri"}]}}, {"contexts": ["For example, machine learning algorithms trained from human-tagged data inadvertently learn to reflect biases of the taggers (Diakopoulos, 2015).", "\u20262015; Tutt, 2016; Zarsky, 2016), or empirical researchers (Kitchin, 2016; Neyland, 2016), using ex post audit studies (Adler et al., 2016; Diakopoulos, 2015; Kitchin, 2016; Romei and Ruggieri, 2014; Sandvig et al., 2014), reflexive ethnographic studies in development and testing\u2026"], "isInfluential": false, "intents": ["methodology", "background"], "citedPaper": {"paperId": "b2e04e4bcd9b1a46541d4b115aae031dbaa1233a", "externalIds": {"MAG": "2332851105", "DOI": "10.1080/21670811.2014.976411"}, "url": "https://www.semanticscholar.org/paper/b2e04e4bcd9b1a46541d4b115aae031dbaa1233a", "title": "Algorithmic Accountability", "abstract": "Every day automated algorithms make decisions that can amplify the power of businesses and governments. Yet as algorithms come to regulate more aspects of our lives, the contours of their power can remain difficult to grasp. This paper studies the notion of algorithmic accountability reporting as a mechanism for elucidating and articulating the power structures, biases, and influences that computational artifacts exercise in society. A framework for algorithmic power based on autonomous decision-making is proffered and motivates specific questions about algorithmic influence. Five cases of algorithmic accountability reporting involving the use of reverse engineering methods in journalism are then studied and analyzed to provide insight into the method and its application in a journalism context. The applicability of transparency policies for algorithms is discussed alongside challenges to implementing algorithmic accountability as a broadly viable investigative method.", "venue": "", "year": 2015, "referenceCount": 68, "citationCount": 283, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2943892", "name": "N. Diakopoulos"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "b697a795ba0ca47ecaa663f24bb21138301a49c5", "externalIds": {"DBLP": "journals/clsr/Schermer11", "MAG": "1992577874", "DOI": "10.1016/J.CLSR.2010.11.009"}, "url": "https://www.semanticscholar.org/paper/b697a795ba0ca47ecaa663f24bb21138301a49c5", "title": "The limits of privacy in automated profiling and data mining", "abstract": "Abstract Automated profiling of groups and individuals is a common practice in our information society. The increasing possibilities of data mining significantly enhance the abilities to carry out such profiling. Depending on its application, profiling and data mining may cause particular risks such as discrimination, de-individualisation and information asymmetries. In this article we provide an overview of the risks associated with data mining and the strategies that have been proposed over the years to mitigate these risks. From there we shall examine whether current safeguards that are mainly based on privacy and data protection law (such as data minimisation and data exclusion) are sufficient. Based on these findings we shall suggest alternative policy options and regulatory instruments for dealing with the risks of data mining, integrating ideas from the field of computer science and that of law and ethics.", "venue": "Comput. Law Secur. Rev.", "year": 2011, "referenceCount": 0, "citationCount": 112, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3212794", "name": "B. Schermer"}]}}]}