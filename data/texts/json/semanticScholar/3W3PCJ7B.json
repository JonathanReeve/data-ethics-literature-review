{"paperId": "d48b0228add49c10a7f97006318f83a365b32293", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/d48b0228add49c10a7f97006318f83a365b32293", "title": "Site Selection Bias in Program Evaluation", "abstract": "\u201cSite selection bias\u201d can occur when the probability that a program is adopted or evaluated is correlated with its impacts. I test for site selection bias in the context of the Opower energy conservation programs, using 111 randomized control trials involving 8.6 million households across the U.S. Predictions based on rich microdata from the first ten replications substantially overstate efficacy in the next 101 sites. Several mechanisms caused this positive selection. For example, utilities in more environmentalist areas are more likely to adopt the program, and their customers are more responsive to the treatment. Also, because utilities initially target treatment at higher-usage consumer subpopulations, efficacy drops as the program is later expanded. The results illustrate how program evaluations can still give systematically biased out-of-sample predictions, even after many replications. Hunt Allcott Department of Economics New York University 19 W. 4th Street, 6th Floor New York, NY 10012 and NBER hunt.allcott@nyu.edu", "venue": "", "year": 2014, "referenceCount": 2, "citationCount": 186, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "5587090", "name": "X. Gin\u00e9"}, {"authorId": "118578603", "name": "J. Hotz"}, {"authorId": "47166531", "name": "G. Imbens"}, {"authorId": "145014338", "name": "Larry Katz"}, {"authorId": "119109683", "name": "C. Knittel"}, {"authorId": "46459823", "name": "Dan Levy"}, {"authorId": "102239567", "name": "Konrad Menzel"}, {"authorId": "3504516", "name": "E. Oster"}, {"authorId": "50540913", "name": "R. Pande"}]}