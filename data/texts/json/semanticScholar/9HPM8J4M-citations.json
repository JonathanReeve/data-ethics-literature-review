{"offset": 0, "next": 100, "data": [{"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0a1664f4eb2e2825c4b794f6fff6309614a1f3f8", "externalIds": {"DOI": "10.1016/b978-0-323-85852-6.00007-x"}, "url": "https://www.semanticscholar.org/paper/0a1664f4eb2e2825c4b794f6fff6309614a1f3f8", "title": "Bioactivity characterization of herbal molecules", "abstract": null, "venue": "Herbal Biomolecules in Healthcare Applications", "year": 2022, "referenceCount": 166, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2037066144", "name": "Leticia F\u00e9lix-Cuencas"}, {"authorId": "2131175830", "name": "E. Delis-Hechavarria"}, {"authorId": "2138008395", "name": "Alexandra Jarro"}, {"authorId": "1413720318", "name": "I. Parola-Contreras"}, {"authorId": "1752805701", "name": "A. Escamilla-Garc\u00eda"}, {"authorId": "2136743066", "name": "Irineo Torres-Pacheco"}, {"authorId": "1410895075", "name": "J. F. Garc\u00eda-Trejo"}, {"authorId": "1402199637", "name": "G. M. Soto-Zaraz\u00faa"}, {"authorId": "1382474827", "name": "R. Guevara-Gonz\u00e1lez"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "00655a5af5465b21922d2687bbd39da39d86c273", "externalIds": {"MAG": "3142352692", "DOI": "10.1108/ITP-04-2019-0173"}, "url": "https://www.semanticscholar.org/paper/00655a5af5465b21922d2687bbd39da39d86c273", "title": "Artificial intelligence video interviewing for employment: perspectives from applicants, companies, developer and academicians", "abstract": "PurposeIn 2018, an artificial intelligence (AI) interview platform was introduced and adopted by companies in Korea. This study aims to explore the perspectives of applicants who have experienced an AI-based interview through this platform and examines the opinions of companies, a platform developer and academia.Design/methodology/approachThis study uses a phenomenological approach. The participants, who had recent experience of AI video interviews, were recruited offline and online. Eighteen job applicants in their 20s, two companies that have adopted this interview platform, a software developer who created the platform and three professors participated in the study. To collect data, focus group interviews and in-depth interviews were conducted.FindingsAs a result, all of them believed that an AI-based interview was more efficient than a traditional one in terms of cost and time savings and is likely to be adopted by more companies in the future. They pointed to the possibility of data bias requiring an improvement in AI accountability. Applicants perceived an AI-based interview to be better than traditional evaluation procedures in procedural fairness, objectivity and consistency of algorithms. However, some applicants were dissatisfied about being assessed by AI. Digital divide and automated inequality were recurring themes in this study.Originality/valueThe study is important, as it addresses the real application of AI in detail, and a case study of smart hiring tools would be valuable in finding the practical and theoretical implications of such hiring in the fields of employment and AI.", "venue": "", "year": 2021, "referenceCount": 61, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "2110055363", "name": "Jin-young Kim"}, {"authorId": "2067164653", "name": "Wan Heo"}]}}, {"contexts": ["Similarly, the range of methods proposed reflected the range of stakeholders that are necessary to develop \u2018trustworthy AI\u2019: from governments with the mandate to create regulation, to researchers with their ability to shape the implications and features of their AI development process, and from industry actors who are in a position to create more diverse hiring processes (Crawford 2016), to civil society actors with their power to demand and engage in multi-stakeholder dialogues.", "\u2026with their ability to shape the implications and features of their AI development process, and from industry actors who are in a position to create more diverse hiring processes (Crawford 2016), to civil society actors with their power to demand and engage in multi-stakeholder dialogues."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "16dc267a80a87e81e6a8ec4ed8c5a020657dd7da", "externalIds": {"ArXiv": "2102.12406", "DBLP": "journals/see/Stix21", "PubMedCentral": "7895786", "DOI": "10.1007/s11948-020-00277-3", "PubMed": "33608756"}, "url": "https://www.semanticscholar.org/paper/16dc267a80a87e81e6a8ec4ed8c5a020657dd7da", "title": "Actionable Principles for Artificial Intelligence Policy: Three Pathways", "abstract": "In the development of governmental policy for artificial intelligence (AI) that is informed by ethics, one avenue currently pursued is that of drawing on \u201cAI Ethics Principles\u201d. However, these AI Ethics Principles often fail to be actioned\u00a0in governmental\u00a0policy. This paper proposes a novel framework for the development of \u2018Actionable Principles for AI\u2019. The approach acknowledges the relevance of AI Ethics Principles and homes in on methodological elements to increase their practical implementability in policy processes. As a case study, elements are extracted from the development process of the Ethics Guidelines for Trustworthy AI of the European Commission\u2019s \u201cHigh Level Expert Group on AI\u201d. Subsequently,\u00a0these elements are expanded on and\u00a0evaluated in light of their ability to contribute to a prototype framework for the development of\u00a0'Actionable Principles for AI'. The paper proposes the\u00a0following three propositions for the\u00a0formation of such a prototype\u00a0framework: (1) preliminary landscape assessments; (2) multi-stakeholder participation and cross-sectoral feedback; and, (3) mechanisms to support implementation and operationalizability.", "venue": "Sci. Eng. Ethics", "year": 2021, "referenceCount": 71, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1397118333", "name": "C. Stix"}]}}, {"contexts": ["A consistent 475 theme from the literature is the benefit of engaging a variety of stakeholders and maintaining 476 diversity along social lines where bias is a concern (racial diversity, gender diversity, age 477 diversity, diversity of physical ability) [32]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "39b667643ee3bb6553eae5f202e4ae6fa4d34aeb", "externalIds": {"MAG": "3176685063", "DOI": "10.6028/nist.sp.1270-draft"}, "url": "https://www.semanticscholar.org/paper/39b667643ee3bb6553eae5f202e4ae6fa4d34aeb", "title": "A Proposal for Identifying and Managing Bias in Artificial Intelligence", "abstract": null, "venue": "", "year": 2021, "referenceCount": 176, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": null, "name": "Reva Schwartz"}, {"authorId": "2129405820", "name": "Leann Down"}, {"authorId": "2054601833", "name": "A. Jonas"}, {"authorId": "2326261", "name": "Elham Tabassi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "76a50fc259327eff27d6010ff500c8519f45fd0b", "externalIds": {"DBLP": "conf/sigcpr/Monroe-White21", "DOI": "10.1145/3458026.3462161"}, "url": "https://www.semanticscholar.org/paper/76a50fc259327eff27d6010ff500c8519f45fd0b", "title": "Emancipatory Data Science: A Liberatory Framework for Mitigating Data Harms and Fostering Social Transformation", "abstract": "The cross-cutting and interdisciplinary nature of data work has created an opportunity to engage more students from diverse backgrounds in data science and has expanded pathways for entry for future data professionals. However, without greater representation of Black, Indigenous, and other marginalized people of color in data science, we risk reinforcing existing systems of differentiated power that oppress as opposed to empower these groups. In this paper, the term emancipatory data science is coined to highlight the unique contributions of individuals who use their expertise to mitigate data harms for minoritized, and marginalized populations and to suggest a way forward for the data science workforce and research community given our increasingly algorithmic society.", "venue": "SIGMIS-CPR", "year": 2021, "referenceCount": 144, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1399011392", "name": "T. Monroe-White"}]}}, {"contexts": ["In particular, public debate questioned the solidity of the Digital Tech industry\u2019s ethical credentials, including its failure to grapple effectively with AI\u2019s continuing \u201cWhite Guy\u201d problem (Crawford 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "9c8922caa56feef7f1502cb2ff3e656fa60b75ae", "externalIds": {"DOI": "10.1111/rego.12437"}, "url": "https://www.semanticscholar.org/paper/9c8922caa56feef7f1502cb2ff3e656fa60b75ae", "title": "Algorithmic regulation: A maturing concept for investigating regulation\n of\n and\n through\n algorithms", "abstract": "This paper offers a critical synthesis of the articles in this Special Issue with a view to assessing the concept of \u201calgorithmic regulation\u201d as a mode of social coordination and control articulated by Yeung in 2017. We highlight significant changes in public debate about the role of algorithms in society occurring in the last five years. We also highlight prominent themes that emerge from the contributions, illuminating what is distinctive about the concept of algorithmic regulation, reflecting upon some of its strengths, limitations, and its relationship with the broader research field. In closing, we argue that the core concept is valuable and maturing. It has evolved into an analytical bridge that fosters cross-disciplinary development and analysis in ways that enrich its early \u201cskeletal\u201d form, thereby enabling careful and context-sensitive analysis of algorithmic regulation in concrete settings while facilitating critical reflection concerning the legitimacy of existing and proposed regulatory regimes.", "venue": "Regulation & Governance", "year": 2021, "referenceCount": 125, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2094412391", "name": "Lena Ulbricht"}, {"authorId": "3002344", "name": "K. Yeung"}]}}, {"contexts": ["More recently, ML has entered a variety of domains including marketing [22, 36], ethics [7], law, and policy [42, 68]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "a35e6dcf111c252173d4141170f77adb02c90083", "externalIds": {"DBLP": "conf/www/GuYR21", "DOI": "10.1145/3442381.3450092"}, "url": "https://www.semanticscholar.org/paper/a35e6dcf111c252173d4141170f77adb02c90083", "title": "Understanding User Sensemaking in Machine Learning Fairness Assessment Systems", "abstract": "A variety of systems have been proposed to assist users in detecting machine learning (ML) fairness issues. These systems approach bias reduction from a number of perspectives, including recommender systems, exploratory tools, and dashboards. In this paper, we seek to inform the design of these systems by examining how individuals make sense of fairness issues as they use different de-biasing affordances. In particular, we consider the tension between de-biasing recommendations which are quick but may lack nuance and \u201dwhat-if\u201d style exploration which is time consuming but may lead to deeper understanding and transferable insights. Using logs, think-aloud data, and semi-structured interviews we find that exploratory systems promote a rich pattern of hypothesis generation and testing, while recommendations deliver quick answers which satisfy participants at the cost of reduced information exposure. We highlight design requirements and trade-offs in the design of ML fairness systems to promote accurate and explainable assessments.", "venue": "WWW", "year": 2021, "referenceCount": 103, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "9865532", "name": "Ziwei Gu"}, {"authorId": "2112650228", "name": "Jing Nathan Yan"}, {"authorId": "2494495", "name": "Jeffrey M. Rzeszotarski"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "a3c607f7f209184858b6483d76a61860c70ce032", "externalIds": {"MAG": "3194483009", "DOI": "10.47059/revistageintec.v11i4.2512"}, "url": "https://www.semanticscholar.org/paper/a3c607f7f209184858b6483d76a61860c70ce032", "title": "Implications of GDPR on Emerging Technologies", "abstract": "In the present digital era, organizations worldwide are facing several opportunities and challenges in safeguarding and preserving significant data that are essential in the efficient functioning of organizational activities. Organizations have realized and understood the importance of data collection and analysis to influence target achievements and meet futuristic demands. Many small-scale enterprises and start-ups have also initiated decisions to implement technologies that enable a positive long-term impact. Due to these requirements, the General Data Protection Regulation (GDPR) implementation has become a necessity to ensure future sustenance and functionality of existing and growing organizations. The operational activities in the organizational environment are forced to comply with the general data protection regulation policies and framework. The European Union general data protection regulation, which was imposed in May 2018 (European Union General Data Protection Regulation, 2016), has proven to be effective, both within the country's internal boundaries and globally. However, many organizations are still not familiar with the general data protection regulation compliance policies. The emergence of general data protection regulation has been a recent interest in the activities of various organizational sectors as they are attempting to understand the policies and compliance requirements on the implementation of GDPR applications. The research intends to explore the implications between the GDPR and emerging technologies and suggests various recommendations for organizations to implement and follow the GDPR guidelines that could enhance organizational activities.", "venue": "Revista Gest\u00e3o Inova\u00e7\u00e3o e Tecnologias", "year": 2021, "referenceCount": 24, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Business"], "authors": [{"authorId": "1450700447", "name": "S. Biswal"}, {"authorId": "48460770", "name": "M. Kulkarni"}]}}, {"contexts": ["For another example involving Google\u2019s photo app, see Crawford (2016) and Emspak (2016)."], "isInfluential": false, "intents": ["methodology"], "citingPaper": {"paperId": "afdc57412a7dadf2a392a16276f05df757d5b98b", "externalIds": {"DBLP": "journals/ml/Heinze-DemlM21", "MAG": "2795857097", "DOI": "10.1007/s10994-020-05924-1"}, "url": "https://www.semanticscholar.org/paper/afdc57412a7dadf2a392a16276f05df757d5b98b", "title": "Conditional variance penalties and domain shift robustness", "abstract": "When training a deep neural network for image classification, one can broadly distinguish between two types of latent features of images that will drive the classification. We can divide latent features into (i) \"core\" or \"conditionally invariant\" features $X^\\text{core}$ whose distribution $X^\\text{core}\\vert Y$, conditional on the class $Y$, does not change substantially across domains and (ii) \"style\" features $X^{\\text{style}}$ whose distribution $X^{\\text{style}} \\vert Y$ can change substantially across domains. Examples for style features include position, rotation, image quality or brightness but also more complex ones like hair color, image quality or posture for images of persons. Our goal is to minimize a loss that is robust under changes in the distribution of these style features. In contrast to previous work, we assume that the domain itself is not observed and hence a latent variable. \nWe do assume that we can sometimes observe a typically discrete identifier or \"$\\mathrm{ID}$ variable\". In some applications we know, for example, that two images show the same person, and $\\mathrm{ID}$ then refers to the identity of the person. The proposed method requires only a small fraction of images to have $\\mathrm{ID}$ information. We group observations if they share the same class and identifier $(Y,\\mathrm{ID})=(y,\\mathrm{id})$ and penalize the conditional variance of the prediction or the loss if we condition on $(Y,\\mathrm{ID})$. Using a causal framework, this conditional variance regularization (CoRe) is shown to protect asymptotically against shifts in the distribution of the style variables. Empirically, we show that the CoRe penalty improves predictive accuracy substantially in settings where domain changes occur in terms of image quality, brightness and color while we also look at more complex changes such as changes in movement and posture.", "venue": "Mach. Learn.", "year": 2017, "referenceCount": 90, "citationCount": 83, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1393626786", "name": "C. Heinze-Deml"}, {"authorId": "1941834", "name": "N. Meinshausen"}]}}, {"contexts": ["This is a serious barrier to the adoption of these methods, because organizations must be persuaded to sacrifice the effectiveness of their systems for the altruistic goal of fair behavior [12].", "As of now, these reasons have not been sufficient for widespread adoption [12].", "A reduction in a deployed model\u2019s predictive performance can harm an organization\u2019s profitability, which the management may not be willing to tolerate [12].", "Stakeholders who are impacted by these systems are often far-removed or under-represented in the AI research laboratories in academia and industry that design them, yet their voices must be heard [12]."], "isInfluential": true, "intents": ["background"], "citingPaper": {"paperId": "bc291e734e45b4527197bf21e53742331824108e", "externalIds": {"DBLP": "conf/aies/IslamPF21", "DOI": "10.1145/3461702.3462614"}, "url": "https://www.semanticscholar.org/paper/bc291e734e45b4527197bf21e53742331824108e", "title": "Can We Obtain Fairness For Free?", "abstract": "There is growing awareness that AI and machine learning systems can in some cases learn to behave in unfair and discriminatory ways with harmful consequences. However, despite an enormous amount of research, techniques for ensuring AI fairness have yet to see widespread deployment in real systems. One of the main barriers is the conventional wisdom that fairness brings a cost in predictive performance metrics such as accuracy which could affect an organization's bottom-line. In this paper we take a closer look at this concern. Clearly fairness/performance trade-offs exist, but are they inevitable? In contrast to the conventional wisdom, we find that it is frequently possible, indeed straightforward, to improve on a trained model's fairness without sacrificing predictive performance. We systematically study the behavior of fair learning algorithms on a range of benchmark datasets, showing that it is possible to improve fairness to some degree with no loss (or even an improvement) in predictive performance via a sensible hyper-parameter selection strategy. Our results reveal a pathway toward increasing the deployment of fair AI methods, with potentially substantial positive real-world impacts.", "venue": "AIES", "year": 2021, "referenceCount": 57, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "47755547", "name": "Rashidul Islam"}, {"authorId": null, "name": "Shimei Pan"}, {"authorId": "40289577", "name": "James R. Foulds"}]}}, {"contexts": ["Studies in software engineering [41], machine learning [17], computer game design [31] and social sciences [108] have adequately demonstrated that technologies can (by design or accident) embody values, especially of those who create them."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "c68e0de15146d8fbfabe7561e8bbc1a6cd2b2019", "externalIds": {"DBLP": "journals/corr/abs-2108-05624", "ArXiv": "2108.05624"}, "url": "https://www.semanticscholar.org/paper/c68e0de15146d8fbfabe7561e8bbc1a6cd2b2019", "title": "Operationalizing Human Values in Software Engineering: A Survey", "abstract": "Human values (e.g., pleasure, privacy, and social justice) are what a person or a society considers important. Inability to address them in software-intensive systems can result in numerous undesired consequences (e.g., financial losses) for individuals and communities. Various solutions (e.g., methodologies, techniques) are developed to help \u201coperationalize values in software\u201d. The ultimate goal is to ensure building software (better) reflects and respects human values. In this survey, \u201coperationalizing values\u201d is referred to as the process of identifying human values and translating them to accessible and concrete concepts so that they can be implemented, validated, verified, and measured in software. This paper provides a deep understanding of the research landscape on operationalizing values in software engineering, covering 51 primary studies. It also presents an analysis and taxonomy of 51 solutions for operationalizing values in software engineering. Our survey reveals that most solutions attempt to help operationalize values in the early phases (requirements and design) of the software development life cycle. However, the later phases (implementation and testing) and other aspects of software development (e.g., \u201cteam organization\u201d) still need adequate consideration. We outline implications for research and practice and identify open issues and future research directions to advance this area.", "venue": "ArXiv", "year": 2021, "referenceCount": 119, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "32723366", "name": "Mojtaba Shahin"}, {"authorId": "3470719", "name": "Waqar Hussain"}, {"authorId": "3060591", "name": "Arif Nurwidyantoro"}, {"authorId": "39775242", "name": "Harsha Perera"}, {"authorId": "1727225", "name": "R. Shams"}, {"authorId": "1687239", "name": "J. Grundy"}, {"authorId": "1795817", "name": "J. Whittle"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "eb863f2f34a0d37e2aef183554816700bd7de5d1", "externalIds": {"MAG": "3159953285", "DOI": "10.13110/DISCOURSE.43.1.0031"}, "url": "https://www.semanticscholar.org/paper/eb863f2f34a0d37e2aef183554816700bd7de5d1", "title": "Artificial Eye: The Modernist Origins of AI's Gender Problem", "abstract": null, "venue": "", "year": 2021, "referenceCount": 133, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Art"], "authors": [{"authorId": "146070029", "name": "A. Fryxell"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "00d57c97d8c390d281f296264edc75c78ea03360", "externalIds": {"DOI": "10.1215/9781478007272-013"}, "url": "https://www.semanticscholar.org/paper/00d57c97d8c390d281f296264edc75c78ea03360", "title": "References", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "03a297d0514c4ba65899743b75a84fa2bc3f7f1f", "externalIds": {"DOI": "10.1215/9781478007272-004"}, "url": "https://www.semanticscholar.org/paper/03a297d0514c4ba65899743b75a84fa2bc3f7f1f", "title": "Hostile Environment (DoD)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "04a55a31b2153f97df079f676078135dbe0b49ed", "externalIds": {"MAG": "2955666017", "DOI": "10.1080/13600869.2020.1733762"}, "url": "https://www.semanticscholar.org/paper/04a55a31b2153f97df079f676078135dbe0b49ed", "title": "Fear the Reaper: how content moderation rules are enforced on social media", "abstract": "ABSTRACT This paper investigates the enforcement stage of the content moderation process on social media platforms. It argues that the current approach adopted by most platforms is underdeveloped, poses serious human rights issues, and would benefit from a number of reforms. This paper will first explain what the role of the moderator entails and how the enforcement process occurs at social media platforms. The investigation will then turn to the enforcement process itself and identify a number of problems in how content rules are generally enforced including bias in decision-making, an over-reliance on efficiency as a solution, and inconsistent enforcement of terms and conditions. Finally, it will provide a number of suggestions for reform including moving away from the efficiency narrative to consider larger issues in enforcement (which are not so easily solved) such as human rights and rule of law issues and the adoption of a body of precedents as a tool for accountability and empowerment of users.", "venue": "", "year": 2020, "referenceCount": 118, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "123386536", "name": "MacKenzie Common"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "08ad78773cbf338f20b5a1942cf915ea00745315", "externalIds": {"DOI": "10.1215/9781478009276-010"}, "url": "https://www.semanticscholar.org/paper/08ad78773cbf338f20b5a1942cf915ea00745315", "title": "The Unattributable", "abstract": null, "venue": "Cloud Ethics", "year": 2020, "referenceCount": 264, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": ["For instance, as a well-known example of algorithmic bias, Google identifies black people as gorillas (Crawford, 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "0aa0e4179bc14d2011fb1a7e5785835c8cb9fef7", "externalIds": {"ArXiv": "2002.10703", "MAG": "3007919392", "DBLP": "journals/corr/abs-2002-10703"}, "url": "https://www.semanticscholar.org/paper/0aa0e4179bc14d2011fb1a7e5785835c8cb9fef7", "title": "G\u00f6del's Sentence Is An Adversarial Example But Unsolvable", "abstract": "In recent years, different types of adversarial examples from different fields have emerged endlessly, including purely natural ones without perturbations. A variety of defenses are proposed and then broken quickly. Two fundamental questions need to be asked: What's the reason for the existence of adversarial examples and are adversarial examples unsolvable? In this paper, we will show the reason for the existence of adversarial examples is there are non-isomorphic natural explanations that can all explain data set. Specifically, for two natural explanations of being true and provable, G\\\"odel's sentence is an adversarial example but ineliminable. It can't be solved by the re-accumulation of data set or the re-improvement of learning algorithm. Finally, from the perspective of computability, we will prove the incomputability for adversarial examples, which are unrecognizable.", "venue": "ArXiv", "year": 2020, "referenceCount": 48, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "48610835", "name": "Xiaodong Qi"}, {"authorId": "2256602", "name": "Lansheng Han"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "11f323ce47a0ec80e223e3266fa4a13b2d739e78", "externalIds": {"DOI": "10.1215/9781478007272-012"}, "url": "https://www.semanticscholar.org/paper/11f323ce47a0ec80e223e3266fa4a13b2d739e78", "title": "Notes", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": ["%) and white (\u223c85%), reflecting a demographic homogeneity that plagues the field of AI, and STEM fields more broadly (i.e., the \u2018white guy problem\u2019) (Crawford, 2016).3 To address this, we grant particular attention to the voices of underrepresented participants, as they offer a standpoint\u2026"], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "150298b517c930947ca0fc3d69f25505297ab623", "externalIds": {"MAG": "3001465270", "DOI": "10.1080/1369118X.2020.1713842"}, "url": "https://www.semanticscholar.org/paper/150298b517c930947ca0fc3d69f25505297ab623", "title": "Attributions of ethical responsibility by Artificial Intelligence practitioners", "abstract": "ABSTRACT Systems based on Artificial Intelligence (AI) are increasingly normalized as part of work, leisure, and governance in contemporary societies. Although ethics in AI has received significant attention, it remains unclear where the burden of responsibility lies. Through twenty-one interviews with AI practitioners in Australia, this research seeks to understand how ethical attributions figure into the professional imagination. As institutionally embedded technical experts, AI practitioners act as a connective tissue linking the range of actors that come in contact with, and have effects upon, AI products and services. Findings highlight that practitioners distribute ethical responsibility across a range of actors and factors, reserving a portion of responsibility for themselves, albeit constrained. Characterized by imbalances of decision-making power and technical expertise, practitioners position themselves as mediators between powerful bodies that set parameters for production; users who engage with products once they leave the proverbial workbench; and AI systems that evolve and develop beyond practitioner control. Distributing responsibility throughout complex sociotechnical networks, practitioners preclude simple attributions of accountability for the social effects of AI. This indicates that AI ethics are not the purview of any singular player but instead, derive from collectivities that require critical guidance and oversight at all stages of conception, production, distribution, and use.", "venue": "", "year": 2020, "referenceCount": 73, "citationCount": 21, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "117258238", "name": "W. Orr"}, {"authorId": "152658412", "name": "Jenny L. Davis"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "165995ab49996cabbc718261cc88addca107150f", "externalIds": {"MAG": "3022273229", "DBLP": "journals/dss/BunnellOY20", "DOI": "10.1016/j.dss.2020.113306"}, "url": "https://www.semanticscholar.org/paper/165995ab49996cabbc718261cc88addca107150f", "title": "FinPathlight: Framework for an multiagent recommender system designed to increase consumer financial capability", "abstract": "Abstract In consideration of the general lack of trust in human professional financial advisors due to conflicts of interest, and given inadequacies in terms of the utility of FinTech alternatives for financial goal recommendations, this study establishes a framework for an ontology-based, multiagent recommender system designed to improve financial capability through the recommendation of financial goals, called FinPathlight. The FinPathlight framework provides an architecture for a personal financial recommender system designed to identify and recommend specific, achievable financial goals appropriate to a wide range of financially situated users. This framework contributes principles of implementation for a novel financial technology (FinTech) application aimed at addressing a pervasive lack of trust surrounding traditional financial advisory services, as well as utility inadequacies within the current landscape for FinTech applications, providing a comprehensive set of practical and explicit financial goal recommendations. Considering the importance of users' adoption of an innovation, this study empirically tests its utility in terms of trust and perceived usefulness. The experimental evaluation results show that an application built using this framework would likely be perceived as trustworthy and useful to users for identification and selection of financial capability enhancing objectives.", "venue": "Decis. Support Syst.", "year": 2020, "referenceCount": 91, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "123386366", "name": "Lawrence Bunnell"}, {"authorId": "1398688943", "name": "Kweku-Muata A. Osei-Bryson"}, {"authorId": "1717848", "name": "V. Yoon"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2891f26c5352d59c28b24cb6e3fb6f7e913df470", "externalIds": {"DOI": "10.1215/9781478007272-009"}, "url": "https://www.semanticscholar.org/paper/2891f26c5352d59c28b24cb6e3fb6f7e913df470", "title": "Escalation (DoD)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2a0036b482d9be9ddc3f924ef3c944f73f680618", "externalIds": {"MAG": "3005496847", "DBLP": "conf/aies/Cave20", "DOI": "10.1145/3375627.3375813"}, "url": "https://www.semanticscholar.org/paper/2a0036b482d9be9ddc3f924ef3c944f73f680618", "title": "The Problem with Intelligence: Its Value-Laden History and the Future of AI", "abstract": "This paper argues that the concept of intelligence is highly value-laden in ways that impact on the field of AI and debates about its risks and opportunities. This value-ladenness stems from the historical use of the concept of intelligence in the legitimation of dominance hierarchies. The paper first provides a brief overview of the history of this usage, looking at the role of intelligence in patriarchy, the logic of colonialism and scientific racism. It then highlights five ways in which this ideological legacy might be interacting with debates about AI and its risks and opportunities: 1) how some aspects of the AI debate perpetuate the fetishization of intelligence; 2) how the fetishization of intelligence impacts on diversity in the technology industry; 3) how certain hopes for AI perpetuate notions of technology and the mastery of nature; 4) how the association of intelligence with the professional class misdirects concerns about AI; and 5) how the equation of intelligence and dominance fosters fears of superintelligence. This paper therefore takes a first step in bringing together the literature on intelligence testing, eugenics and colonialism from a range of disciplines with that on the ethics and societal impact of AI.", "venue": "AIES", "year": 2020, "referenceCount": 105, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "51129868", "name": "S. Cave"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2c7c89f80b8f515f7ca1eac127476d7b8fb68232", "externalIds": {"MAG": "2998255389", "DOI": "10.1016/j.bushor.2019.11.004"}, "url": "https://www.semanticscholar.org/paper/2c7c89f80b8f515f7ca1eac127476d7b8fb68232", "title": "Designing, developing, and deploying artificial intelligence systems: Lessons from and for the public sector", "abstract": "Artificial intelligence applications in cognitive computing systems can be found in organizations across every market, including chatbots that help customers navigate websites, predictive analytics systems used for fraud detection, and augmented decision-support systems for knowledge workers. In this article, we share reflections and insights from our experience with AI projects in the public sector that can add value to any organization. We organized our findings into four thematic domains\u2014(1) data, (2) technology, (3) organizational, and (4) environmental\u2014and examine them relative to the phases of AI. We conclude with best practices for capturing value with cognitive computing systems.", "venue": "", "year": 2020, "referenceCount": 27, "citationCount": 32, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1697759", "name": "K. Desouza"}, {"authorId": "3309399", "name": "G. Dawson"}, {"authorId": "2266708", "name": "Daniel Chenok"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "31053e5b1b6373a3c9f97e032412bdb13158e0d5", "externalIds": {"DOI": "10.1215/9781478007272-008"}, "url": "https://www.semanticscholar.org/paper/31053e5b1b6373a3c9f97e032412bdb13158e0d5", "title": "Vital Ground (DoD)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": ["A well-developed ML should have a basis of an interpretable model in order to reduce irrational bias, rather than reinforcing it, to account for public policies[1, 2]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "36c120562c6b8f0e4c875339a6316f3933654792", "externalIds": {"MAG": "2995579932", "ArXiv": "1704.00240", "DBLP": "journals/corr/KajitaK17", "DOI": "10.1016/j.ijforecast.2019.06.005"}, "url": "https://www.semanticscholar.org/paper/36c120562c6b8f0e4c875339a6316f3933654792", "title": "Crime Prediction by Data-Driven Green's Function method", "abstract": "Abstract We develop an algorithm that forecasts cascading events, by employing a Green\u2019s function scheme on the basis of the self-exciting point process model. This method is applied to open data of 10 types of crimes happened in Chicago. It shows a good prediction accuracy superior to or comparable to the standard methods which are the expectation\u2013maximization method and prospective hotspot maps method. We find a cascade influence of the crimes that has a long-time, logarithmic tail; this result is consistent with an earlier study on burglaries. This long-tail feature cannot be reproduced by the other standard methods. In addition, a merit of the Green\u2019s function method is the low computational cost in the case of high density of events and/or large amount of the training data.", "venue": "International Journal of Forecasting", "year": 2017, "referenceCount": 79, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics", "Physics", "Economics"], "authors": [{"authorId": "10642082", "name": "Mami Kajita"}, {"authorId": "48152185", "name": "Seiji Kajita"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "38efeb2862e96bbf1c3b09f9d98a30d9a47cbaa5", "externalIds": {"DOI": "10.1215/9781478009276-009"}, "url": "https://www.semanticscholar.org/paper/38efeb2862e96bbf1c3b09f9d98a30d9a47cbaa5", "title": "The Doubtful Algorithm", "abstract": null, "venue": "Cloud Ethics", "year": 2020, "referenceCount": 264, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4b1ea05c83d44b984db8b7d5764ef306d602dc35", "externalIds": {"DBLP": "conf/chi/DoveF20", "MAG": "3030367426", "DOI": "10.1145/3313831.3376275"}, "url": "https://www.semanticscholar.org/paper/4b1ea05c83d44b984db8b7d5764ef306d602dc35", "title": "Monsters, Metaphors, and Machine Learning", "abstract": "Machine learning (ML) poses complex challenges for user experience (UX) designers. Typically unpredictable and opaque, it may produce unforeseen outcomes detrimental to particular groups or individuals, yet simultaneously promise amazing breakthroughs in areas as diverse as medical diagnosis and universal translation. This results in a polarized view of ML, which is often manifested through a technology-as-monster metaphor. In this paper, we acknowledge the power and potential of this metaphor by resurfacing historic complexities in human-monster relations. We (re)introduce these liminal and ambiguous creatures, and discuss their relation to ML. We offer a background to designers' use of metaphor, and show how the technology-as-monster metaphor can generatively probe and (re)frame the questions ML poses. We illustrate the effectiveness of this approach through a detailed discussion of an early-stage generative design workshop inquiring into ML approaches to supporting student mental health and well-being.", "venue": "CHI", "year": 2020, "referenceCount": 313, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Sociology"], "authors": [{"authorId": "153836482", "name": "G. Dove"}, {"authorId": "79242381", "name": "Anne-Laure Fayard"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "52ad599144f4f42bab61a8c132698296fa9758b7", "externalIds": {"MAG": "3081080629", "DBLP": "journals/software/AdamsK20", "DOI": "10.1109/MS.2020.2975075"}, "url": "https://www.semanticscholar.org/paper/52ad599144f4f42bab61a8c132698296fa9758b7", "title": "The Diversity Crisis of Software Engineering for Artificial Intelligence", "abstract": "Artificial Intelligence (AI) is experiencing a \"diversity crisis.\"1 Several reports1-3 have shown how the breakthrough of modern AI has not yet been able to improve on existing diversity challenges regarding gender, race, geography, and other factors, neither for the end users of those products nor the companies and organizations building them. Plenty of examples have surfaced in which biased data engineering practices or existing data sets led to incorrect, painful, or sometimes even harmful consequences for unassuming end users.4 The problem is that ruling out such biases is not straightforward due to the sheer number of different bias types.5 To have a chance to eliminate as many biases as possible, most of the experts agree that the teams and organizations building AI products should be made more diverse.1-3 This harkens back to Linus' law6 for open source development (\"given enough eyeballs, all bugs are shallow\") but applied to the development process of AI products.", "venue": "IEEE Software", "year": 2020, "referenceCount": 22, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143612725", "name": "B. Adams"}, {"authorId": "1703493", "name": "F. Khomh"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "56cc571eb44711603c1cabad0d6a7b4b82cd4deb", "externalIds": {"DOI": "10.1215/9781478007272-002"}, "url": "https://www.semanticscholar.org/paper/56cc571eb44711603c1cabad0d6a7b4b82cd4deb", "title": "Identification Friend or Foe (DoD)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": ["Even in a domestic context, civil society voices have argued that minorities are placed at risk from AI systems in ways more likely to be overlooked by a disproportionately white, male workforce [13]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "5f22c040529b32352549069f0bbf988c0a858d24", "externalIds": {"MAG": "3038506137", "DBLP": "conf/smsociety/Elhai20", "DOI": "10.1145/3400806.3400832"}, "url": "https://www.semanticscholar.org/paper/5f22c040529b32352549069f0bbf988c0a858d24", "title": "Regulating Digital Harm Across Borders: Exploring a Content Platform Commission", "abstract": "Regulatory systems crafted in and for countries with well-functioning, well-intentioned democratic governments will have unintended effects that spill over to other societies. Looking at the regulation of online content platforms, risks surface that support a regulatory solution that considers the safety and human rights of people in countries whose governments are unwilling or unable to regulate content platforms effectively. This paper explores the strengths and weaknesses of three general approaches to the cross-border regulation of online content platforms \u2013 certification, liability, and self-regulation. Ultimately, it offers a co-regulatory approach in which the U.S. and/or EU governments certifies a quasi-governmental Content Platform Commission charged with setting transparency and redress standards and publishing research on risks and mitigation strategies in each market where content platforms operate. The proposed solution seeks to preserve intermediary liability protections and maintain platforms\u2019 freedom to innovate while ensuring that decisions that affect people's interests are made transparently and with avenues for redress.", "venue": "SMSociety", "year": 2020, "referenceCount": 155, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science", "Computer Science"], "authors": [{"authorId": "118990672", "name": "Wren Elhai"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "61a33cc5299f187ef52846276ad7d76285394728", "externalIds": {"DOI": "10.1215/9781478007272-011"}, "url": "https://www.semanticscholar.org/paper/61a33cc5299f187ef52846276ad7d76285394728", "title": "Armistice (DoD)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "688f1aadcab1fb4f55c3f9642ceb51f0b3b1c97a", "externalIds": {"ArXiv": "2012.09110", "DBLP": "journals/corr/abs-2012-09110", "MAG": "3113336120"}, "url": "https://www.semanticscholar.org/paper/688f1aadcab1fb4f55c3f9642ceb51f0b3b1c97a", "title": "Developing Future Human-Centered Smart Cities: Critical Analysis of Smart City Security, Interpretability, and Ethical Challenges", "abstract": "As we make tremendous advances in machine learning and artificial intelligence technosciences, there is a renewed understanding in the AI community that we must ensure that humans being are at the center of our deliberations so that we don't end in technology-induced dystopias. As strongly argued by Green in his book Smart Enough City, the incorporation of technology in city environs does not automatically translate into prosperity, wellbeing, urban livability, or social justice. There is a great need to deliberate on the future of the cities worth living and designing. There are philosophical and ethical questions involved along with various challenges that relate to the security, safety, and interpretability of AI algorithms that will form the technological bedrock of future cities. Several research institutes on human centered AI have been established at top international universities. Globally there are calls for technology to be made more humane and human-compatible. For example, Stuart Russell has a book called Human Compatible AI. The Center for Humane Technology advocates for regulators and technology companies to avoid business models and product features that contribute to social problems such as extremism, polarization, misinformation, and Internet addiction. In this paper, we analyze and explore key challenges including security, robustness, interpretability, and ethical challenges to a successful deployment of AI or ML in human-centric applications, with a particular emphasis on the convergence of these challenges. We provide a detailed review of existing literature on these key challenges and analyze how one of these challenges may lead to others or help in solving other challenges. The paper also advises on the current limitations, pitfalls, and future directions of research in these domains, and how it can fill the current gaps and lead to better solutions.", "venue": "ArXiv", "year": 2020, "referenceCount": 393, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Political Science"], "authors": [{"authorId": "143623051", "name": "Kashif Ahmad"}, {"authorId": "3458683", "name": "Majdi Maabreh"}, {"authorId": "1658923967", "name": "M. Ghaly"}, {"authorId": "120457379", "name": "Khalil Khan"}, {"authorId": "1598877089", "name": "Junaid Qadir"}, {"authorId": "1389945327", "name": "Ala Al-Fuqaha"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "68b10226a84a4199f411541e118b87c4ec705c03", "externalIds": {"DOI": "10.1215/9781478009276-001"}, "url": "https://www.semanticscholar.org/paper/68b10226a84a4199f411541e118b87c4ec705c03", "title": "Politics and Ethics in the Age of Algorithms", "abstract": null, "venue": "Cloud Ethics", "year": 2020, "referenceCount": 264, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7b54f9bdfa5eff058160434327bd58faa90523ac", "externalIds": {"MAG": "3006672344", "DOI": "10.1080/0267257X.2020.1724178"}, "url": "https://www.semanticscholar.org/paper/7b54f9bdfa5eff058160434327bd58faa90523ac", "title": "The analogue diaries of postdigital consumption", "abstract": "ABSTRACT Even in a world that is saturated with the digital, we still seek out analogue objects. Drawing on concepts of postdigital aesthetics, we examine the use of analogue objects to escape the omnipresence of the digital realm. Based on consumer narratives from interview, archival, and netnographic data involving the use of analogue notebooks and film cameras, we derive the notion of postdigital consumption and analyse the \u2018digital\u2019 as a background object foregrounding the analogue. Our findings reveal ways in which consumers use these analogue objects to escape controlled consumption, to enchant their consumption with their labour, and to seek continuity and permanence, in navigating paradoxical relationships with the digital world.", "venue": "", "year": 2020, "referenceCount": 231, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Art"], "authors": [{"authorId": "123626571", "name": "M. Humayun"}, {"authorId": "69520434", "name": "R. Belk"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7be2979e96dae9ad6d37238923c3990ef17d24d5", "externalIds": {"PubMedCentral": "7280134", "DBLP": "journals/ijinfoman/Sipior20", "MAG": "3033847829", "DOI": "10.1016/j.ijinfomgt.2020.102170", "PubMed": "32836632"}, "url": "https://www.semanticscholar.org/paper/7be2979e96dae9ad6d37238923c3990ef17d24d5", "title": "Considerations for development and use of AI in response to COVID-19", "abstract": "\n Abstract\n \n Artificial intelligence (AI) is playing a key supporting role in the fight against COVID-19 and perhaps will contribute to solutions quicker than we would otherwise achieve in many fields and applications. Since the outbreak of the pandemic, there has been an upsurge in the exploration and use of AI, and other data analytic tools, in a multitude of areas. This paper addresses some of the many considerations for managing the development and deployment of AI applications, including planning; unpredictable, unexpected, or biased results; repurposing; the importance of data; and diversity in AI team membership. We provide implications for research and for practice, according to each of the considerations. Finally we conclude that we need to plan and carefully consider the issues associated with the development and use of AI as we look for quick solutions.\n \n", "venue": "International Journal of Information Management", "year": 2020, "referenceCount": 81, "citationCount": 35, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1729334", "name": "J. Sipior"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7d3fefd8733647f49984d4ada5100079b68223c9", "externalIds": {"DOI": "10.1215/9781478007272-005"}, "url": "https://www.semanticscholar.org/paper/7d3fefd8733647f49984d4ada5100079b68223c9", "title": "In Extremis (DoD)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": ["Such biases may cause discriminatory decisions (e.g., warnings or arrests) that feed back into the increasingly biased datasets (Crawford 2016), thereby completing a vicious cycle.", ", warnings or arrests) that feed back into the increasingly biased datasets (Crawford 2016), thereby completing a vicious cycle."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "7eb03a3c0605d688959eef30518f6eb268996eae", "externalIds": {"PubMedCentral": "7286860", "DBLP": "journals/see/FloridiCKT20", "MAG": "3014499801", "DOI": "10.1007/s11948-020-00213-5", "PubMed": "32246245"}, "url": "https://www.semanticscholar.org/paper/7eb03a3c0605d688959eef30518f6eb268996eae", "title": "How to Design AI for Social Good: Seven Essential Factors", "abstract": "The idea of artificial intelligence for social good (henceforth AI4SG) is gaining traction within information societies in general and the AI community in particular. It has the potential to tackle social problems through the development of AI-based solutions. Yet, to date, there is only limited understanding of what makes AI socially good in theory, what counts as AI4SG in practice, and how to reproduce its initial successes in terms of policies. This article addresses this gap by identifying seven ethical factors that are essential for future AI4SG initiatives. The analysis is supported by 27 case examples of AI4SG projects. Some of these factors are almost entirely novel to AI, while the significance of other factors is heightened by the use of AI. From each of these factors, corresponding best practices are formulated which, subject to context and balance, may serve as preliminary guidelines to ensure that well-designed AI is more likely to serve the social good.", "venue": "Sci. Eng. Ethics", "year": 2020, "referenceCount": 116, "citationCount": 57, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Sociology", "Medicine"], "authors": [{"authorId": "1982425", "name": "L. Floridi"}, {"authorId": "3011486", "name": "Josh Cowls"}, {"authorId": "2057613115", "name": "T. C. King"}, {"authorId": "2084659", "name": "M. Taddeo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "812aef6b80fc34b89ae7d7d688eeea02c038c67b", "externalIds": {"DOI": "10.1215/9781478007272-001"}, "url": "https://www.semanticscholar.org/paper/812aef6b80fc34b89ae7d7d688eeea02c038c67b", "title": "Event Matrix (DoD)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "81321a4ce2c6a3da64eef26390869a4e541a18ff", "externalIds": {"DOI": "10.1215/9781478007272-007"}, "url": "https://www.semanticscholar.org/paper/81321a4ce2c6a3da64eef26390869a4e541a18ff", "title": "Autonomous Operation (DoD)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "89ab36ae8630f6e4058c926816fe8d9a676c54e3", "externalIds": {"DBLP": "conf/chi/LongM20", "MAG": "3029022390", "DOI": "10.1145/3313831.3376727"}, "url": "https://www.semanticscholar.org/paper/89ab36ae8630f6e4058c926816fe8d9a676c54e3", "title": "What is AI Literacy? Competencies and Design Considerations", "abstract": "Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.", "venue": "CHI", "year": 2020, "referenceCount": 210, "citationCount": 70, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "40413589", "name": "D. Long"}, {"authorId": "1691882", "name": "Brian Magerko"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "8b1d009f03fc79151ac0746f1689f8f1b5f9d3d5", "externalIds": {"DOI": "10.1215/9781478009276-003"}, "url": "https://www.semanticscholar.org/paper/8b1d009f03fc79151ac0746f1689f8f1b5f9d3d5", "title": "The Cloud Chambers", "abstract": null, "venue": "Cloud Ethics", "year": 2020, "referenceCount": 264, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "8f56083755549faf9a5017e6eff22315cc1f36a4", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/8f56083755549faf9a5017e6eff22315cc1f36a4", "title": "Latham, Annabel and Goltz, Sean (2019) A Survey of the General Public\u2019s Views on the Ethics of using AI in Education. In: Lecture Notes in Artificial Intelligence", "abstract": "Recent scandals arising from the use of algorithms for user profiling to further political and marketing gain have popularized the debate over the ethical and legal implications of using such \u2018artificial intelligence\u2019 in social media. The need for a legal framework to protect the general public\u2019s data is not new, yet it is not clear whether recent changes in data protection law in Europe, with the introduction of the GDPR, have highlighted the importance of privacy and led to a healthy concern from the general public over online user tracking and use of data. Like search engines, social media and online shopping platforms, intelligent tutoring systems aim to personalize learning and thus also rely on algorithms that automatically profile individual learner traits. A number of studies have been published on user perceptions of trust in robots and computer agents. Unsurprisingly, studies of AI in education have focused on efficacy, so the extent of learner awareness, and acceptance, of tracking and profiling algorithms remains unexplored. This paper discusses the ethical and legal considerations for, and presents a case study examining the general public\u2019s views of, AI in education. A survey was recently taken of attendees at a national science festival event highlighting state-of-the-art AI technologies in education. Whilst most participants (77%) were worried about the use of their data, in learning systems fewer than 8% of adults were \u2018not happy\u2019 being tracked, as opposed to nearly two-thirds (63%) of children surveyed.", "venue": "", "year": 2020, "referenceCount": 45, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "24750810", "name": "Sean Goltz"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "8f82eda5155ff395733686b121118ee92fd5bf87", "externalIds": {"DOI": "10.1215/9781478009276-006"}, "url": "https://www.semanticscholar.org/paper/8f82eda5155ff395733686b121118ee92fd5bf87", "title": "The Uncertain Author", "abstract": null, "venue": "Cloud Ethics", "year": 2020, "referenceCount": 264, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": ["They have also gained successes in applications such as robotic [6, 13, 40] and neural machine translation [14]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "9873bf5621e967bb7d2e4b77ef0ce9abf941dcd6", "externalIds": {"DBLP": "conf/kdd/WangCPSZ20", "MAG": "3080134346", "DOI": "10.1145/3394486.3406469"}, "url": "https://www.semanticscholar.org/paper/9873bf5621e967bb7d2e4b77ef0ce9abf941dcd6", "title": "Recent Advances on Graph Analytics and Its Applications in Healthcare", "abstract": "Graph is a natural representation encoding both the features of the data samples and relationships among them. Analysis with graphs is a classic topic in data mining and many techniques have been proposed in the past. In recent years, because of the rapid development of data mining and knowledge discovery, many novel graph analytics algorithms have been proposed and successfully applied in a variety of areas. The goal of this tutorial is to summarize the graph analytics algorithms developed recently and how they have been applied in healthcare. In particular, our tutorial will cover both the technical advances and the application in healthcare. On the technical aspect, we will introduce deep network embedding techniques, graph neural networks, knowledge graph construction and inference, graph generative models and graph neural ordinary differential equation models. On the healthcare side, we will introduce how these methods can be applied in predictive modeling of clinical risks (e.g., chronic disease onset, in-hospital mortality, condition exacerbation, etc.) and disease subtyping with multi-modal patient data (e.g., electronic health records, medical image and multi-omics), knowledge discovery from biomedical literature and integration with data-driven models, as well as pharmaceutical research and development (e.g., de-novo chemical compound design and optimization, patient similarity for clinical trial recruitment and pharmacovigilance). We will conclude the whole tutorial with a set of potential issues and challenges such as interpretability, fairness and security. In particular, considering the global pandemic of COVID-19, we will also summarize the existing research that have already leveraged graph analytics to help with the understanding the mechanism, transmission, treatment and prevention of COVID-19, as well as point out the available resources and potential opportunities for future research.", "venue": "KDD", "year": 2020, "referenceCount": 34, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "39586294", "name": "Fei Wang"}, {"authorId": "143738684", "name": "Peng Cui"}, {"authorId": "145525190", "name": "J. Pei"}, {"authorId": "95882703", "name": "Y. Song"}, {"authorId": "3440038", "name": "Chengxi Zang"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "9c6002f19789d2186dd31d53934c939d8f56cf81", "externalIds": {"DOI": "10.1215/9781478009276-011"}, "url": "https://www.semanticscholar.org/paper/9c6002f19789d2186dd31d53934c939d8f56cf81", "title": "Notes", "abstract": null, "venue": "Cloud Ethics", "year": 2020, "referenceCount": 264, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "a2f20490558ccf4f7871b40b5aa87d18620b0765", "externalIds": {"DOI": "10.1215/9781478009276-007"}, "url": "https://www.semanticscholar.org/paper/a2f20490558ccf4f7871b40b5aa87d18620b0765", "title": "The Madness of Algorithms", "abstract": null, "venue": "Cloud Ethics", "year": 2020, "referenceCount": 264, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [", marketing [16, 27], policy [21] and search engine results [35]) have drawn much attention towards the implications of their judgments and dependence on potentially biased training data.", "Machine learning has been introduced into domains such as health-care[10, 22], internet search[35], market pricing[16, 27], and policy [21] with the goal of reducing costs and improving accuracy in decision-making."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "a2f648e0d05f038db0c3d647312b760a062540a1", "externalIds": {"DBLP": "conf/chi/YanGLR20", "MAG": "3031923829", "DOI": "10.1145/3313831.3376447"}, "url": "https://www.semanticscholar.org/paper/a2f648e0d05f038db0c3d647312b760a062540a1", "title": "Silva: Interactively Assessing Machine Learning Fairness Using Causality", "abstract": "Machine learning models risk encoding unfairness on the part of their developers or data sources. However, assessing fairness is challenging as analysts might misidentify sources of bias, fail to notice them, or misapply metrics. In this paper we introduce Silva, a system for exploring potential sources of unfairness in datasets or machine learning models interactively. Silva directs user attention to relationships between attributes through a global causal view, provides interactive recommendations, presents intermediate results, and visualizes metrics. We describe the implementation of Silva, identify salient design and technical challenges, and provide an evaluation of the tool in comparison to an existing fairness optimization tool.", "venue": "CHI", "year": 2020, "referenceCount": 107, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2112650228", "name": "Jing Nathan Yan"}, {"authorId": "9865532", "name": "Ziwei Gu"}, {"authorId": "49955761", "name": "Hubert Lin"}, {"authorId": "2494495", "name": "Jeffrey M. Rzeszotarski"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "aa3d08febaf8c8fdeead7050d1af90bd7427ac3d", "externalIds": {"MAG": "3087415437", "DOI": "10.1002/ajim.23183", "PubMed": "32926431"}, "url": "https://www.semanticscholar.org/paper/aa3d08febaf8c8fdeead7050d1af90bd7427ac3d", "title": "Envisioning the future of work to safeguard the safety, health, and well-being of the workforce: A perspective from the CDC's National Institute for Occupational Safety and Health.", "abstract": "The future of work embodies changes to the workplace, work, and workforce, which require additional occupational safety and health (OSH) stakeholder attention. Examples include workplace developments in organizational design, technological job displacement, and work arrangements; work advances in artificial intelligence, robotics, and technologies; and workforce changes in demographics, economic security, and skills. This paper presents the Centers for Disease Control and Prevention, National Institute for Occupational Safety and Health's Future of Work Initiative; suggests an integrated approach to address worker safety, health, and well-being; introduces priority topics and subtopics that confer a framework for upcoming future of work research directions and resultant practical applications; and discusses preliminary next steps. All future of work issues impact\u00a0one another. Future of work transformations are contingent upon each of the standalone factors discussed in this paper and their combined effects. Occupational safety and health stakeholders are becoming more aware of the significance and necessity of these factors for the workplace, work, and workforce to flourish, merely survive, or disappear altogether as the future evolves. The future of work offers numerous opportunities, while also presenting critical but not clearly understood difficulties, exposures, and hazards. It is the responsibility of OSH researchers and other partners to understand the implications of future of work scenarios to translate effective interventions into practice for employers safeguarding the safety, health, and well-being of their workers.", "venue": "American journal of industrial medicine", "year": 2020, "referenceCount": 150, "citationCount": 30, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "3500336", "name": "S. Tamers"}, {"authorId": "14594834", "name": "Jessica M K Streit"}, {"authorId": "1400100595", "name": "Rene Pana-Cryan"}, {"authorId": "1398100878", "name": "Tapas K. Ray"}, {"authorId": "3772232", "name": "L. Syron"}, {"authorId": "47935732", "name": "Michael A. Flynn"}, {"authorId": "50616316", "name": "D. Castillo"}, {"authorId": "2055274014", "name": "G. Roth"}, {"authorId": "46469171", "name": "C. Geraci"}, {"authorId": "50376650", "name": "Rebecca J. Guerin"}, {"authorId": "34802638", "name": "P. Schulte"}, {"authorId": "13067963", "name": "S. Henn"}, {"authorId": "26663792", "name": "Chia-chia Chang"}, {"authorId": "3866273", "name": "S. Felknor"}, {"authorId": "50547340", "name": "J. Howard"}]}}, {"contexts": ["If this system is then put into operation and users come to rely blindly on the long-term results, it could lead to situations of sexism, racism and other forms of discrimination [13]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "abc6cb504897cdc902bf6c6b1d38be009569f8ab", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/abc6cb504897cdc902bf6c6b1d38be009569f8ab", "title": "Evaluation of the Bias in the Management of Patient\u2019s Appointments in a Pediatric Office", "abstract": "The application of Machine Learning algorithms must always take into account the objectives set within the project, the characteristics of the domain where the project will be carried out and the data available to use. Given this, it is essential before collecting data considered as representative of the problem to be solved, because otherwise there may be hidden biases in the data and these may solve a different problem from the one intended. In this context, the aim of this work is to apply a process based on the Gridding method that allows the analysis of the features of the data to be used. This process is applied to the historical data of a pediatric medical office where it is sought to implement an intelligent system that allows to predict the number of normal and overshift appointments for a particular date and time, since it is desired to hire, when necessary, another pediatric doctor to assist in the care of patients.", "venue": "", "year": 2020, "referenceCount": 51, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2081475729", "name": "Cinthia Vegega"}, {"authorId": "2089536498", "name": "Pablo Pytel"}, {"authorId": "2100194639", "name": "Mar\u00eda Florencia Pollo-Cattaneo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b6deeb7b61b50f6d6fcb5d3beb8d27de3c933785", "externalIds": {"MAG": "3030733585", "DOI": "10.11591/ijai.v9.i2.pp336-348"}, "url": "https://www.semanticscholar.org/paper/b6deeb7b61b50f6d6fcb5d3beb8d27de3c933785", "title": "Conflicting opinions in connection with digital superintelligence", "abstract": "In 1964, Nikolai Kardashev proposed the Kardashev scale, a system for measuring the extent of technological advancement of a civilization based on the magnitude of energy consumption. We are approaching an inevitable type-1 civilization, and artificial superintelligence superior to that of humans can concur with a higher-hierarchy Kardashev civilization. We aim to survey public opinions, specifically video gamers, worldwide compared to those in Poland, concerning artificial general intelligence and superintelligence. We implemented an amalgam of cross-sectional and longitudinal analyses of the database of literature and Google search engine. The geographic mapping of surface web users who are interested in artificial superintelligence revealed the top ten contributing countries: Iran, Mexico, Colombia, Brazil, India, Peru, South Africa, Romania, Switzerland, and Chile. Developing countries accounted for 54.84% of the total map. Polish people were less enthusiastic about artificial general intelligence and superintelligence compared with the rest of the world. Futuristic technological innovations imply an acceleration in artificial intelligence and superintelligence. This scenario can be pessimistic, as superintelligence can render human-based activities obsolete. However, integrating artificial intelligence with humans, via brain-computer interface technologies, can be protective. Nonetheless, legislation in connection with information technologies is mandatory to regulate upcoming digital knowledge and superintelligence.", "venue": "IAES International Journal of Artificial Intelligence (IJ-AI)", "year": 2020, "referenceCount": 126, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1422212891", "name": "A. Al-Imam"}, {"authorId": "3228097", "name": "M. Motyka"}, {"authorId": "1488658687", "name": "M. J\u0119drzejko"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "bd681d99121aa32c3155f7d2fad3279d65a01b30", "externalIds": {"DOI": "10.1215/9781478007272-003"}, "url": "https://www.semanticscholar.org/paper/bd681d99121aa32c3155f7d2fad3279d65a01b30", "title": "Centralized Control/Decentralized Execution (DoD)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "be9ec7d5a225ad13761d7387205906672d92bb74", "externalIds": {"DOI": "10.1215/9781478009276-012"}, "url": "https://www.semanticscholar.org/paper/be9ec7d5a225ad13761d7387205906672d92bb74", "title": "Bibliography", "abstract": null, "venue": "Cloud Ethics", "year": 2020, "referenceCount": 264, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "cb51ba7d983dd43a1a9fe1f0ef7002665c110c73", "externalIds": {"DOI": "10.1215/9781478009276-004"}, "url": "https://www.semanticscholar.org/paper/cb51ba7d983dd43a1a9fe1f0ef7002665c110c73", "title": "The Learning Machines", "abstract": null, "venue": "Cloud Ethics", "year": 2020, "referenceCount": 264, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "cc67a613e1556a8c230168ba81a0063e5685b984", "externalIds": {"DOI": "10.1215/9781478007272-010"}, "url": "https://www.semanticscholar.org/paper/cc67a613e1556a8c230168ba81a0063e5685b984", "title": "Unidentified Flying Objects (USAF)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "d5f0fa3fb3b9b119068a2102df197d02a4e11239", "externalIds": {"MAG": "2986836976", "DOI": "10.1016/j.actaastro.2019.11.013"}, "url": "https://www.semanticscholar.org/paper/d5f0fa3fb3b9b119068a2102df197d02a4e11239", "title": "Does artificial intelligence dream of non-terrestrial techno-signatures?", "abstract": "Abstract Today, we live in the midst of a surge in the use of artificial intelligence in many scientific and technological applications, including the Search for Extraterrestrial Intelligence (SETI). However, human perception and decision-making is still the last part of the chain in any data analysis or interpretation of results or outcomes. One of the potential applications of artificial intelligence is not only to assist in big data analysis but to help to discern possible artificiality or oddities in patterns of either radio signals, megastructures or techno-signatures in general. In this study, we review the comparative results of an experiment based on geometric patterns reconnaissance and a perception task, performed by 163 human volunteers and an artificial intelligence convolutional neural network (CNN) computer vision model. To test the model, we used an image of the famous bright spots on the Occator crater on Ceres. We wanted to investigate how the search for techno-signatures or oddities might be influenced by our cognitive skills and consciousness, and whether artificial intelligence could help or not in this task. This article also discusses how unintentional human cognitive bias might affect the search for extraterrestrial intelligence and techno-signatures compared with artificial intelligence models, and how such artificial intelligence models might perform in this type of task. We discuss how searching for unexpected, irregular features might prevent us from detecting other nearside or in-plain-sight rare and unexpected signs. The results strikingly showed that a CNN trained to detect triangles and squares scored positive hits on these two geometric shapes as some humans did.", "venue": "", "year": 2020, "referenceCount": 22, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3488710", "name": "G. G. de la Torre"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "e5f1e3d1d334779ab2396ea508df2aa8ff3ef86c", "externalIds": {"DOI": "10.1215/9781478007272-006"}, "url": "https://www.semanticscholar.org/paper/e5f1e3d1d334779ab2396ea508df2aa8ff3ef86c", "title": "Intelligence, Surveillance, and Reconnaissance (DoD)", "abstract": null, "venue": "Killer Apps", "year": 2020, "referenceCount": 376, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "e782bb6700d705d83beacfee5b7dd9b0bf246b8a", "externalIds": {"DBLP": "conf/ijcnn/CrockettOK20", "MAG": "3080599865", "DOI": "10.1109/IJCNN48605.2020.9207684"}, "url": "https://www.semanticscholar.org/paper/e782bb6700d705d83beacfee5b7dd9b0bf246b8a", "title": "Automated Deception Detection of Males and Females From Non-Verbal Facial Micro-Gestures", "abstract": "Gender bias within Artificial intelligence driven systems is currently a hot topic and is one of a number of areas where the data used to train, validate and test machine learning algorithms is under more scrutiny than ever before. In this paper we investigate if there is a difference between the nonverbal cues to deception generated by males and females through the use of an automated deception detection system. The system uses hierarchical neural networks to extract 36 channels of non-verbal head and facial behaviors whilst male and female participants are engaged in either a deceptive or truthful roleplaying task. An Image Vector dataset, comprising of 86584 vectors, is collated which uses a fixed sliding window slot of 1 second to record deceptive or truthful slots. Experiments were conducted on three variants of the dataset, all males, all females and mixed in order to examine if the differences in cues generated by males and females lead to differences in the accuracies of machine learning algorithms which classify their behavior. Results showed differences in nonverbal cues between males and females, with both genders at a disadvantage when treated by classifiers trained on both genders rather than classifiers specifically trained for each gender. However, there was no striking disadvantageous effect beyond the influence of their relative frequency of occurrence in the dataset.", "venue": "2020 International Joint Conference on Neural Networks (IJCNN)", "year": 2020, "referenceCount": 33, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Psychology", "Computer Science"], "authors": [{"authorId": "2627074", "name": "Keeley A. Crockett"}, {"authorId": "1410239467", "name": "J. O'Shea"}, {"authorId": "40173139", "name": "Wasiq Khan"}]}}, {"contexts": ["This requires more accountability from the software engineering community, governments, and public organizations [33].", "Another example involving sexisms is when it was found that women were less likely to get a Google add for a highly paid job [33]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "e9d1361f5dc24a5539a80d1ef6c7e78d77e45bea", "externalIds": {"MAG": "3080296189", "DOI": "10.1109/ICISC47916.2020.9171118"}, "url": "https://www.semanticscholar.org/paper/e9d1361f5dc24a5539a80d1ef6c7e78d77e45bea", "title": "A detailed survey of Artificial Intelligence and Spftware Engineering: Emergent Issues", "abstract": "Recently, Artificial Intelligence (AI) has become a trend and part of our typical mainstream albeit its existence ages ago. AI is at the top of the list of trending technologies and one of the hottest topics. In the coming decades, the world is going in the direction of increasing automation and cognitive technologies. This study attempts to explore and discuss the tipping points and evolution of AI. Further, it envisions the future impact that the advances of AI would have on occupations in various sectors such as technological unemployment. The study also tackles on the ethical and policy concerns involved in this upcoming AI evolution. Organizations should be able to manage both humans and robots at the same time. Software engineers\u2019 roles will be completely changed. Thus, we need to redesign education in a way that can make the future generations ready for the future that has a totally different requirement than ours. Policies should also be in place. If the AI applications are controlled by particular parties, AI will only represent that narrow and biased specific segment. The researchers should play an important role to make AI a beneficial invention rather than a risk to humanity.", "venue": "2020 Fourth International Conference on Inventive Systems and Control (ICISC)", "year": 2020, "referenceCount": 40, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "1717703561", "name": "Zainab Alkashri"}, {"authorId": "51228071", "name": "Nur Siyam"}, {"authorId": "32510983", "name": "Omar Alqaryouti"}]}}, {"contexts": ["\u2026decision making, the negative consequences of such automated technologies have recently covered the front pages of news outlets, reporting on racism and sexism (Zou and Schiebinger 2018; Crawford 2016; Leavy 2018), as well as other biases in recent AI deployments (Ellora Thadaney Israni 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "f3f8340f6139bea15b2ac533d6a9133694654959", "externalIds": {"MAG": "3085745698", "DOI": "10.1080/0144929x.2020.1818828"}, "url": "https://www.semanticscholar.org/paper/f3f8340f6139bea15b2ac533d6a9133694654959", "title": "Human-centred artificial intelligence: a contextual morality perspective", "abstract": "The emergence of big data combined with the technical developments in Artificial Intelligence has enabled novel opportunities for autonomous and continuous decision support. While initial work has ...", "venue": "", "year": 2020, "referenceCount": 115, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1470783994", "name": "Niels van Berkel"}, {"authorId": "2575168", "name": "Benjamin Tag"}, {"authorId": "144769817", "name": "Jorge Gon\u00e7alves"}, {"authorId": "3318049", "name": "S. Hosio"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "fbd379895b143811a6915d50dc9dd22f2feab388", "externalIds": {"MAG": "3008164626"}, "url": "https://www.semanticscholar.org/paper/fbd379895b143811a6915d50dc9dd22f2feab388", "title": "Utility of predictive policing: Evaluating a Spatial-Temporal Forecasting Model in an operational deployment", "abstract": "The advent of big data has fuelled a machine-learning revolution, which in turn has included the emergence of predictive policing by law enforcement agencies, although little is known about the efficacy of this strategy and the conditions under which it can be effective in reducing levels of crime. The study involved an evaluation of a predictive policing pilot project implemented by the Vancouver Police Department in British Columbia, Canada, and focused on the use of a machine-learning system designed to conduct spatial-temporal crime forecasting on residential break and enters. It was also designed to contribute to the published literature on predictive policing and to offer guidance for police services considering adoption of this technology, by providing a template that could be used to assess the strategy\u2019s effectiveness. The evaluation was conducted through the use of a pilot project that extended over 6 months from April to September 2016, during which time patrol resources were deployed to specific locations, at particular times of the day, as determined by a predictive policing model. The effectiveness of the deployments in reducing residential break and enter rates throughout this time period was compared to a control period during which police patrols were not directed by the predictive model. A multimodal approach that included the use of geo-temporal data analysis techniques to evaluate the distribution, intensity of patterns, and volume of residential break and enters during the 6-month pilot project was compared to data obtained during the previous 4 years, to determine whether directed police resources based on the predictions, had an effect. The evaluation indicated the pilot project had a quantifiable effect in 4 out of the 6 months, with inconclusive findings for the remaining. The role of big data, machine learning, and artificial intelligence in forecasting, as well as its potential benefits and concerns were also discussed, with a focus on data integrity, ethics, and the human components of predictive policing.", "venue": "", "year": 2020, "referenceCount": 274, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "118945256", "name": "Ryan Prox"}]}}, {"contexts": ["(2018); Crawford K. (2016). For algorithms reproducing gender biases, see Zhao, J."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "04c5c6dcf7919190a11e11e8e82322248f137821", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/04c5c6dcf7919190a11e11e8e82322248f137821", "title": "Data quality and artificial intelligence \u2013 mitigating bias and error to protect fundamental rights", "abstract": null, "venue": "", "year": 2019, "referenceCount": 34, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": []}}, {"contexts": ["The limitations to this study obviously are most apparent in the inherent bias that can be introduced from any crowd, a problem experienced by other companies and AI algorithms [27] already."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "11ead2f23631c8c44e14cb6c7be639c9af3c65c0", "externalIds": {"MAG": "3023344289", "DOI": "10.29024/jsim.47"}, "url": "https://www.semanticscholar.org/paper/11ead2f23631c8c44e14cb6c7be639c9af3c65c0", "title": "Diabetic Retinopathy Detection Using Collective Intelligence", "abstract": "Much attention has been focused on describing the utility of artificial intelligence (AI) applied to diabetic retinopathy data. It has been determined that there are ample opportunities for AI algorithms within medicine and that AI is even superior to what we can determine with the professional human eye. However, fewer studies actually have looked at a combined model, or rather, a collective intelligence approach of both human and computer/machine efforts. We attempt to describe and demonstrate the power of collective intelligence in the future of medicine and to offer ways to consider a more complementary approach to both humans and computers.", "venue": "Journal of Scientific Innovation in Medicine", "year": 2020, "referenceCount": 35, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "147772942", "name": "Param Bhatter"}, {"authorId": "4643784", "name": "Emily H Frisch"}, {"authorId": "51892000", "name": "Erik P. Duhaime"}, {"authorId": "2116748012", "name": "Anant Jain"}, {"authorId": "40216431", "name": "C. Fischetti"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "1a05113da179f1557c47e6e0200252796c942ac0", "externalIds": {"MAG": "2971973564", "DOI": "10.16997/wpcc.323"}, "url": "https://www.semanticscholar.org/paper/1a05113da179f1557c47e6e0200252796c942ac0", "title": "From High Visibility to High Vulnerability: Feminist, Postcolonial and Anti-Gentrification Activism at Risk", "abstract": "This editorial considers how this special issue on media and activism reflects or extends current debates in the field and how it explores the possibilities for progressive activists around the world to use the media to resist the current rise of the extreme right alongside the disturbing and growing evidence of the techniques of fascism: populism, propaganda and fake news, hate speech and hate crimes. It follows Graham Meikle (2018) in defining \u2018activism\u2019 as \u2018the widest range of attempts to effect [progressive] social or cultural change\u2019 whilst its understanding of \u2018the media\u2019 includes a broad range of communication platforms, from traditional journalism to digital networks.The issue itself looks at macro- and meso-levels of activism with this editorial explaining how contributions reflect different critical and research approaches focusing variously on media as enabling activists to organise; the mediation of activism; and media as a tool through which activists can professionally deliver their strategic objectives.It calls for measures to make digital space a safer place for activists; to help activists own their narrative without constant risks of hijacking and abuse; and to celebrate the thriving strategies and tactics that bring together activists and the public who care.", "venue": "Westminster Papers in Communication and Culture", "year": 2019, "referenceCount": 32, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "122609557", "name": "Anastasia Denisova"}, {"authorId": null, "name": "Michaela O\u2019Brien"}]}}, {"contexts": [", Google image recognition software that classified black people as gorillas [4], [5]), or something right, but in a biased way (like the \u201cbackground bias\u201d problem, causing for instance husky images to be recognised only Workshop \"From Objects to Agents\" (WOA 2019)"], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "1e345972e7625c771554c15d362b98fd2e86d8f4", "externalIds": {"DBLP": "conf/woa/CalegariCDO19", "MAG": "2965856400"}, "url": "https://www.semanticscholar.org/paper/1e345972e7625c771554c15d362b98fd2e86d8f4", "title": "Interpretable Narrative Explanation for ML Predictors with LP: A Case Study for XAI", "abstract": "In the era of digital revolution, individual lives are going to cross and interconnect ubiquitous online domains and offline reality based on smart technologies\u2014discovering, storing, processing, learning, analysing, and predicting from huge amounts of environment-collected data. Sub-symbolic techniques, such as deep learning, play a key role there, yet they are often built as black boxes, which are not inspectable, interpretable, explainable. New research efforts towards explainable artificial intelligence (XAI) are trying to address those issues, with the final purpose of building understandable, accountable, and trustable AI systems\u2014still, seemingly with a long way to go. Generally speaking, while we fully understand and appreciate the power of sub-symbolic approaches, we believe that symbolic approaches to machine intelligence, once properly combined with sub-symbolic ones, have a critical role to play in order to achieve key properties of XAI such as observability, interpretability, explainability, accountability, and trustability. In this paper we describe an example of integration of symbolic and sub-symbolic techniques. First, we sketch a general framework where symbolic and sub-symbolic approaches could fruitfully combine to produce intelligent behaviour in AI applications. Then, we focus in particular on the goal of building a narrative explanation for ML predictors: to this end, we exploit the logical knowledge obtained translating decision tree predictors into logical programs.", "venue": "WOA", "year": 2019, "referenceCount": 28, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology", "Computer Science"], "authors": [{"authorId": "2572844", "name": "Roberta Calegari"}, {"authorId": "46611901", "name": "Giovanni Ciatto"}, {"authorId": "150142258", "name": "Jason Dellaluce"}, {"authorId": "3119182", "name": "Andrea Omicini"}]}}, {"contexts": ["For example, it was reported in various media outlets that a feature within Google\u2019s photo apps, used for auto labelling photographs, was classifying images of black people as gorillas and Nikon\u2019s camera software was interpreting images of Asian people as blinking (Crawford, 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "26c3ec3bdf3c9cfb107969cef7640690b6f08693", "externalIds": {"MAG": "2943290018", "DOI": "10.1080/14606925.2019.1594979"}, "url": "https://www.semanticscholar.org/paper/26c3ec3bdf3c9cfb107969cef7640690b6f08693", "title": "Forget the Singularity, its mundane artificial intelligence that should be our immediate concern", "abstract": "Fuelled by Science Fiction and the pronouncements of Silicon Valley gurus such as Elon Musk, the \u2018Singularity\u2019 is arguably the biggest geek myth of our time and is distracting us from addressing the numerous problems emerging with the increasing use of Artificial intelligence (AI). Artificial General Intelligence (AGI) is often perceived to mean super human like intelligence such as the ones depicted in movies like Her (2013) and Ex Machina (2014). These anthropomorphic representations of AI besiege our attention away from the very real threat of biases introduced through Machine Learning (ML). In this paper we will consider whether current practices within Human-Centred Design (HCD) permit designers to consider interactions and services in which non-human algorithms play a significant role and consider how approaches inspired by Object Oriented Ontology (OOO) may offer newperspectives for framing design activities concerning AI.", "venue": "The Design Journal", "year": 2019, "referenceCount": 58, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "121401948", "name": "Franziska Pilling"}, {"authorId": "1693223", "name": "P. Coulton"}]}}, {"contexts": ["They may be hurtful practices such as those of data discrimination and inequality (Crawford, 2016; Metcalf & Crawford, 2016; Eubanks, 2018); or mainly mundane, everyday practices that appear seamless and normalized, and take place in both public and private spaces, such as skimming through a dating\u2026", "They may be hurtful practices such as those of data discrimination and inequality (Crawford, 2016; Metcalf & Crawford, 2016; Eubanks, 2018); or mainly mundane, everyday practices that appear seamless and normalized, and take place in both public and private spaces, such as skimming through a dating site like Grindr or Tinder (see Albury et al."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "274d5a8504cff673efe119e69f6477d7876f44cf", "externalIds": {"MAG": "2921176623", "DOI": "10.4324/9781351247375-17"}, "url": "https://www.semanticscholar.org/paper/274d5a8504cff673efe119e69f6477d7876f44cf", "title": "Understanding citizen data practices from a feminist perspective", "abstract": "This chapter traces the relevance of practice theory for understanding datafication from a feminist perspective. The first section shows how the practice paradigm, as developed in the social sciences and in media studies, can be applied in the study of data practices. Here it is argued that the notion of data practice incorporates a range of practices that may not be deemed or intended to be explicitly political, and thus allows us to analyse data politics and power relations in seemingly mundane, everyday settings. The chapter then introduces how the notion of \u2018care\u2019, as developed in feminist science and technology studies (de la Bellacasa, 2011), can be a productive analytical and critical approach when scrutinizing the manifestation of power relations in data practices. Approaching data power in everyday data practices as \u2018matters of care\u2019 allows us to account for their affective, embodied, and material elements, including the habitually devalued human labour of data users, activists, producers, consumers, and citizens. Outlining briefly justice (Dencik et al., 2016; Taylor 2017) and ethics approaches to data power, it is suggested that the notion of care inserts particularity and empathy in social justice frameworks. In this way the chapter maps a theoretical roadmap of feminist data studies and practice theory, which is focused on materiality and embodiment and is committed to unsettling the power relation of race, class, gender, and ability in datafied worlds. Introduction: Understanding data practices The spread of data-driven systems and technologies such as social media and tracking apps has meant that citizen media also become saturated by data. Community organizations learn to use open data for advocacy and to reflect on their own \u2018data burden\u2019 (Darkin et al., 2016). Young unemployed women are algorithmically categorized as NEET (not in education, employment or training) by a digital bureaucratic system that appears as gender free and apolitical (Thornham & Gomez 2017). Open governmental data are used for advocacy and campaigning, or are used otherwise politically for activism and social change, for example in humanitarian aid (Millan & Gutierrez, 2015; Gutierrez, 2018). At the same time, matters of privacy, sharing and access to personal data is a hot matter of legal and cultural negotiations such as the EU General Data Protection Regulation (GDPR). And feminists use hashtags to coordinate street protests, but also to evaluate cultural changes around gender equality at a global scale (Mendes et al., 2018). These are only a few examples of data-saturated citizen media practices in the last decade. These citizen media practices are complemented by data", "venue": "Citizen Media and Practice", "year": 2019, "referenceCount": 47, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science", "Sociology"], "authors": [{"authorId": "3430638", "name": "A. Fotopoulou"}]}}, {"contexts": ["\u2026evidence indicates that decisionmaking based on datafied surveillance reproduces and amplifies the structural inequalities reflected in the underlying datasets employed (Barocas and Selbst, 2016; Crawford, 2016), which could have multiple adverse impacts for data subjects long term (Katell, 2018).", "Growing evidence indicates that decisionmaking based on datafied surveillance reproduces and amplifies the structural inequalities reflected in the underlying datasets employed (Barocas and Selbst, 2016; Crawford, 2016), which could have multiple adverse impacts for data subjects long term (Katell, 2018).", "Some recent studies have begun to move towards this aim (Ananny and Crawford, 2016; Goodman, 2016; Yeung, 2017; Alkhatib and Bernstein, 2019; LaBrie and Steinke 2019; Morley et al. 2019; Reddy et al., 2019)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "29a1282e98b7e1830a77c0550a67bdda49ffee58", "externalIds": {"MAG": "2976766564", "DOI": "10.1177/2053951719868492"}, "url": "https://www.semanticscholar.org/paper/29a1282e98b7e1830a77c0550a67bdda49ffee58", "title": "Municipal surveillance regulation and algorithmic accountability", "abstract": "A wave of recent scholarship has warned about the potential for discriminatory harms of algorithmic systems, spurring an interest in algorithmic accountability and regulation. Meanwhile, parallel concerns about surveillance practices have already led to multiple successful regulatory efforts of surveillance technologies\u2014many of which have algorithmic components. Here, we examine municipal surveillance regulation as offering lessons for algorithmic oversight. Taking the 2017 Seattle Surveillance Ordinance as our primary case study and surveying efforts across five other cities, we describe the features of existing surveillance regulation; including procedures for describing surveillance technologies in detail, requirements for public engagement, and processes for establishing acceptable uses. Although the Seattle Surveillance Ordinance was not intended to address algorithmic accountability, we find these considerations to be relevant to the law\u2019s aim of surfacing disparate impacts of systems in use. We also find that in notable cases government employees did not identify regulated algorithmic surveillance technologies as reliant on algorithmic or machine learning systems, highlighting definitional gaps that could hinder future efforts toward algorithmic regulation. We argue that (i) finer-grained distinctions between types of information systems in the language of law and policy, and (ii) risk assessment tools integrated into their implementation would strengthen future regulatory efforts by rendering underlying algorithmic components more legible and accountable to political and community stakeholders.", "venue": "Big Data & Society", "year": 2019, "referenceCount": 71, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "47533677", "name": "Meg Young"}, {"authorId": "2495939", "name": "Michael A. Katell"}, {"authorId": "143782314", "name": "P. Krafft"}]}}, {"contexts": ["group exemplifies the machine learning tendency to amplify biases [76], [77]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "35b4a4cab77fb30cc3c65b1c86ee68c3ee64f812", "externalIds": {"MAG": "2804103354", "DBLP": "journals/tciaig/MawhorterSKH19", "DOI": "10.1109/TG.2018.2835776"}, "url": "https://www.semanticscholar.org/paper/35b4a4cab77fb30cc3c65b1c86ee68c3ee64f812", "title": "Identifying Regional Trends in Avatar Customization", "abstract": "Since virtual identities such as social media profiles and avatars have become a common venue for self-expression, it has become important to consider the ways in which existing systems embed the values of their designers. In order to design virtual identity systems that reflect the needs and preferences of diverse users, understanding how the virtual identity construction differs between groups is important. This paper presents a new methodology that leverages deep learning and differential clustering for comparative analysis of profile images, with a case study of almost 100 000 avatars from a large online community using a popular avatar creation platform. We use novelty discovery to segment the avatars, then cluster avatars by region to identify visual trends among low- and high-novelty avatars. We find that avatar customization correlates with increased social activity, and we are able to identify distinct visual trends among the U.S.-region and Japan-region profiles. Among these trends, realistic, idealistic, and creative self-representation can be distinguished. We observe that the realistic self-expression mirrors regional demographics, idealistic self-expression reflects shared mass-media tropes, and creative self-expression propagates within the communities.", "venue": "IEEE Transactions on Games", "year": 2019, "referenceCount": 95, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144786600", "name": "Peter A. Mawhorter"}, {"authorId": "116700977", "name": "Sercan Seng\u00fcn"}, {"authorId": "2592694", "name": "Haewoon Kwak"}, {"authorId": "144408445", "name": "D. Harrell"}]}}, {"contexts": ["Yet, if not used responsibly \u2014 in accordance with legal and ethical norms \u2014 the same technology can reinforce economic and political inequities, destabilize global markets, and reaffirm systemic bias [1,4,6,7,14,17]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "36b964453958e2d011184ef1a94050e4c5aef70e", "externalIds": {"MAG": "2974518446", "DBLP": "conf/caise/Stoyanovich19"}, "url": "https://www.semanticscholar.org/paper/36b964453958e2d011184ef1a94050e4c5aef70e", "title": "TransFAT: Translating Fairness, Accountably and Transparency into Data Science Practice", "abstract": "Data science holds incredible promise for improving peoples lives, accelerating scientific discovery and innovation, and bringing about positive societal change. Yet, if not used responsibly \u2014 in accordance with legal and ethical norms \u2014 the same technology can reinforce economic and political inequities, destabilize global markets, and reaffirm systemic bias. In this paper I discuss an ongoing regulatory effort in New York City, where the goal is to develop a methodology for enabling responsible use of algorithms and data in city agencies. I then highlight some ongoing work that makes part of the Data, Responsibly project, aiming to operationalize fairness, diversity, accountability, transparency, and data protection at all stages of the data science lifecycle. Additional information about the project, including technical papers, teaching materials, and open-source tools, is available at dataresponsibly.github.io.", "venue": "PIE@CAiSE", "year": 2019, "referenceCount": 42, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Business", "Computer Science"], "authors": [{"authorId": "1682824", "name": "Julia Stoyanovich"}]}}, {"contexts": ["Racism, sexism and discrimination may be an actual part of machine learning algorithms that can classify or make recommendations [5]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "3f760a2a58b29d3a960c95e18f3001a5b5abce01", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/3f760a2a58b29d3a960c95e18f3001a5b5abce01", "title": "Probing User Perceptions on Machine Learning", "abstract": "Machine Learning is a technology that has risen in popularity in the last decade. Designers face difficulties in working with Machine Learning as a design material. In order to help designers to cope with this material, many different approaches have been suggested, from books to insights of experienced designers with Machine Learning. In this research, the focus is on the users\u2019 perceptions on Machine Learning and how these could contribute to better design. For this purpose, 10 participants deployed probes to investigate the term Machine Learning. Probes consisted of simple tasks that provoked participants to recognize Machine Learning elements in applications they already use and were deployed with the use of their smart phones. Participants formed personalized perceptions on Machine Learning which varied from creativity in Machine Learning to preoccupations about data use. Based on these findings, suggestions to designers were proposed. Moreover, a secondary research question that emerged was the difficulties the researcher faced while working with probing on Machine Learning user experiences for the specific research. SAMMANFATTNING Maskininl\u00e4rning \u00e4r ett teknologi som har blivit popul\u00e4r det senaste decenniet. Som designer kan det vara sv\u00e5rt att jobba med maskininl\u00e4rning som ett \u201cdesignmaterial\". Olika tillv\u00e4gag\u00e5ngss\u00e4tt har f\u00f6reslagits f\u00f6r att hj\u00e4lpa designers att hantera det h\u00e4r material. I studien som presenteras h\u00e4r l\u00e4ggs fokus p\u00e5 anv\u00e4ndarens uppfattningar om maskininl\u00e4rning och hur deras f\u00f6rst\u00e5else skulle kunna bidra till b\u00e4ttre design. Tio deltagare anv\u00e4nde s\u00e5 kallade \u201cprobes\" i syfte att unders\u00f6ka hur vi m\u00f6ter maskininl\u00e4rning i vardagen. Dessa \u201cprobes\" bestod av enkla uppgifter som uppmuntrade deltagare att notera och utforska hur maskininl\u00e4rning ing\u00e5r som element i till\u00e4mpningar som de anv\u00e4nder i t ex smartphones. Deltagarna uttryckte sin personliga f\u00f6rst\u00e5else och funderingar om maskininl\u00e4rning, vilket omfattade allt fr\u00e5n kreativitet till oro kring hur personliga data anv\u00e4nds i dessa system. Baserat p\u00e5 en analys av resultaten formulerar vi r\u00e5d till hur en designer ska utforma interaktion med maskininl\u00e4rningssystem. Slutligen adderar vi en reflektion om sv\u00e5righeterna med att anv\u00e4nda probes f\u00f6r att studera maskininl\u00e4rning. Probing User Perceptions on Machine Learning Theofronia Androulakaki KTH Royal Institute of Technology Stockholm, Sweden androu@kth.se", "venue": "", "year": 2019, "referenceCount": 27, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "41125008", "name": "Theofronia Androulakaki"}, {"authorId": "2103178979", "name": "Sondera Anv\u00e4ndares"}, {"authorId": "2083727493", "name": "F\u00f6rst\u00e5else av Maskininl\u00e4rning"}, {"authorId": "41125008", "name": "Theofronia Androulakaki"}]}}, {"contexts": ["Recently, there has been an upsurge of attention given to machine learning algorithms and the practices of inequality and discrimination that are potentially being built into them (Buolamwini and Gebru 2018, Crawford 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "455626573013a77ca1b719ed06ca138594b24dbf", "externalIds": {"DBLP": "conf/aies/BryantH19", "MAG": "2956725921", "DOI": "10.1145/3306618.3314284"}, "url": "https://www.semanticscholar.org/paper/455626573013a77ca1b719ed06ca138594b24dbf", "title": "A Comparative Analysis of Emotion-Detecting AI Systems with Respect to Algorithm Performance and Dataset Diversity", "abstract": "In recent news, organizations have been considering the use of facial and emotion recognition for applications involving youth such as tackling surveillance and security in schools. However, the majority of efforts on facial emotion recognition research have focused on adults. Children, particularly in their early years, have been shown to express emotions quite differently than adults. Thus, before such algorithms are deployed in environments that impact the wellbeing and circumstance of youth, a careful examination should be made on their accuracy with respect to appropriateness for this target demographic. In this work, we utilize several datasets that contain facial expressions of children linked to their emotional state to evaluate eight different commercial emotion classification systems. We compare the ground truth labels provided by the respective datasets to the labels given with the highest confidence by the classification systems and assess the results in terms of matching score (TPR), positive predictive value, and failure to compute rate. Overall results show that the emotion recognition systems displayed subpar performance on the datasets of children's expressions compared to prior work with adult datasets and initial human ratings. We then identify limitations associated with automated recognition of emotions in children and provide suggestions on directions with enhancing recognition accuracy through data diversification, dataset accountability, and algorithmic regulation.", "venue": "AIES", "year": 2019, "referenceCount": 40, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "32306124", "name": "De'Aira G. Bryant"}, {"authorId": "145065293", "name": "A. Howard"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "46f27e4c309d1af008ae232c820a4e9a7f4defa6", "externalIds": {"MAG": "2943775761", "DOI": "10.1177/0969733019840752", "PubMed": "31032700"}, "url": "https://www.semanticscholar.org/paper/46f27e4c309d1af008ae232c820a4e9a7f4defa6", "title": "Knowledge development, technology and questions of nursing ethics", "abstract": "This article explores emerging ethical questions that result from knowledge development in a complex, technological age. Nursing practice is at a critical ideological and ethical precipice where decision-making is enhanced and burdened by new ways of knowing that include artificial intelligence, algorithms, Big Data, genetics and genomics, neuroscience, and technological innovation. On the positive side is the new understanding provided by large data sets; the quick and efficient reduction of data into useable pieces; the replacement of redundant human tasks by machines, error reduction, pattern recognition, and so forth. However, these innovations require skepticism and critique from a profession whose mission is to care for and protect patients. The promise of technology and the new biological sciences to radically and positively transform healthcare may seem compelling when couched in terms of safety, efficiency, and effectiveness but their role in the provision of ethical nursing care remains uncertain. Given the profound moral and clinical implications of how today\u2019s knowledge is developed and utilized, it is time to reconsider the relationship between ethics and knowledge development in this new uncharted area.", "venue": "Nursing ethics", "year": 2020, "referenceCount": 75, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Sociology"], "authors": [{"authorId": "3729775", "name": "A. Peirce"}, {"authorId": "114536156", "name": "Suzanne Elie"}, {"authorId": "143765696", "name": "A. George"}, {"authorId": "116107306", "name": "Mariya Gold"}, {"authorId": "1435430352", "name": "Kim O'Hara"}, {"authorId": "1435429473", "name": "Wendella Rose-Facey"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "49c58ea9d9a71913aef89d38c1994ff62c570634", "externalIds": {"MAG": "2944277342", "DOI": "10.2139/SSRN.3312874"}, "url": "https://www.semanticscholar.org/paper/49c58ea9d9a71913aef89d38c1994ff62c570634", "title": "Artificial Intelligence: American Attitudes and Trends", "abstract": "This report presents a broad look at the American public\u2019s attitudes toward artificial intelligence (AI) and AI governance, based on findings from a nationally representative survey of 2,000 American adults. As the study of the public opinion toward AI is relatively new, we aimed for breadth over depth, with our questions touching on: workplace automation; attitudes regarding international cooperation; the public\u2019s trust in various actors to develop and regulate AI; views about the importance and likely impact of different AI governance challenges; and historical and cross-national trends in public opinion regarding AI. Our results provide preliminary insights into the character of US public opinion regarding AI.", "venue": "SSRN Electronic Journal", "year": 2019, "referenceCount": 66, "citationCount": 76, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "1406217399", "name": "Baobao Zhang"}, {"authorId": "3198576", "name": "A. Dafoe"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4e5f96f0b7f4a1ff8ed8d1199079b012d0562792", "externalIds": {"MAG": "2955687484", "DOI": "10.2139/SSRN.3388669"}, "url": "https://www.semanticscholar.org/paper/4e5f96f0b7f4a1ff8ed8d1199079b012d0562792", "title": "Designing AI for Social Good: Seven Essential Factors", "abstract": "The idea of Artificial Intelligence for Social Good (henceforth AI4SG) is gaining traction within information societies in general and the AI community in particular. It has the potential to address social problems effectively through the development of AI-based solutions. Yet, to date, there is only limited understanding of what makes AI socially good in theory, what counts as AI4SG in practice, and how to reproduce its initial successes in terms of policies (Cath et al. 2018). This article addresses this gap by extrapolating seven ethical factors that are essential for future AI4SG initiatives from the analysis of 27 case studies of AI4SG projects. Some of these factors are almost entirely novel to AI, while the significance of other factors is heightened by the use of AI. From each of these factors, corresponding best practices are formulated which, subject to context and balance, may serve as preliminary guidelines to ensure that well-designed AI is more likely to serve the social good.", "venue": "SSRN Electronic Journal", "year": 2019, "referenceCount": 92, "citationCount": 35, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "3011486", "name": "Josh Cowls"}, {"authorId": "2057613115", "name": "T. C. King"}, {"authorId": "2084659", "name": "M. Taddeo"}, {"authorId": "1982425", "name": "L. Floridi"}]}}, {"contexts": ["In particular, the recent hype on ML, Deep Learning (DL), and other numeric AI methods \u2013 commonly referred as \u201cthird AI spring\u201d \u2013 has led to a situation where several decisions are delegated to subsymbolic predictors out of human control and understanding\u2014 as demonstrated by the many cases where they blatantly misbehaved [11,15,9]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "50fe6c0cb83a2f1bfd0fd625e9d461a7068b1bae", "externalIds": {"MAG": "2990849311", "DBLP": "conf/aiia/CiattoCOC19"}, "url": "https://www.semanticscholar.org/paper/50fe6c0cb83a2f1bfd0fd625e9d461a7068b1bae", "title": "Towards XMAS: eXplainability through Multi-Agent Systems", "abstract": "In the context of the Internet of Things (IoT), intelligent systems (IS) are increasingly relying on Machine Learning (ML) techniques. Given the opaqueness of most ML techniques, however, humans have to rely on their intuition to fully understand the IS outcomes: helping them is the target of eXplainable Artificial Intelligence (XAI). Current solutions \u2013 mostly too specific, and simply aimed at making ML easier to interpret \u2013 cannot satisfy the needs of IoT, characterised by heterogeneous stimuli, devices, and data-types concurring in the composition of complex information structures. Moreover, Multi-Agent Systems (MAS) achievements and advancements are most often ignored, even when they could bring about key features like explainability and trustworthiness. Accordingly, in this paper we (i) elicit and discuss the most significant issues affecting modern IS, and (ii) devise the main elements and related interconnections paving the way towards reconciling interpretable and explainable IS using MAS.", "venue": "AI&IoT@AI*IA", "year": 2019, "referenceCount": 27, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "46611901", "name": "Giovanni Ciatto"}, {"authorId": "2572844", "name": "Roberta Calegari"}, {"authorId": "3119182", "name": "Andrea Omicini"}, {"authorId": "2405073", "name": "D. Calvaresi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "53056c23709abccb979b2deadb7805514901d0a0", "externalIds": {"DBLP": "journals/corr/abs-1901-07694", "ArXiv": "1901.07694", "MAG": "3098054859", "DOI": "10.1145/3301275.3302310"}, "url": "https://www.semanticscholar.org/paper/53056c23709abccb979b2deadb7805514901d0a0", "title": "Explaining models: an empirical study of how explanations impact fairness judgment", "abstract": "Ensuring fairness of machine learning systems is a human-in-the-loop process. It relies on developers, users, and the general public to identify fairness problems and make improvements. To facilitate the process we need effective, unbiased, and user-friendly explanations that people can confidently rely on. Towards that end, we conducted an empirical study with four types of programmatically generated explanations to understand how they impact people's fairness judgments of ML systems. With an experiment involving more than 160 Mechanical Turk workers, we show that: 1) Certain explanations are considered inherently less fair, while others can enhance people's confidence in the fairness of the algorithm; 2) Different fairness problems-such as model-wide fairness issues versus case-specific fairness discrepancies-may be more effectively exposed through different styles of explanation; 3) Individual differences, including prior positions and judgment criteria of algorithmic fairness, impact how people react to different styles of explanation. We conclude with a discussion on providing personalized and adaptive explanations to support fairness judgments of ML systems.", "venue": "IUI", "year": 2019, "referenceCount": 46, "citationCount": 98, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1519478862", "name": "Jonathan Dodge"}, {"authorId": "144921048", "name": "Q. Liao"}, {"authorId": "2108127520", "name": "Yunfeng Zhang"}, {"authorId": "143725437", "name": "R. Bellamy"}, {"authorId": "2391727", "name": "Casey Dugan"}]}}, {"contexts": ["The translation of biases present in the datasets brings extremely worrying social problems present in the widespread adoption of ML technologies, as these ML\u2013constructed models of information do have an enormous social impact on the technologies they fuel [7]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "5832937c7b7951e8f4a366af32e741913bdc58e9", "externalIds": {"MAG": "2922014115", "DBLP": "conf/tei/LaurenzoV19", "DOI": "10.1145/3294109.3301262"}, "url": "https://www.semanticscholar.org/paper/5832937c7b7951e8f4a366af32e741913bdc58e9", "title": "DOOR: Ethnicity in Artificial Intelligence", "abstract": "DOOR is an artwork that aims at exposing some of the social and political impact of artificial intelligence, computer vision, and automation. The project uses a commercially available computer vision system that predicts the interactor's ethnicity, and locks or unlocks itself depending on this prediction. The artwork showcases a possible use of computer vision making explicit the fact that every technological implantation crystallises a political worldview.", "venue": "Tangible and Embedded Interaction", "year": 2019, "referenceCount": 16, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2517274", "name": "Tom\u00e1s Laurenzo"}, {"authorId": "2055319441", "name": "Katia Vega"}]}}, {"contexts": ["The\nbiases of programmers has become the subject of concern in recent years as experiments in artificial intelligence and other complex programming have shown themselves to be less than impartial (Crawford, 2016; Garcia, 2017).", "biases of programmers has become the subject of concern in recent years as experiments in artificial intelligence and other complex programming have shown themselves to be less than impartial (Crawford, 2016; Garcia, 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "5f97d57f8da2e6df2c35e47d9ab6d1bcb3b6fa95", "externalIds": {"MAG": "2991497131", "DOI": "10.1177/0263395719889563"}, "url": "https://www.semanticscholar.org/paper/5f97d57f8da2e6df2c35e47d9ab6d1bcb3b6fa95", "title": "The case for epistocratic republicanism", "abstract": "In recent years, the fortunes of democracy have waned both in theory and practice. This has added impetus not only to the republican case for strengthening democratic institutions but also to new anti-democratic thought. This article examines the claim made by Jason Brennan that epistocracy, rule by the \u2018knowledgeable\u2019, is compatible with freedom from domination. It begins by briefly explaining epistocracy and republicanism. It then presents the argument for epistocratic republicanism: that democracy can be a source of domination and that freedom from domination can be secured through non-democratic political institutions. The case against epistocratic republicanism is grounded in concerns about systemic domination and the ability of epistocrats to arbitrarily set the terms of social cooperation. These two arguments are judged on the basis of which better minimises domination while respecting its value to all people. Epistocratic republicanism is found to be less reliable because of the risks of epistemic injustice that accompanies systemic domination; democracy, accompanied by other republican institutions, is better at minimising domination and respecting persons. It concludes that republicans ought to be democrats.", "venue": "", "year": 2020, "referenceCount": 65, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "118488112", "name": "Gwilym David Blunt"}]}}, {"contexts": ["funded the technological advancement of algorithms (Crawford 2016; Thomas et  al. 2018; Watson 2016).", "Traditionally, white affluent men have developed and funded the technological advancement of algorithms (Crawford 2016; Thomas et\u00a0 al."], "isInfluential": false, "intents": ["background", "methodology"], "citingPaper": {"paperId": "66a9df2562131a87b81408b7660f9dc9ea546e73", "externalIds": {"PubMedCentral": "6868110", "MAG": "2948100573", "DOI": "10.1007/s10551-019-04204-w", "PubMed": "31814653"}, "url": "https://www.semanticscholar.org/paper/66a9df2562131a87b81408b7660f9dc9ea546e73", "title": "The Challenges of Algorithm-Based HR Decision-Making for Personal Integrity", "abstract": "Organizations increasingly rely on algorithm-based HR decision-making to monitor their employees. This trend is reinforced by the technology industry claiming that its decision-making tools are efficient and objective, downplaying their potential biases. In our manuscript, we identify an important challenge arising from the efficiency-driven logic of algorithm-based HR decision-making, namely that it may shift the delicate balance between employees\u2019 personal integrity and compliance more in the direction of compliance. We suggest that critical data literacy, ethical awareness, the use of participatory design methods, and private regulatory regimes within civil society can help overcome these challenges. Our paper contributes to literature on workplace monitoring, critical data studies, personal integrity, and literature at the intersection between HR management and corporate responsibility.", "venue": "Journal of business ethics : JBE", "year": 2019, "referenceCount": 234, "citationCount": 33, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Business"], "authors": [{"authorId": "1412830230", "name": "Ulrich Leicht-Deobald"}, {"authorId": "31561475", "name": "T. Busch"}, {"authorId": "79575035", "name": "C. Schank"}, {"authorId": "1813534", "name": "Antoinette Weibel"}, {"authorId": "102112129", "name": "Simon Schafheitle"}, {"authorId": "2060097150", "name": "Isabelle Wildhaber"}, {"authorId": "3356394", "name": "G. Kasper"}]}}, {"contexts": ["The societal impact of these algorithmic decisions have raised concerns about their fairness [2, 9], and a flurry of recent research has started to investigate how to incorporate formalized notions of fairness into machine prediction models (e."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "67ec6205a70d15ea76cc4c464bb5fc9a83f15536", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/67ec6205a70d15ea76cc4c464bb5fc9a83f15536", "title": "Learning Fair Representations by Incorporating Human Knowledge on Fairness Preethi", "abstract": "With the advent of machine learning for algorithmic decision making, recent research has pursued enhancing prediction models with various notions of fairness. This paper proposes a representation learning algorithm, called PFR, that aims to augment machine predictions by incorporating human knowledge on fairness. We investigate opportunities for obtaining such knowledge, and formalize it into a novel notion of fairness graphs. The problem of fair representation learning is then cast into a graph embedding problem that aims to jointly preserve similarity in feature spaces and similarity in fairness graphs. Comprehensive experiments with synthetic and real-life data demonstrate the practical viability of our model and its advantages over state-of-the-art baselines.", "venue": "", "year": 2019, "referenceCount": 27, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "81388497", "name": "Lahoti"}, {"authorId": "2060780087", "name": "Max"}, {"authorId": "1751591", "name": "G. Weikum"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "76747636a3682ea8ae657a236901f2f12f71c7f5", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/76747636a3682ea8ae657a236901f2f12f71c7f5", "title": "devoted to Concept Theory , Classification , Indexing and Knowledge Representation Contents", "abstract": "Knowledge organization (KO) is considered a distinctive disciplinary focus of information science, with strong connections to other intellectual domains such as philosophy, computer science, psychology, sociology, and more. Given its inherent interdisciplinarity, we ask what might a map of the physical, cultural, and intellectual geography of the KO community look like? Who is participating in this discipline\u2019s scholarly discussion, and from what locations, both geographically and intellectually? Using the unit of authorship in the journal Knowledge Organization, where is the nexus of KO activity and what patterns of authorship can be identified? Cultural characteristics were applied as a lens to explore who is and is not participating in the international conversation about KO. World Bank GNI per capita estimates were used to compare relative wealth of countries and Hofstede\u2019s Individualism dimension was identified as a way of understanding attributes of countries whose scholars are participating in this dialog. Descriptive statistics were generated through Excel, and data visualizations were rendered through Tableau Public and TagCrowd. The current project offers one method for examining an international and interdisciplinary field of study but also suggests potential for analyzing other interdisciplinary areas within the larger discipline of information science. Hauser, Elliott and Joseph T. Tennis. 2019. \u201cEpisemantics: Aboutness as Aroundness.\u201d Knowledge Organization 46 (8): 590595. 16 references. DOI:10.5771/0943-7444-2019-8-590. Abstract: Aboutness ranks amongst our field\u2019s greatest bugbears. What is a work about? How can this be known? This mirrors debates within the philosophy of language, where the concept of representation has similarly evaded satisfactory definition. This paper proposes that we abandon the strong sense of the word aboutness, which seems to promise some inherent relationship between work and subject, or, in philosophical terms, between word and world. Instead, we seek an etymological reset to the older sense of aboutness as \u201cin the vicinity, nearby; in some place or various places nearby; all over a surface.\u201d To distinguish this sense in the context of information studies, we introduce the term episemantics. The authors have each independently applied this term in slightly different contexts and scales (Hauser 2018a; Tennis 2016), and this article presents a unified definition of the term and guidelines for applying it at the scale of both words and works. The resulting weak concept of aboutness is pragmatic, in Star\u2019s sense of a focus on consequences over antecedents, while reserving space for the critique and improvement of aboutness determinations within various contexts and research programs. The paper finishes with a discussion of the implication of the concept of episemantics and methodological possibilities it offers for knowledge organization research and practice. We draw inspiration from Melvil Dewey\u2019s use of physical aroundness in his first classification system and ask how aroundness might be more effectively operationalized in digital environments. Aboutness ranks amongst our field\u2019s greatest bugbears. What is a work about? How can this be known? This mirrors debates within the philosophy of language, where the concept of representation has similarly evaded satisfactory definition. This paper proposes that we abandon the strong sense of the word aboutness, which seems to promise some inherent relationship between work and subject, or, in philosophical terms, between word and world. Instead, we seek an etymological reset to the older sense of aboutness as \u201cin the vicinity, nearby; in some place or various places nearby; all over a surface.\u201d To distinguish this sense in the context of information studies, we introduce the term episemantics. The authors have each independently applied this term in slightly different contexts and scales (Hauser 2018a; Tennis 2016), and this article presents a unified definition of the term and guidelines for applying it at the scale of both words and works. The resulting weak concept of aboutness is pragmatic, in Star\u2019s sense of a focus on consequences over antecedents, while reserving space for the critique and improvement of aboutness determinations within various contexts and research programs. The paper finishes with a discussion of the implication of the concept of episemantics and methodological possibilities it offers for knowledge organization research and practice. We draw inspiration from Melvil Dewey\u2019s use of physical aroundness in his first classification system and ask how aroundness might be more effectively operationalized in digital environments. Broughton, Vanda. 2019. \u201cThe Respective Roles of Intellectual Creativity and Automation in Representing Diversity: Human and Machine Generated Bias.\u201d Knowledge Organization 46(8): 596606. 82 references. DOI:10.5771/0943-7444-2019-8-596. Abstract: The paper traces the development of the discussion around ethical issues in artificial intelligence, and considers the way in which humans have affected the knowledge bases used in machine learning. The phenomenon of bias or discrimination in machine ethics is seen as inherited from humans, either through the use of biased data or through the semantics inherent in intellectually-built tools sourced by intelligent agents. The kind of biases observed in AI are compared with those identified in the field of knowledge organization, using religious adherents as an example of a community potentially marginalized by bias. A practical demonstration is given of apparent religious prejudice inherited from source material in a large database deployed widely in computational linguistics and automatic indexing. Methods to address the problem of bias are discussed, including the modelling of the moral process on neuroscientific understanding of brain function. The question is posed whether it is possible to model religious belief in a similar way, so that robots of the future may have both an ethical and a religious sense and themselves address the problem of prejudice. The paper traces the development of the discussion around ethical issues in artificial intelligence, and considers the way in which humans have affected the knowledge bases used in machine learning. The phenomenon of bias or discrimination in machine ethics is seen as inherited from humans, either through the use of biased data or through the semantics inherent in intellectually-built tools sourced by intelligent agents. The kind of biases observed in AI are compared with those identified in the field of knowledge organization, using religious adherents as an example of a community potentially marginalized by bias. A practical demonstration is given of apparent religious prejudice inherited from source material in a large database deployed widely in computational linguistics and automatic indexing. Methods to address the problem of bias are discussed, including the modelling of the moral process on neuroscientific understanding of brain function. The question is posed whether it is possible to model religious belief in a similar way, so that robots of the future may have both an ethical and a religious sense and themselves address the problem of prejudice. Chen, Shu-Jiun. 2019. \u201cSemantic Enrichment of Linked Personal Authority Data: A Case Study of Elites in Late Imperial China.\u201d Knowledge Organization 46(8): 607-614. 13 references. DOI:10. 5771/0943-7444-2019-8-607. Abstract: The study uses the Database of Names and Biographies (DNB) as an example to explore how in the transformation of original data into linked data, semantic enrichment can enhance engagement in digital humanities. In the preliminary results, we have defined instance-based and schema-based categories of semantic enrichment. In the instance-based category, in which enrichment occurs by enhancing the content of entities, we further determined three types, including: 1) enriching the entities by linking to diverse external resources in order to provide additional data of multiple perspectives; 2) enriching the entities with missing data, which is needed to satisfy the semantic queries; and, 3) providing the entities with access to an extended knowledge base. In the schema-based categories that enrichment occurs by enhancing the relations between the properties, we The study uses the Database of Names and Biographies (DNB) as an example to explore how in the transformation of original data into linked data, semantic enrichment can enhance engagement in digital humanities. In the preliminary results, we have defined instance-based and schema-based categories of semantic enrichment. In the instance-based category, in which enrichment occurs by enhancing the content of entities, we further determined three types, including: 1) enriching the entities by linking to diverse external resources in order to provide additional data of multiple perspectives; 2) enriching the entities with missing data, which is needed to satisfy the semantic queries; and, 3) providing the entities with access to an extended knowledge base. In the schema-based categories that enrichment occurs by enhancing the relations between the properties, we Knowl. Org. 46(2019)No.8 KO KNOWLEDGE ORGANIZATION Official Journal of the International Society for Knowledge Organization ISSN 0943 \u2013 7444 International Journal devoted to Concept Theory, Classification, Indexing and Knowledge Representation have identified two types, including: 1) enriching the properties by defining the hierarchical relations between properties; and, 2) specifying properties\u2019 domain and range for data reasoning. In addition, the study implements the LOD dataset in a digital humanities platform to demonstrate how instances and entities can be applied in the full texts where the relationship between entities are highlighted in order to bring scholars more semantic details of the texts. Clavi", "venue": "", "year": 2019, "referenceCount": 241, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "3353164", "name": "V. Broughton"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7bf0331d5465333fe27fc5f56f436799cb16b54d", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/7bf0331d5465333fe27fc5f56f436799cb16b54d", "title": "International Journal of Innovative Technology and Exploring Engineering (IJITEE)", "abstract": "3870 Published By: Blue Eyes Intelligence Engineering & Sciences Publication Retrieval Number: B7620129219/2019\u00a9BEIESP DOI: 10.35940/ijitee.B7620.129219 \uf020 Abstract: Signcryption perform both encryption and signature verification simultaneously with minimum computational time and overhead when compared to that of the traditional signature model. Certificateless Sigcryption rectifies issues corresponding to the key escrow problem and hence reducing the key management in the traditional key cryptography in Cloud environment. There has been some Certificateless Signcryption methods proposed, most of which are proved secured using the proxy pairing operations. However, with proxy pairing found to be computationally difficult in understanding and with the discrete operation reducing the advantages gained from smaller key size, data security is said to be compromised. To address this issue, in this work, a method called, Bilinear Quantum Mutual Exclusive Signcryption (BQ-MES) for data security in cloud environment is presented based on quantum principles. The new method inherits the security of bilinear mapping along with quantum, which possesses lower computation complexity than proxy operations, employed in signcrypting data in cloud environment. In the BQ-MES method, only a designated authorized cloud user recovers the data stored in the cloud via cloud service provider by verifying the validity of a signcrypted data. This is performed using Mutually Exclusive Probability model. Experimental works are conducted on the parameters such as computational time, computational overhead and data security rate. By evaluating the performance with related schemes, results show that the data stored in cloud environment is secured using BQ-MES method and computationally efficient.", "venue": "", "year": 2019, "referenceCount": 961, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1420261047", "name": "M. S. El-Feky"}, {"authorId": "116881935", "name": "A. El-Tair"}, {"authorId": "102560476", "name": "M. Kohail"}, {"authorId": "113520189", "name": "M. Serag"}]}}, {"contexts": ["The societal impact of these algorithmic decisions have raised concerns about their fairness [3, 13], and recent research has started to investigate how to incorporate formalized notions of fairness into machine prediction models (e."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "7c9cd579e2ca3f23d7d87de7241735e387ae8f06", "externalIds": {"DBLP": "journals/corr/abs-1907-01439", "MAG": "3102350826", "ArXiv": "1907.01439", "DOI": "10.14778/3372716.3372723"}, "url": "https://www.semanticscholar.org/paper/7c9cd579e2ca3f23d7d87de7241735e387ae8f06", "title": "Operationalizing Individual Fairness with Pairwise Fair Representations", "abstract": "We revisit the notion of individual fairness proposed by Dwork et al. A central challenge in operationalizing their approach is the difficulty in eliciting a human specification of a similarity metric. In this paper, we propose an operationalization of individual fairness that does not rely on a human specification of a distance metric. Instead, we propose novel approaches to elicit and leverage side-information on equally deserving individuals to counter subordination between social groups. We model this knowledge as a fairness graph, and learn a unified Pairwise Fair Representation (PFR) of the data that captures both data-driven similarity between individuals and the pairwise side-information in fairness graph. We elicit fairness judgments from a variety of sources, including human judgments for two real-world datasets on recidivism prediction (COMPAS) and violent neighborhood prediction (Crime & Communities). Our experiments show that the PFR model for operationalizing individual fairness is practically viable.", "venue": "Proc. VLDB Endow.", "year": 2019, "referenceCount": 41, "citationCount": 31, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "8777021", "name": "Preethi Lahoti"}, {"authorId": "1958921", "name": "K. Gummadi"}, {"authorId": "1751591", "name": "G. Weikum"}]}}, {"contexts": ["Lack of Diversity in Training Datasets Discussion of the lack of diversity in training datasets has for the most part centered on the lack of racial and gender diversity in facial recognition datasets, what has been called artificial intelligence\u2019s \u201cwhite guy problem\u201d [4]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "8a30fbbf541a884ca875abb3d1c10858d07cae0b", "externalIds": {"MAG": "2981514442", "DBLP": "conf/assets/Nakamura19", "DOI": "10.1145/3308561.3353812"}, "url": "https://www.semanticscholar.org/paper/8a30fbbf541a884ca875abb3d1c10858d07cae0b", "title": "My Algorithms Have Determined You're Not Human: AI-ML, Reverse Turing-Tests, and the Disability Experience", "abstract": "The past decade has seen an exponential growth in the capabilities and deployment of artificial intelligence systems based on deep neural networks. These are visible through the speech recognition and natural language processing of Alexa/Siri/Google that structure many of our everyday interactions, and the promise of SAE Level 5 autonomous driving provided by Tesla and Waze. Aside from these shiny and visible applications of AI-ML are many other uses that are more subtle: AI-ML is now being used to screen job applicants as well as determine which web ads we are shown. And while many vendors of AI-ML technologies have promised that these tools provide for greater access and freedom from human prejudice, disabled users have found that these tools can embed and deploy newer, subtler forms of discrimination against disabled people. At their worst, AI-ML systems can deny disabled people their humanity. The explosion of AI-ML technologies in the last decade has been driven by at least three factors. First, the deep neural networks algorithms that currently drive much machine learning have been improved dramatically through the use of backpropagation [1], generative adversarial nets [2], and convolution [3], allowing for their deployment across a broad variety of datasets. Second, the cost of computing hardware (especially GPUs) has dropped dramatically while large scale cloud computing facilities and widespread fiber/ broadband/4G has provided for universal availability. Finally, large datasets have come online to aid in the training of the neural nets - for example, the image datasets provided through Google and Facebook, the large natural language datasets driving Amazon Alexa, and so forth. Deep neural networks themselves have two key features or flaws, depending on the perspective. First, they are highly dependent on the diversity of the training dataset used. Second, their internal operations when deployed are entirely opaque not only to the end-user but also to the designers of the system itself.", "venue": "ASSETS", "year": 2019, "referenceCount": 7, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2110774963", "name": "Karen Nakamura"}]}}, {"contexts": ["However, also data annotated by humans are not free from bias: it has been reported, for example, that differences in gender or ethnic and social origin can produce different biases in the evaluation of the meaning of an image or of a concept (Bencke, 2016) (Crawford, 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "8ce8019a623045cbc0d3e2f68cad807c3f7b383d", "externalIds": {"MAG": "2916034493", "DOI": "10.1108/DPRG-08-2018-0049"}, "url": "https://www.semanticscholar.org/paper/8ce8019a623045cbc0d3e2f68cad807c3f7b383d", "title": "AI: from rational agents to socially responsible agents", "abstract": "\nPurpose\nThis paper aims to analyze the limitations of the mainstream definition of artificial intelligence (AI) as a rational agent, which currently drives the development of most AI systems. The authors advocate the need of a wider range of driving ethical principles for designing more socially responsible AI agents.\n\n\nDesign/methodology/approach\nThe authors follow an experience-based line of reasoning by argument to identify the limitations of the mainstream definition of AI, which is based on the concept of rational agents that select, among their designed actions, those which produce the maximum expected utility in the environment in which they operate. The problem of biases in the data used by AI is taken as example, and a small proof of concept with real datasets is provided.\n\n\nFindings\nThe authors observe that biases measurements on the datasets are sufficient to demonstrate potential risks of discriminations when using those data in AI rational agents. Starting from this example, the authors discuss other open issues connected to AI rational agents and provide a few general ethical principles derived from the White Paper AI at the service of the citizen, recently published by Agid, the agency of the Italian Government which designs and monitors the evolution of the IT systems of the Public Administration.\n\n\nOriginality/value\nThe paper contributes to the scientific debate on the governance and the ethics of AI with a critical analysis of the mainstream definition of AI.\n", "venue": "Digital Policy, Regulation and Governance", "year": 2019, "referenceCount": 53, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3326020", "name": "Antonio Vetr\u00f2"}, {"authorId": "1471635988", "name": "A. Santangelo"}, {"authorId": "144069855", "name": "E. Beretta"}, {"authorId": "9266311", "name": "J. de Martin"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "9b271d6532b54262f608ac3953586ec4e51bb891", "externalIds": {"MAG": "2965360814", "DOI": "10.2139/SSRN.3422429"}, "url": "https://www.semanticscholar.org/paper/9b271d6532b54262f608ac3953586ec4e51bb891", "title": "The Extended Corporate Mind: When Corporations Use AI to Break the Law", "abstract": "Algorithms may soon replace employees as the leading cause of corporate harm. For centuries, the law has defined corporate misconduct \u2014 anything from civil discrimination to criminal insider trading \u2014 in terms of employee misconduct. Today, however, breakthroughs in artificial intelligence and big data allow automated systems to make many corporate decisions, e.g., who gets a loan or what stocks to buy. These technologies introduce valuable efficiencies, but they do not remove (or even always reduce) the incidence of corporate harm. Unless the law adapts, corporations will become increasingly immune to civil and criminal liability as they transfer responsibility from employees to algorithms. \n \nThis Article is the first to tackle the full extent of the growing doctrinal gap left by algorithmic corporate misconduct. To hold corporations accountable, the law must sometimes treat them as if they \u201cknow\u201d information stored on their servers and \u201cintend\u201d decisions reached by their automated systems. Cognitive science and the philosophy of mind offer a path forward. The \u201cextended mind thesis\u201d complicates traditional views about the physical boundaries of the mind. The thesis states that the mind encompasses any system that sufficiently assists thought, e.g. by facilitating recall or enhancing decision-making. For natural people, the thesis implies that minds can extend beyond the brain to include external cognitive aids, like rolodexes and calculators. This Article adapts the thesis to corporate law. It motivates and proposes a doctrinal framework for extending the corporate mind to the algorithms that are increasingly integral to corporate thought. The law needs such an innovation if it is to hold future corporations to account for their most serious harms.", "venue": "SSRN Electronic Journal", "year": 2019, "referenceCount": 108, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Business"], "authors": [{"authorId": "26331275", "name": "Mihailis E. Diamantis"}]}}, {"contexts": ["Recent work on fairness in machine learning arose out of a growing concern for biases seeping into algorithmic procedures [1, 2, 4, 12, 15]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "ace26d1c17e1419775a81699134c069c0922e7e1", "externalIds": {"ArXiv": "1911.06837", "DBLP": "journals/corr/abs-1911-06837", "MAG": "2985461281"}, "url": "https://www.semanticscholar.org/paper/ace26d1c17e1419775a81699134c069c0922e7e1", "title": "Dynamic Modeling and Equilibria in Fair Decision Making", "abstract": "Recent studies on fairness in automated decision making systems have both investigated the potential future impact of these decisions on the population at large, and emphasized that imposing ''typical'' fairness constraints such as demographic parity or equality of opportunity does not guarantee a benefit to disadvantaged groups. However, these previous studies have focused on either simple one-step cost/benefit criteria, or on discrete underlying state spaces. In this work, we first propose a natural continuous representation of population state, governed by the Beta distribution, using a loan granting setting as a running example. Next, we apply a model of population dynamics under lending decisions, and show that when conditional payback probabilities are estimated correctly 1) ``optimal'' behavior by lenders can lead to ''Matthew Effect'' bifurcations (i.e., ''the rich get richer and the poor get poorer''), but that 2) many common fairness constraints on the allowable policies cause groups to converge to the same equilibrium point. Last, we contrast our results in the case of misspecified conditional probability estimates with prior work, and show that for this model, different levels of group misestimation guarantees that even fair policies lead to bifurcations. We illustrate some of the modeling conclusions on real data from credit scoring.", "venue": "ArXiv", "year": 2019, "referenceCount": 32, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "48783041", "name": "Joshua Williams"}, {"authorId": "145116464", "name": "J. Z. Kolter"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "bc997151fc23ad1cd5576b868ee15d42cdff35f6", "externalIds": {"MAG": "2985307090", "DOI": "10.7551/mitpress/11304.001.0001"}, "url": "https://www.semanticscholar.org/paper/bc997151fc23ad1cd5576b868ee15d42cdff35f6", "title": "Human Rights in the Age of Platforms", "abstract": "Scholars from across law and internet and media studies examine the human rights implications of today's platform society.Today such companies as Apple, Facebook, Google, Microsoft, and Twitter play an increasingly important role in how users form and express opinions, encounter information, debate, disagree, mobilize, and maintain their privacy. What are the human rights implications of an online domain managed by privately owned platforms? According to the Guiding Principles on Business and Human Rights, adopted by the UN Human Right Council in 2011, businesses have a responsibility to respect human rights and to carry out human rights due diligence. But this goal is dependent on the willingness of states to encode such norms into business regulations and of companies to comply. In this volume, contributors from across law and internet and media studies examine the state of human rights in today's platform society.The contributors consider the ?datafication? of society, including the economic model of data extraction and the conceptualization of privacy. They examine online advertising, content moderation, corporate storytelling around human rights, and other platform practices. Finally, they discuss the relationship between human rights law and private actors, addressing such issues as private companies' human rights responsibilities and content regulation.ContributorsAnja Bechmann, Fernando Bermejo, Agnes Callamard, Mikkel Flyverbom, Rikke Frank Jorgensen, Molly K. Land, Tarlach McGonagle, Jens-Erik Mai, Joris van Hoboken, Glen Whelan, Jillian C. York, Shoshana Zuboff, Ethan ZuckermanOpen access edition published with generous support from Knowledge Unlatched and the Danish Council for Independent Research.", "venue": "", "year": 2019, "referenceCount": 240, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "1405810924", "name": "R. J\u00f8rgensen"}]}}, {"contexts": ["First, fundamental values and ethical frames have been too complex to be formalized into a deductive decision-making system [20,21]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "c3adbf49edbbca69bf2d2d82513fb4e626fa4748", "externalIds": {"MAG": "2922510530", "DOI": "10.3390/JOITMC5010018"}, "url": "https://www.semanticscholar.org/paper/c3adbf49edbbca69bf2d2d82513fb4e626fa4748", "title": "Ethical Framework for Designing Autonomous Intelligent Systems", "abstract": "To gain the potential benefit of autonomous intelligent systems, their design and development need to be aligned with fundamental values and ethical principles. We need new design approaches, methodologies and processes to deploy ethical thought and action in the contexts of autonomous intelligent systems. To open this discussion, this article presents a review of ethical principles in the context of artificial intelligence design, and introduces an ethical framework for designing autonomous intelligent systems. The framework is based on an iterative, multidisciplinary perspective yet a systematic discussion during an Autonomous Intelligent Systems (AIS) design process, and on relevant ethical principles for the concept design of autonomous systems. We propose using scenarios as a tool to capture the essential user\u2019s or stakeholder\u2019s specific qualitative information, which is needed for a systematic analysis of ethical issues in the specific design case.", "venue": "Journal of Open Innovation: Technology, Market, and Complexity", "year": 2019, "referenceCount": 83, "citationCount": 32, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3329099", "name": "J. Leikas"}, {"authorId": "2688601", "name": "R. Koivisto"}, {"authorId": "3050292", "name": "Nadezhda Gotcheva"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "d546f3d79378404847ddca2cad9babcf9a38d530", "externalIds": {"DOI": "10.5771/9783956505508-147"}, "url": "https://www.semanticscholar.org/paper/d546f3d79378404847ddca2cad9babcf9a38d530", "title": "The Respective Roles of Intellectual Creativity and Automation in Representing Diversity: human and machine generated bias", "abstract": "The paper traces the development of the discussion around ethical issues in artificial intelligence, and considers the way in which humans have affected the knowledge bases used in machine learning. The phenomenon of bias or discrimination in machine ethics is seen as inherited from humans, either through the use of biased data or through the semantics inherent in intellectually-built tools sourced by intelligent agents. The kind of biases observed in AI are compared with those identified in the field of knowledge organization, using religious adherents as an example of a community potentially marginalized by bias. A practical demonstration is given of apparent religious prejudice inherited from source material in a large database deployed widely in computational linguistics and automatic indexing. Methods to address the problem of bias are discussed, including the modelling of the moral process on neuroscientific understanding of brain function. The question is posed whether it is possible to model religious belief in a similar way, so that robots of the future may have both an ethical and a religious sense and themselves address the problem of prejudice. Received: 18 September 2019; Revised: 12 November 2019; Accepted 14 November 2019", "venue": "The Human Position in an Artificial World: Creativity, Ethics and AI in Knowledge Organization", "year": 2019, "referenceCount": 94, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "3353164", "name": "V. Broughton"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "d68b25ba8b37aaeae19ea460e1787bdb7552564f", "externalIds": {"MAG": "2954731382", "DOI": "10.1177/0840470419843831", "PubMed": "31234654"}, "url": "https://www.semanticscholar.org/paper/d68b25ba8b37aaeae19ea460e1787bdb7552564f", "title": "Healthcare uses of artificial intelligence: Challenges and opportunities for growth", "abstract": "Forms of Artificial Intelligence (AI), like deep learning algorithms and neural networks, are being intensely explored for novel healthcare applications in areas such as imaging and diagnoses, risk analysis, lifestyle management and monitoring, health information management, and virtual health assistance. Expected benefits in these areas are wide-ranging and include increased speed in imaging, greater insight into predictive screening, and decreased healthcare costs and inefficiency. However, AI-based clinical tools also create a host of situations wherein commonly-held values and ethical principles may be challenged. In this short column, we highlight three potentially problematic aspects of AI use in healthcare: (1) dynamic information and consent, (2) transparency and ownership, and (3) privacy and discrimination. We discuss their impact on patient/client, clinician, and health institution values and suggest ways to tackle this impact. We propose that AI-related ethical challenges may represent an opportunity for growth in organizations.", "venue": "Healthcare management forum", "year": 2019, "referenceCount": 12, "citationCount": 20, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "6589083", "name": "E. Racine"}, {"authorId": "80810463", "name": "W. Boehlen"}, {"authorId": "50574592", "name": "M. Sample"}]}}, {"contexts": ["In 2015, users found that Google\u2019s photo app, which labels pictures in digital photo album, was unintentionally classifying images of black people as gorillas; Hewlett-Packard\u2019s web camera software which could not recognise people with dark skin tones; Nikon\u2019s camera software misread Asian faces as blinking (Crawford, 2016).", "\u2026photo app, which labels pictures in digital photo album, was unintentionally classifying images of black people as gorillas; Hewlett-Packard\u2019s web camera software which could not recognise people with dark skin tones; Nikon\u2019s camera software misread Asian faces as blinking (Crawford, 2016).", "However, discriminatory factors are being built into the system that underlie the technology behind many intelligent systems that shape the way we are categorised and addressed (Crawford, 2016).", "Otherwise, we risk constructing a system with ingrained forms of bias (Crawford, 2016)."], "isInfluential": true, "intents": ["background"], "citingPaper": {"paperId": "dc8661a5bab58a1018c9211198a4e0db95dca1fa", "externalIds": {"MAG": "2999142621"}, "url": "https://www.semanticscholar.org/paper/dc8661a5bab58a1018c9211198a4e0db95dca1fa", "title": "Risks of using na\u00efve approaches to Artificial Intelligence: A case study", "abstract": "In recent years, there has been a push in the Artificial intelligence (AI) field to simplify the application of machine learning so that its use can be more widely adopted. While this reduces barriers to entry for AI and machine learning, they also introduce the risk that persons or organisations with insufficient expertise will have the ability to use these systems to make decisions that have a significant impact on society based on discriminatory factors. Implementers and decision-makers need to have a good understanding of the features that the system might use and infer from to make predictions, and how these can affect their stakeholders. In this paper, we outline the risks of this phenomena occurring in a specific case \u2013 the application of machine learning applied to secondary school student grades. We demonstrate that naive approaches can have unanticipated consequences and can generate predictions based on discriminatory factors such as gender or race. The impact of the application of such flawed decisions in matters such as awards, scholarships etc. could entrench detrimental bias in the education systems.", "venue": "", "year": 2019, "referenceCount": 18, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143967850", "name": "Sunitha Prabhu"}, {"authorId": "2058073049", "name": "Riley Hunter"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "f6a13db36687d61eb3751e6d769f19a52c49cf8d", "externalIds": {"MAG": "2965997714", "DOI": "10.1177/1077699019859901"}, "url": "https://www.semanticscholar.org/paper/f6a13db36687d61eb3751e6d769f19a52c49cf8d", "title": "Artificial Intelligence and Journalism", "abstract": null, "venue": "Journalism & Mass Communication Quarterly", "year": 2019, "referenceCount": 136, "citationCount": 15, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "51967212", "name": "Meredith Broussard"}, {"authorId": "2943892", "name": "N. Diakopoulos"}, {"authorId": "2869893", "name": "Andrea L. Guzman"}, {"authorId": "5651696", "name": "Rediet Abebe"}, {"authorId": "69034455", "name": "Michel Dupagne"}, {"authorId": "2061766", "name": "C. Chuan"}]}}, {"contexts": ["Automated processing can produce negative consequences for Data Subjects as they can recreate, for example, patterns of discrimination (Crawford, 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "0f2f96b1d91b23983220496572ae2fce93ec12a7", "externalIds": {"DBLP": "conf/ukais/AddisK18", "MAG": "2899409594"}, "url": "https://www.semanticscholar.org/paper/0f2f96b1d91b23983220496572ae2fce93ec12a7", "title": "The General Data Protection Regulation (GDPR), Emerging Technologies and UK Organisations: Awareness, Implementation and Readiness", "abstract": "The GDPR will be enforceable in May 2018 and its impact is expected to be significant, both in Europe and outside Europe. To date, many UK organisations are still unaware of the new legislation, with most still focused on the first implementation stage. A high number of organisations are expected not to be GDPR compliant, and therefore potentially liable to high sanctions. This paper draws upon research on the GDPR and organisations in the UK, carried out in 2017. The research intended to explore the relation between the GDPR and emerging technologies, and the impact of the new legislations on adopters of emerging technologies. The study aimed to understand knowledge, implementation and impact of the new legislation, its relation to emerging technologies and its future in the UK, particularly considering the impact of Brexit. The research results can help to understand the current state of awareness and implementation of the new data protection legislation in the UK.", "venue": "UKAIS", "year": 2018, "referenceCount": 25, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Business"], "authors": [{"authorId": "134346699", "name": "Maria Chiara Addis"}, {"authorId": "3330181", "name": "M. Kutar"}]}}, {"contexts": ["users (Herman, 2017); (ii) the detection of biased views in a model (Crawford, 2016; Caliskan et al., 2017); (iii) the identification of situations in which the model works adequately and safely (Barocas & Selbst, 2016; Coglianese & Lehr, 2016; Friedler et al.", "\u2026purposes: (i) transparency in the model to facilitate understanding by users (Herman, 2017); (ii) the detection of biased views in a model (Crawford, 2016; Caliskan et al., 2017); (iii) the identification of situations in which the model works adequately and safely (Barocas & Selbst,\u2026"], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "1d5923618a0698946aed9d7a001a3ad15b3d0e07", "externalIds": {"MAG": "2809661092", "DBLP": "journals/corr/abs-1806-07470", "ArXiv": "1806.07470"}, "url": "https://www.semanticscholar.org/paper/1d5923618a0698946aed9d7a001a3ad15b3d0e07", "title": "Contrastive Explanations with Local Foil Trees", "abstract": "Recent advances in interpretable Machine Learning (iML) and eXplainable AI (XAI) construct explanations based on the importance of features in classification tasks. However, in a high-dimensional feature space this approach may become unfeasible without restraining the set of important features. We propose to utilize the human tendency to ask questions like \"Why this output (the fact) instead of that output (the foil)?\" to reduce the number of features to those that play a main role in the asked contrast. Our proposed method utilizes locally trained one-versus-all decision trees to identify the disjoint set of rules that causes the tree to classify data points as the foil and not as the fact. In this study we illustrate this approach on three benchmark classification tasks.", "venue": "ICML 2018", "year": 2018, "referenceCount": 39, "citationCount": 44, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "37066124", "name": "J. V. D. Waa"}, {"authorId": "8458962", "name": "M. Robeer"}, {"authorId": "1754414", "name": "J. Diggelen"}, {"authorId": "47369965", "name": "Matthieu J. S. Brinkhuis"}, {"authorId": "1784286", "name": "M. Neerincx"}]}}, {"contexts": ["\u2026of discrimination or welfare loss resulting from poorly developed algorithms have been found in policing systems and judicial institutions (Angwin et al., 2016), in commercial access to services (Crawford, 2016), as well as in differential treatment in online representation (Sweeney, 2013).", ", 2016), in commercial access to services (Crawford, 2016), as well as in differential treatment in online representation (Sweeney, 2013)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "22b370574ada6c4546c2162e0e270d5866359fc3", "externalIds": {"MAG": "2920869496"}, "url": "https://www.semanticscholar.org/paper/22b370574ada6c4546c2162e0e270d5866359fc3", "title": "The implications of algorithms in rental markets with matching mechanisms : the role of exposure, choice and efficiency in the sharing economy", "abstract": "IN ENGLISH Given the increased use and dependence of peer-to-peer markets on algorithmic optimisation processes, the problems biased algorithms may bring about for individuals have been gaining significance. With the emergence of the sharing economy, more people can be subjected to damaging allocation systems. This paper makes a first attempt to connect the notion of bias, frequently discussed in computer science, with classic problems of systematic discrimination observed in labour markets. Following the methods used in identifying such biases, the authors discuss aspects of equity and efficiency that arise from different matching mechanisms. In offering a framework for analysing the algorithms present in such markets, the authors conclude that any verdict depends on the normative stance of the reviewer, leaving the reader with a set of questions and directions for future research. ABSTRACT IN CATALAN Donat l\u2019increment de l\u2019\u00fas i la depend\u00e8ncia dels mercats peer-to-peer (mercats entre pars) de processos d\u2019optimitzaci\u00f3 algor\u00edtmica, els problemes que poden ocasionar els algoritmes esbiaixats per als individus han estat guanyant import\u00e0ncia. Amb l\u2019emerg\u00e8ncia de l\u2019economia col\u00b7laborativa, m\u00e9s gent pot estar subjecte a sistemes d\u2019assignaci\u00f3 perjudicial. Aquest article fa un primer intent de connectar la noci\u00f3 de biaix, freq\u00fcentment discutit en la ci\u00e8ncia inform\u00e0tica, amb els problemes cl\u00e0ssics de discriminaci\u00f3 sistem\u00e0tica observats als mercats de treball. Seguint els m\u00e8todes utilitzats en identificar aquests biaixos, els autors discuteixen aspectes com l\u2019equitat i l\u2019efici\u00e8ncia que sorgeixen de diferents mecanismes d\u2019emparellament. En oferir un marc per a analitzar els algoritmes presents en aquests mercats, els autors conclouen que qualsevol veredicte dep\u00e8n de la posici\u00f3 normativa del revisor, deixant el lector amb una s\u00e8rie de preguntes i direccions per a futura recerca.IN CATALAN Donat l\u2019increment de l\u2019\u00fas i la depend\u00e8ncia dels mercats peer-to-peer (mercats entre pars) de processos d\u2019optimitzaci\u00f3 algor\u00edtmica, els problemes que poden ocasionar els algoritmes esbiaixats per als individus han estat guanyant import\u00e0ncia. Amb l\u2019emerg\u00e8ncia de l\u2019economia col\u00b7laborativa, m\u00e9s gent pot estar subjecte a sistemes d\u2019assignaci\u00f3 perjudicial. Aquest article fa un primer intent de connectar la noci\u00f3 de biaix, freq\u00fcentment discutit en la ci\u00e8ncia inform\u00e0tica, amb els problemes cl\u00e0ssics de discriminaci\u00f3 sistem\u00e0tica observats als mercats de treball. Seguint els m\u00e8todes utilitzats en identificar aquests biaixos, els autors discuteixen aspectes com l\u2019equitat i l\u2019efici\u00e8ncia que sorgeixen de diferents mecanismes d\u2019emparellament. En oferir un marc per a analitzar els algoritmes presents en aquests mercats, els autors conclouen que qualsevol veredicte dep\u00e8n de la posici\u00f3 normativa del revisor, deixant el lector amb una s\u00e8rie de preguntes i direccions per a futura recerca.", "venue": "", "year": 2018, "referenceCount": 39, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Business"], "authors": [{"authorId": "123866718", "name": "Nikhil Buthada"}, {"authorId": "2092284729", "name": "Marc Mir\u00f3 i Escol\u00e0"}, {"authorId": "1403465475", "name": "Daniel M\u00fcller-Demary"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "26c3f4e7b64015e93c0a2cddc184fefdd9b4dd38", "externalIds": {"DBLP": "conf/aies/Kalyanakrishnan18", "MAG": "2910326112", "DOI": "10.1145/3278721.3278738"}, "url": "https://www.semanticscholar.org/paper/26c3f4e7b64015e93c0a2cddc184fefdd9b4dd38", "title": "Opportunities and Challenges for Artificial Intelligence in India", "abstract": "In the future of India lies the future of a sixth of the world's population. As the Artificial Intelligence (AI) revolution sweeps through societies and enters daily life, its role in shaping India's development and growth is bound to be substantial. For India, AI holds promise as a catalyst to accelerate progress, while providing mechanisms to leapfrog traditional hurdles such as poor infrastructure and bureaucracy. At the same time, an investment in AI is accompanied by risk factors with long-term implications on society: it is imperative that risks be vetted at this early stage. In this paper, we describe opportunities and challenges for AI in India. We detail opportunities that are cross-cutting (bridging India's linguistic divisions, mining public data), and also specific to one particular sector (healthcare). We list challenges that originate from existing social conditions (such as equations of caste and gender). Thereafter we distill out concrete steps and safeguards, which we believe are necessary for robust and inclusive development as India enters the AI era.", "venue": "AIES", "year": 2018, "referenceCount": 109, "citationCount": 14, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3027736", "name": "Shivaram Kalyanakrishnan"}, {"authorId": "1900467897", "name": "R. Panicker"}, {"authorId": "116023504", "name": "S. Natarajan"}, {"authorId": "121690432", "name": "Shreya Rao"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2c2fb0f151d984b95eadd5917aae5323dc2084c6", "externalIds": {"DBLP": "journals/chb/NashLDY18", "MAG": "2911621803", "DOI": "10.1016/j.chb.2017.09.018"}, "url": "https://www.semanticscholar.org/paper/2c2fb0f151d984b95eadd5917aae5323dc2084c6", "title": "The bionic blues: Robot rejection lowers self-esteem", "abstract": "Humans can fulfill their social needs with fictional and non-living entities that act as social surrogates. Though recent research demonstrates that social surrogates have beneficial effects on the individual similar to human relations, it is unclear whether surrogates can also cause similar harm to humans through social rejection. After playing a game of connect-4 with a human-sized robot, participants were informed by the robot that it would like to see them again (acceptance), would not like to see them again (rejection), or told nothing regarding a future interaction (control). Data revealed that social rejection from a robot significantly reduced self-esteem relative to receiving no-feedback and social acceptance (the latter two did not differ from each other). However, robot rejection had no impact on negative attitudes and opposition to the use of robots in everyday life. These findings demonstrate that social surrogates have the potential to cause psychological harm. Examines influence of robot rejection and acceptance on human self-esteem.Robot rejection decreased self-esteem, while acceptance had no effect on self-esteem.Demonstrates that robot rejection can cause psychological harm.", "venue": "Comput. Hum. Behav.", "year": 2018, "referenceCount": 37, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology", "Computer Science"], "authors": [{"authorId": "2062127", "name": "Kyle Nash"}, {"authorId": "38076359", "name": "J. Lea"}, {"authorId": "145619413", "name": "T. Davies"}, {"authorId": "46567605", "name": "K. Yogeeswaran"}]}}, {"contexts": ["To raise awareness and illustrate the potential for wide-ranging consequences, researchers and the press have pointed out a number of specific instances of algorithmic unfairness [19,58], for example, in predictive policing [19,43], the online housing marketplace [27,28], online ads [13,17,20,29,82], and image search results [49,64]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "2c49725003db28870e9a8aee31bae8229e5eb76d", "externalIds": {"MAG": "2795743913", "DBLP": "conf/chi/WoodruffFRW18", "DOI": "10.1145/3173574.3174230"}, "url": "https://www.semanticscholar.org/paper/2c49725003db28870e9a8aee31bae8229e5eb76d", "title": "A Qualitative Exploration of Perceptions of Algorithmic Fairness", "abstract": "Algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment, finances, and other opportunities. In some cases, algorithmic systems may be more or less favorable to certain groups or individuals, sparking substantial discussion of algorithmic fairness in public policy circles, academia, and the press. We broaden this discussion by exploring how members of potentially affected communities feel about algorithmic fairness. We conducted workshops and interviews with 44 participants from several populations traditionally marginalized by categories of race or class in the United States. While the concept of algorithmic fairness was largely unfamiliar, learning about algorithmic (un)fairness elicited negative feelings that connect to current national discussions about racial injustice and economic inequality. In addition to their concerns about potential harms to themselves and society, participants also indicated that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product.", "venue": "CHI", "year": 2018, "referenceCount": 103, "citationCount": 102, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Sociology", "Computer Science"], "authors": [{"authorId": "2064061699", "name": "Allison Woodruff"}, {"authorId": "145529969", "name": "Sarah E. Fox"}, {"authorId": "1413458031", "name": "Steven Rousso-Schindler"}, {"authorId": "51915040", "name": "J. Warshaw"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3571672f8f3b82dc90eab7d4f414788bf288b7d1", "externalIds": {"MAG": "2790704063", "DOI": "10.1080/10811680.2018.1430418"}, "url": "https://www.semanticscholar.org/paper/3571672f8f3b82dc90eab7d4f414788bf288b7d1", "title": "Silencing Bad Bots: Global, Legal and Political Questions for Mean Machine Communication", "abstract": "As digital automation expands across social contexts, the way in which legal systems respond when algorithms produce lies and hate presents a pressing policy problem. Search results, autofill suggestions, and intelligent personal assistants generate seemingly objective information for users in order to be helpful, efficient or fun but, as social technologies, can also produce prejudicial and false content. Chatbots and trending lists have made headlines for quickly being transformed from sweet to spiteful and political to inaccurate. As humans progressively engage with and rely on machine communication, the legality of algorithmically created information that harms the reputation or dignity of an individual, entity or group is a policy question posed and answered differently around the world. This article compares various defamation and hate speech laws through the lens of algorithmic content production \u2013 mean machine communication \u2013 and presents a set of outstanding issues that will require international and interdisciplinary attention.", "venue": "", "year": 2018, "referenceCount": 29, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "39990134", "name": "Meg Leta Jones"}]}}]}