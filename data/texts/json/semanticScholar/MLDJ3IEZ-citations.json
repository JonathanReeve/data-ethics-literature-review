{"offset": 0, "next": 100, "data": [{"contexts": ["This design decision is an expression of previous research finding that vendors often provide inflated claims as to the capabilities of their systems [11] and that governments also tend to foreground idealized and desirable outcomes of system use above unintended uses, deleterious uses, and misuses [21]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "0065be6d1ad6a14c3bf69c7f1443cf1be6c65705", "externalIds": {"DBLP": "conf/fat/KrafftYKLNEDHTG21", "DOI": "10.1145/3442188.3445938"}, "url": "https://www.semanticscholar.org/paper/0065be6d1ad6a14c3bf69c7f1443cf1be6c65705", "title": "An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists", "abstract": "Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.", "venue": "FAccT", "year": 2021, "referenceCount": 80, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143782314", "name": "P. Krafft"}, {"authorId": "47533677", "name": "Meg Young"}, {"authorId": "2495939", "name": "Michael A. Katell"}, {"authorId": "2108441993", "name": "Jennifer Lee"}, {"authorId": "2051526931", "name": "Shankar Narayan"}, {"authorId": "2052717083", "name": "Micah Epstein"}, {"authorId": "1845779", "name": "Dharma Dailey"}, {"authorId": "11484361", "name": "Bernease Herman"}, {"authorId": "2070778588", "name": "Aaron Tam"}, {"authorId": "120844932", "name": "Vivian Guetler"}, {"authorId": "2051528428", "name": "Corinne Bintz"}, {"authorId": "1451643902", "name": "Daniella Raz"}, {"authorId": "2051526220", "name": "P. Jobe"}, {"authorId": "2051528143", "name": "Franziska Putz"}, {"authorId": "2051526965", "name": "B. Robick"}, {"authorId": "2051526496", "name": "Bissan Barghouti"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "03c7d092c43a16695c3a5171b32be6cec33f4f36", "externalIds": {"MAG": "3195951377", "DOI": "10.15448/1984-7289.2021.2.39918"}, "url": "https://www.semanticscholar.org/paper/03c7d092c43a16695c3a5171b32be6cec33f4f36", "title": "Sobre repovoar narrativas", "abstract": "O presente estudo objetivou elaborar uma revis\u00e3o narrativa da literatura sobre influenciadores digitais e sugerir uma an\u00e1lise inspirada no campo de Science and Technology Studies. A literatura tem\u00e1tica mant\u00e9m o eixo explicativo das pesquisas centrado na figura do influenciador enquanto produtor da a\u00e7\u00e3o e usu\u00e1rio das tecnologias digitais. No entanto, argumento que \u00e9 poss\u00edvel perceber a constru\u00e7\u00e3o de influ\u00eancia nas plataformas digitais enquanto efeito de uma vasta rede de elementos heterog\u00eaneos e n\u00e3o apenas de um ou mais sujeitos. Proponho que o trabalho dos influenciadores s\u00f3 \u00e9 poss\u00edvel quando essa rede sociot\u00e9cnica \u00e9 acionada de modo que seria interessante do ponto de vista anal\u00edtico descrev\u00ea-la. As vantagens dessa abordagem sociot\u00e9cnica seriam encontrar respostas sobre como essas articula\u00e7\u00f5es acontecem e apontar para outras formas de compreender as associa\u00e7\u00f5es entre sujeitos e media\u00e7\u00f5es t\u00e9cnicas.", "venue": "Civitas - Revista de Ci\u00eancias Sociais", "year": 2021, "referenceCount": 24, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "2131929071", "name": "Sandra Stephanie Holanda Ponte Ribeiro"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0846ca4323615b9b9c39072d301318864473c104", "externalIds": {"MAG": "3137930360", "DOI": "10.1177/2378023121999581"}, "url": "https://www.semanticscholar.org/paper/0846ca4323615b9b9c39072d301318864473c104", "title": "Toward a Sociology of Artificial Intelligence: A Call for Research on Inequalities and Structural Change", "abstract": "This article outlines a research agenda for a sociology of artificial intelligence (AI). The authors review two areas in which sociological theories and methods have made significant contributions ...", "venue": "", "year": 2021, "referenceCount": 92, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "50711913", "name": "K. Joyce"}, {"authorId": "1399053359", "name": "Laurel Smith\u2010doerr"}, {"authorId": "47099210", "name": "Sharla Alegria"}, {"authorId": "35091291", "name": "S. Bell"}, {"authorId": "16202536", "name": "Taylor M. Cruz"}, {"authorId": "38636328", "name": "Steve G. Hoffman"}, {"authorId": "72273447", "name": "S. Noble"}, {"authorId": "101739970", "name": "Benjamin Shestakofsky"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0dc0420555ed47e1ca9112db7770300adb8d2423", "externalIds": {"DOI": "10.1007/978-3-030-74128-0_6"}, "url": "https://www.semanticscholar.org/paper/0dc0420555ed47e1ca9112db7770300adb8d2423", "title": "Technology in the Workplace: Opportunities and Challenges", "abstract": null, "venue": "Flexible Working Practices and Approaches", "year": 2021, "referenceCount": 93, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "8151154", "name": "Y. Griep"}, {"authorId": "9016058", "name": "I. Vranjes"}, {"authorId": "4118464", "name": "Madelon L. M. van Hooff"}, {"authorId": "153719010", "name": "Debby G. J. Beckers"}, {"authorId": "5862026", "name": "S. Geurts"}]}}, {"contexts": ["Ethnographic work has found substantially similar attitudes toward decision-making which is fully embodied in a system across disparate domains [59]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "0e32520c5061216d2730904eab1baa5a2dc4a55d", "externalIds": {"DBLP": "journals/corr/abs-2101-09385", "ArXiv": "2101.09385", "DOI": "10.1145/3442188.3445937"}, "url": "https://www.semanticscholar.org/paper/0e32520c5061216d2730904eab1baa5a2dc4a55d", "title": "Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems", "abstract": "Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.", "venue": "FAccT", "year": 2021, "referenceCount": 375, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145973578", "name": "Joshua A. Kroll"}]}}, {"contexts": ["In some cases, workers will fully utilise technology features that they perceive as being well aligned with their work context or work practices and ignore features that are not seen as being aligned with their work context or work practices (Christin, 2017).", "Workers are likely to treat technology with ignorance or resistance if it drastically changes their work, ways of working or the existing expertise required to execute a task (Christin, 2017; Chu and Robey, 2008; Lapointe and Rivard, 2005).", "Workers may also choose to \u201cloosely couple\u201d existing work practices with the new work practices to deal with the discrepancies between existing work practices and the new technology (Christin, 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "3063323037d91769d6ec61642667dd2aa1ab0b98", "externalIds": {"DBLP": "conf/ecis/EkandjoCC21"}, "url": "https://www.semanticscholar.org/paper/3063323037d91769d6ec61642667dd2aa1ab0b98", "title": "The Impact of Intelligent Personal Assistants on Work Practices", "abstract": "Individuals and organisations are increasingly turning to artificial intelligence (AI) technologies to enhance productivity and performance. This research considers the impact of intelligent personal assistants (IPAs) on work practices. IPAs are intelligent applications that automate and perform routine tasks, collaborate with workers, and offer actionable insights and recommendations to help workers make data-driven decisions. In theory, using IPAs can improve workers\u2019 productivity by supporting and/or helping to transform relevant work practices. However, empirical field-based evidence substantiating such propositions is scarce, and there is little understanding of how such transformation may occur in practice. We, therefore, propose a multiple case study to investigate how work practices change when workers collaborate with IPAs. We aim to contribute to the body of knowledge that explores the relationship between human-AI collaboration and work practices. Moreover, we expect our research to provide useful insights for organisations to better understand the implications of AI in the workplace.", "venue": "ECIS", "year": 2021, "referenceCount": 84, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "66832726", "name": "Talitakuum A. T. Ekandjo"}, {"authorId": "2042156", "name": "J. Cranefield"}, {"authorId": "3179359", "name": "Yi-Te Chiu"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3f0a07384403f7fb729b9f27869005a47e512d25", "externalIds": {"MAG": "3196841485", "DOI": "10.2966/scrip.180121.57"}, "url": "https://www.semanticscholar.org/paper/3f0a07384403f7fb729b9f27869005a47e512d25", "title": "Legal Algorithms and Solutionism: Reflections on Two Recidivism Scores", "abstract": "Algorithms have entered courts, e.g. via scores assessing recidivism. At first sight, recent applications appear to be clear cases of solutionism, i.e. attempts at fixing social problems with technological solutions. Deploying thematic analysis on assessments of two of the most prominent and widespread examples of recidivism scores, COMPAS and the PSA, casts doubt on this notion. Crucial problems \u2013 as different as \u201cfairness\u201d (COMPAS) and \u201cproper application\u201d (PSA) \u2013 are not tackled in a technological manner but rather by installing conversations. It shows that even technorationalists never see the technological solution in isolation but are actively searching for flanking social methods thereby accounting for problems that cannot be eased technologically. Furthermore, we witness social scientists called upon as active parts of such engineering.", "venue": "SCRIPT-ed", "year": 2021, "referenceCount": 21, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "2090685094", "name": "Marc M\u00f6lders"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4c2705ee5a75d6ecadbb53df07d008fd73e98ad9", "externalIds": {"MAG": "3178993634", "DOI": "10.1177/20539517211020332"}, "url": "https://www.semanticscholar.org/paper/4c2705ee5a75d6ecadbb53df07d008fd73e98ad9", "title": "Algorithmic management in a work context", "abstract": "The rapid development of machine-learning algorithms, which underpin contemporary artificial intelligence systems, has created new opportunities for the automation of work processes and management functions. While algorithmic management has been observed primarily within the platform-mediated gig economy, its transformative reach and consequences are also spreading to more standard work settings. Exploring algorithmic management as a sociotechnical concept, which reflects both technological infrastructures and organizational choices, we discuss how algorithmic management may influence existing power and social structures within organizations. We identify three key issues. First, we explore how algorithmic management shapes pre-existing power dynamics between workers and managers. Second, we discuss how algorithmic management demands new roles and competencies while also fostering oppositional attitudes toward algorithms. Third, we explain how algorithmic management impacts knowledge and information exchange within an organization, unpacking the concept of opacity on both a technical and organizational level. We conclude by situating this piece in broader discussions on the future of work, accountability, and identifying future research steps.", "venue": "Big Data & Society", "year": 2021, "referenceCount": 93, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "34891162", "name": "M. H. Jarrahi"}, {"authorId": "34633345", "name": "G. Newlands"}, {"authorId": null, "name": "Min Kyung Lee"}, {"authorId": "39440857", "name": "Christine T. Wolf"}, {"authorId": "150986327", "name": "Eliscia Kinder"}, {"authorId": "153510213", "name": "Will Sutherland"}]}}, {"contexts": ["She argues that the \u2018descriptions [managers] provide of algorithmic use often diverge from what takes place among their employees and many algorithms are either ignored or actively resisted\u2019 (Christin 2017, p. 9).", "Based on ethnographic fieldwork with web journalists and criminal justice practitioners, Christin (2017) similarly describes discrepancies between the rhetoric of employers and the everyday role of algorithms as processes of \u2018decoupling\u2019."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "5540855204dd1d53172c0c00d160cc15c3903037", "externalIds": {"MAG": "3134098883", "DOI": "10.1080/09502386.2021.1895250"}, "url": "https://www.semanticscholar.org/paper/5540855204dd1d53172c0c00d160cc15c3903037", "title": "Fissures in algorithmic power: platforms, code, and contestation", "abstract": "ABSTRACT Digital labour platforms do not attempt to build trust between worker, client, and platform on the basis of strong and durable ties. Instead, platforms utilize a double articulation of algorithmic power to govern spatially dispersed workforces in both material and discursive ways. However, algorithms do not have hegemonic outcomes, and they do not entirely strip away agency from platform workers. Through manipulation, subversion, and disruption, workers bring fissures in algorithmic power into being. Fissures in algorithmic power are moments in which algorithms do not govern as intended. While these moments do not simply result in positive outcomes for workers, they show that algorithmic power is inherently only ever partial.", "venue": "Cultural Studies", "year": 2021, "referenceCount": 112, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2004166280", "name": "Fabian Ferrari"}, {"authorId": "144643908", "name": "Mark Graham"}]}}, {"contexts": ["Conceptualizing algorithms in terms of culture resembles communicative theories in so far as algorithms may be understood to give shape to (Gilbert, 2018; Striphas, 2015) or take shape in cultural contexts (Christin, 2017; Gillespie, 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "56d6e17babbafe99815dfd61c3ccfac72ae3f802", "externalIds": {"DOI": "10.1177/20539517211026702"}, "url": "https://www.semanticscholar.org/paper/56d6e17babbafe99815dfd61c3ccfac72ae3f802", "title": "Algorithms as organizational figuration: The sociotechnical arrangements of a fintech start-up", "abstract": "Building on critical approaches that understand algorithms in terms of communication, culture and organization, this paper offers the supplementary conceptualization of algorithms as organizational figuration, defined as material and meaningful sociotechnical arrangements that develop in spatiotemporal processes and are shaped by multiple enactments of affordance\u2013agency relations. We develop this conceptualization through a case study of a Danish fintech start-up that uses machine learning to create opportunities for sustainable pensions investments. By way of ethnographic and literary methodology, we provide an in-depth analysis of the dynamic trajectory in and through which the organization gives shape to and takes shape from its key algorithmic tool, mapping the shifting sociotechnical arrangements of the start-up, from its initial search for a viable business model through the development of the algorithm to the public launch of its product. On this basis, we argue that conceptualizing algorithms as organizational figuration enables us to detail not only what algorithms do but also what they are.", "venue": "", "year": 2021, "referenceCount": 102, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "121646811", "name": "S. Dahlman"}, {"authorId": "114882058", "name": "Ib T Gulbrandsen"}, {"authorId": "101290142", "name": "S. Just"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5eff4728167d4fd007a3e50322f8530829bc92a7", "externalIds": {"MAG": "3154262560", "DOI": "10.1111/PUAR.13391"}, "url": "https://www.semanticscholar.org/paper/5eff4728167d4fd007a3e50322f8530829bc92a7", "title": "Algorithmization of Bureaucratic Organizations: Using a Practice Lens to Study How Context Shapes Predictive Policing Systems", "abstract": null, "venue": "", "year": 2021, "referenceCount": 51, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "46679956", "name": "A. Meijer"}, {"authorId": "2075636815", "name": "L. Lorenz"}, {"authorId": "32155529", "name": "M. Wessels"}]}}, {"contexts": ["While much work has studied the technological aspects of the algorithms themselves, comparatively less discussion has focused on \u201calgorithms in practice\u201d [9], and the impact of these systems on user behaviors and beliefs.", "A wide range of algorithms, from those used in the criminal justice system [9] to online market places [24] to search algorithms [30] are coming under scrutiny, and for good reason."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "5f04832fb0f77b23ae9b929a5e19b3a600497b4e", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/5f04832fb0f77b23ae9b929a5e19b3a600497b4e", "title": "An Image of Society: Gender and Racial Representation and Impact in Image Search Results for Occupations", "abstract": "Algorithmically-mediated content is both a product and producer of dominant social narratives, and it has the potential to impact users\u2019 beliefs and behaviors.We present two studies on the content and impact of gender and racial representation in image search results for common occupations. In Study 1, we compare 2020 workforce gender and racial composition to that reflected in image search. We find evidence of underrepresentation on both dimensions: women are underrepresented in search at a rate of 42% women for a field with 50% women; people of color are underrepresented with 16% in search compared to an occupation with 22% people of color (the latter being proportional to the U.S. workforce). We also compare our gender representation data with that collected in 2015 by Kay et al., finding little improvement in the last half-decade. In Study 2, we study people\u2019s impressions of occupations and sense of belonging in a given field when shown search results with different proportions of women and people of color. We find that both axes of representation as well as people\u2019s own racial and gender identities impact their experience of image search results. We conclude by emphasizing the need for designers and auditors of algorithms to consider the disparate impacts of algorithmic content on users of marginalized identities.", "venue": "", "year": 2021, "referenceCount": 56, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1393229896", "name": "D. Metaxa"}, {"authorId": "2062566733", "name": "M. Gan"}, {"authorId": "9522307", "name": "J. Landay"}]}}, {"contexts": ["While they are a critical system component, additional work is required to understand the user experience of receiving the explanations they produce [3, 15, 28]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "65ca414e97f0dfee05adc0542feb662337e9efe4", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/65ca414e97f0dfee05adc0542feb662337e9efe4", "title": "Shaping Habit Formation Insights with Shapley Values: Towards an Explainable AI-system for Self-understanding and Health Behavior Change", "abstract": "This paper presents our ongoing work to design an explainable artificial intelligence (XAI) system that helps individuals to form new healthy habits. We are developing this system on data collected from our recent observational study in which 62 participants attempted to develop a new mindful breathing habit over 6 weeks. We discuss the design and empirical results of our system, which uses Shapley values to generate explanations for predictions about user behavior, and outline how our technical approach can enable adaptive and personalized intervention tools that assist users in realizing health behavior change in the wild.", "venue": "", "year": 2021, "referenceCount": 29, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2115373399", "name": "Robert Lewis"}, {"authorId": "66797183", "name": "Massachusetts"}, {"authorId": null, "name": "YUANBO LIU"}, {"authorId": "1825791352", "name": "Matt Groh"}, {"authorId": "1719389", "name": "Rosalind W. Picard"}, {"authorId": "2115373399", "name": "Robert Lewis"}, {"authorId": null, "name": "Yuanbo Liu"}, {"authorId": "1825791352", "name": "Matt Groh"}]}}, {"contexts": ["Sociologist Ang\u00e9le Christin, for example, has recently called for more ethnographic studies on actual practices around and interactions with algorithmic technologies as she argues that the current approaches to understand algorithmic systems remain decontextualized [5, 6] and that missing from the discussion are \u201cthe actual practices, ap-"], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "690d3aba55e28035506ea538eff665a98879038a", "externalIds": {"DBLP": "conf/chi/AndersenNWKMM21", "DOI": "10.1145/3411763.3441347"}, "url": "https://www.semanticscholar.org/paper/690d3aba55e28035506ea538eff665a98879038a", "title": "Realizing AI in Healthcare: Challenges Appearing in the Wild", "abstract": "The last several years have shown a strong growth of Artificial Intelligence (AI) technologies with promising results for many areas of healthcare. HCI has contributed to these discussions, mainly with studies on explainability of advanced algorithms. However, there are only few AI-systems based on machine learning algorithms that make it to the real world and everyday care. This challenging move has been named the \u201clast mile\u201d of AI in healthcare, emphasizing the sociotechnical uncertainties and unforeseen learnings from involving users in the design or use of AI-based systems. The aim of this workshop is to set the stage for a new wave of HCI research that accounts for and begins to develop new insights, concepts, and methods, for transitioning from development to implementation and use of AI in healthcare. Participants are invited to collaboratively define an HCI research agenda focused on healthcare AI in the wild, which will require examining end-user engagements and questioning underlying concepts of AI in healthcare.", "venue": "CHI Extended Abstracts", "year": 2021, "referenceCount": 28, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "153416097", "name": "T. Andersen"}, {"authorId": "2106115106", "name": "Francisco Nunes"}, {"authorId": "2073552960", "name": "Lauren Wilcox"}, {"authorId": "3203819", "name": "Elizabeth Kaziunas"}, {"authorId": "2278772", "name": "Stina Matthiesen"}, {"authorId": "2817191", "name": "F. Magrabi"}]}}, {"contexts": ["In a study by Christin (2017) on the introduction of ADS systems in the criminal justice system and in web journalism, managers generally emphasised the effectiveness of the introduced systems and the up-to-dateness of their enterprise.", "ADS is used in more and more domains every year (e. g. human resources (Dreyer and Schulz 2019, p. 7), access to welfare and credit (O\u2019Neil 2016), policing (Bennett Moses and Chan 2018; Ferguson 2017) and sentencing (Christin 2017)), affecting decisions with profound influence on people\u2019s lives.", "A considerable amount of research has pointed out that the data used for the models underlying ADS are not objective and often merely reproduce the status quo (Barocas and Selbst 2016; Christin 2017; Eubanks 2018; O\u2019Neil 2016).", "\u2026were previously responsible for decision-making on their own, but where decisions are now made using ADS systems, decision makers often do their best to circumvent the systems, if possible (Christin 2017), or accept the decisions without questioning them if they have little say (Sch\u00e4ufele 2017).", "This kind of technology is often perceived as particularly capable of making more objective and value-neutral decisions by applying more processing power and using more data than any human being ever could, which is stipulated to be an advantage for decision-making (Christin 2017; Gillespie 2016)."], "isInfluential": true, "intents": ["methodology", "background"], "citingPaper": {"paperId": "835148c9507cbe17e41fb7c2810b1d473d9ae598", "externalIds": {"DBLP": "journals/corr/abs-2110-11037", "ArXiv": "2110.11037", "DOI": "10.1016/j.jrt.2021.100014"}, "url": "https://www.semanticscholar.org/paper/835148c9507cbe17e41fb7c2810b1d473d9ae598", "title": "\"Computer Says No\": Algorithmic Decision Support and Organisational Responsibility", "abstract": "Algorithmic decision support is increasingly used in a whole array of different contexts and structures in various areas of society, influencing many people\u2019s lives. Its use raises questions, among others, about accountability, transparency and responsibility. While there is substantial research on the issue of algorithmic systems and responsibility in general, there is little to no prior research on organisational responsibility and its attribution. Our article aims to fill that gap; we give a brief overview of the central issues connected to ADS, responsibility and decision-making in organisational contexts and identify open questions and research gaps. Furthermore, we describe a set of guidelines and a complementary digital tool to assist practitioners in mapping responsibility when introducing ADS within their organisational context.", "venue": "Journal of Responsible Technology", "year": 2021, "referenceCount": 105, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2078164015", "name": "Angelika Adensamer"}, {"authorId": "1582869438", "name": "Rita Gsenger"}, {"authorId": "3006074", "name": "Lukas Daniel Klausner"}]}}, {"contexts": ["The power of interpretation can also be seen through the multiple accounts of friction and resentment created when workers\u2019 interpretations of a technology\u2019s use don\u2019t match those of the organization [7, 16]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "844dc928874a62e41ebe3244f2738ed6f03c447b", "externalIds": {"DBLP": "conf/iui/Watkins21"}, "url": "https://www.semanticscholar.org/paper/844dc928874a62e41ebe3244f2738ed6f03c447b", "title": "The Tension Between Information Justice and Security: Perceptions of Facial Recognition Targeting", "abstract": "In the discourse on human perceptions of algorithmic fairness, researchers have begun to analyze how these perceptions are shaped by sociotechnical context. In thinking through contexts of work, a half-century of research on organizational decision-making tells us that perceptions and interpretations made within these spaces are highly bounded by surrounding contextual constraints. In this paper I report early findings from a survey I conducted to bridge these two conversations, and scrutinize real-world perceptions of algorithmic decision-making in situ in a space of work. I analyze these perceptions through the case of facial recognition (or more accurately, facial verification) as account verification in gig work. In this survey I asked 100 Uber drivers, who all had been actually subjected to Uber\u2019s facial verification process known as Real Time Check ID, their fairness perceptions of this process. I designed the survey to elicit their perceptions across five disparate dimensions of justice: informational, distributive, procedural, reciprocal, and interactional. I also asked them about their strategies for integrating Real Time Check ID into their work flow, including efforts at repair when the system breaks down and their potential preferences for subversive practices. Of those workers who report engaging in subversive tactics to avoid facial recognition, such as taking a picture of their car seat, their hand, or their passenger instead of their own face, one dimension of fairness elicited worse perceptions than any other: informational justice, a.k.a. transparency, of facial recognition targeting (the process for deciding which workers trigger this extra layer of verification). This research reveals tensions between transparency, security, and workers\u2019 perceptions of the \u201cfairness\u201d of an algorithmic system: while \u201ctoo much\u201d transparency into how workers are targeted for verification may permit bad actors to defraud the system, \u201ctoo little\u201d explanation, this research shows, is no solution either. Results have crucial implications for the allocation of transparency and the design of explanations in user-facing algorithmic fraud detection, which must address tensions between information justice and security.", "venue": "IUI Workshops", "year": 2021, "referenceCount": 43, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "29830026", "name": "E. Watkins"}]}}, {"contexts": ["Detailed ethnographic research on actual judges\u2019 interactions with risk assessment tools reveals that their appropriation of such systems is informed by routines, norms, obligations of professional identity, and their position relative to others within the organizational hierarchy [3]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "922a21245a0c52645c9f784c881f6beff3f58690", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/922a21245a0c52645c9f784c881f6beff3f58690", "title": "Crowdworkers Are Not Judges: Rethinking Crowdsourced Vignette Studies as a Risk Assessment Evaluation Technique", "abstract": "Copyright held by the owner/author(s). CHI\u201920,, April 25\u201330, 2020, Honolulu, HI, USA ACM 978-1-4503-6819-3/20/04. https://doi.org/10.1145/3334480.XXXXXXX Abstract Algorithmic risk assessments are widely deployed as judicial decision-support tools in the U.S. criminal justice system. A review of recent CS/HCI/CSCW research around algorithmic risk assessments reveals a potentially troubling trend: the use of crowdworkers as a stand-in for judges when analyzing the impact of algorithmic risk assessments. We raise three concerns about this approach to understanding algorithms in practice, and call for a reevaluation of whether human-centered AI research should rely on experimental crowdworker studies as a means to assess the impact of algorithmic risk assessments in the criminal justice system.", "venue": "", "year": 2021, "referenceCount": 12, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": null, "name": "Emma Lurie"}, {"authorId": null, "name": "Deirdre K. Mulligan"}]}}, {"contexts": ["Although the study of human-algorithm interaction is developing slowly in areas such as web journalism (Christin, 2017), forecasting (Dietvorst et al.", "Although the study of human-algorithm interaction is developing slowly in areas such as web journalism (Christin, 2017), forecasting (Dietvorst et al., 2018), and criminal justice (Green and Chen, 2019b; Grgic-Hlaca et al., 2019), research has not yet sufficiently taken into account the\u2026"], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "9756098105b2b32e3d292207e6b90fe7fe79d564", "externalIds": {"DBLP": "conf/ecis/EitlePWB21"}, "url": "https://www.semanticscholar.org/paper/9756098105b2b32e3d292207e6b90fe7fe79d564", "title": "The Impact of CV Recommender Systems on Procedural Justice in Recruiting: An Experiment in Candidate Selection", "abstract": "Due to the increasing amount of digitally available applicant information recruiters have difficulties to manage applications through manual recruiting practices. Using CV recommender systems in the selection phase supports recruiters in identifying the most suitable candidates by computing the similarity between a candidate\u2019s profile and job requirements. While recent research has mainly focused on technical improvements, we seek to gain more insights about human-algorithm interactions in recruiting. Our study aims to examine what impact the use of a CV recommender system has on procedural justice in the selection process. Through an experimental set-up with 74 recruiters from 22 multinational companies, our study shows that the incorporation of a CV recommender system helps recruiters to ensure the rule of consistency and bias suppression in the selection phase. Thus, our quantitative results indicate that CV recommender systems can have an impact on procedural justice in candidate selection.", "venue": "ECIS", "year": 2021, "referenceCount": 68, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "35873450", "name": "Verena Eitle"}, {"authorId": "1747459", "name": "F. Peters"}, {"authorId": "36062648", "name": "A. Welsch"}, {"authorId": "1688241", "name": "Peter Buxmann"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "a92cc4835e69c45a6828a4726b5f28e7761e1d3a", "externalIds": {"MAG": "3152968801", "DOI": "10.30658/HMC.2.8"}, "url": "https://www.semanticscholar.org/paper/a92cc4835e69c45a6828a4726b5f28e7761e1d3a", "title": "Negotiating Agency and Control: Theorizing Human-Machine Communication from a Structurational Perspective", "abstract": "Intelligent technologies have the potential to transform organizations and organizing processes. In particular, they are unique from prior organizational technologies in that they reposition technology as agent rather than a tool or object of use. Scholars studying human-machine communication (HMC) have begun to theorize the dual role played by human and machine agency, but they have focused primarily on the individual level. Drawing on Structuration Theory (Giddens, 1984), we propose a theoretical framework to explain agency in HMC as a process involving the negotiation of control between human and machine agents. This article contributes to HMC scholarship by offering a framework and research agenda to guide future theory-building and research on the use of intelligent technologies in organizational contexts.", "venue": "", "year": 2021, "referenceCount": 75, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "36456380", "name": "Jennifer L. Gibbs"}, {"authorId": "114377893", "name": "G. Kirkwood"}, {"authorId": "2100680548", "name": "Chengyu Fang"}, {"authorId": "113540490", "name": "J. Wilkenfeld"}]}}, {"contexts": ["lematic agendas as people selectively apply the AI\u2019s recommendations [4] or undercut its validity [21].", "For example, judges can game, critique, or undermine the AI when they dislike its recommendations [21]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "b86662a592c0649d7c1f60b4fb8bd70a1d4bae3c", "externalIds": {"ArXiv": "2106.11521", "DBLP": "journals/corr/abs-2106-11521"}, "url": "https://www.semanticscholar.org/paper/b86662a592c0649d7c1f60b4fb8bd70a1d4bae3c", "title": "ESR: Ethics and Society Review of Artificial Intelligence Research", "abstract": "Artificial intelligence (AI) research is routinely criticized for its real and potential impacts on society, and we lack adequate institutional responses to this criticism and to the responsibility that it reflects. AI research often falls outside the purview of existing feedback mechanisms such as the Institutional Review Board (IRB), which are designed to evaluate harms to human subjects rather than harms to human society. In response, we have developed the Ethics and Society Review board (ESR), a feedback panel that works with researchers to mitigate negative ethical and societal aspects of AI research. The ESR\u2019s main insight is to serve as a requirement for funding: researchers cannot receive grant funding from a major AI funding program at our university until the researchers complete the ESR process for the proposal. In this article, we describe the ESR as we have designed and run it over its first year across 41 proposals. We analyze aggregate ESR feedback on these proposals, finding that the panel most commonly identifies issues of harms to minority groups, inclusion of diverse stakeholders in the research plan, dual use, and representation in data. Surveys and interviews of researchers who interacted with the ESR found that 58% felt that it had influenced the design of their research project, 100% are willing to continue submitting future projects to the ESR, and that they sought additional scaffolding for reasoning through ethics and society issues.", "venue": "ArXiv", "year": 2021, "referenceCount": 95, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145879842", "name": "Michael S. Bernstein"}, {"authorId": "40322691", "name": "M. Levi"}, {"authorId": "2064090389", "name": "D. Magnus"}, {"authorId": "2006654928", "name": "Betsy Rajala"}, {"authorId": "2977654", "name": "Debra M. Satz"}, {"authorId": "1580578116", "name": "Charla Waeiss"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "c8e8146901f22c79026c11d63e78217875380c2c", "externalIds": {"MAG": "3173921441", "DOI": "10.12795/ietscientia.2021.i01.05"}, "url": "https://www.semanticscholar.org/paper/c8e8146901f22c79026c11d63e78217875380c2c", "title": "Challenges of the forensic science facing new technologies.", "abstract": "In the era of new technologies, used in the field of criminal justice, the forensic science has passed real challenges facing those means of gathering and administering scientific evidence in criminal proceedings. Artificial intelligence and how it meets the judiciary is a well-known question for the hightech in the field. The current paper aims at analyzing and discussing the features the judicial activity in criminal matters is characterized with during the criminal proceedings. The most important elements of new technologies come to state the consequences that they produce in criminal cases investigated by means of forensic evidence including new digital technologies. In order to achieve the proposed goal of the current paper, certain main purposes have been highlighted, which consist particularly in the procedure of using methods of forensic science for the investigation of crimes, as well as elements of new means of technologies including artificial intelligence. The proposed topic is carried out through qualitative research methods conducted on approaching challenges of the forensic science facing new technologies, combined with in-depth elements of criminal proceedings.", "venue": "IUS ET SCIENTIA", "year": 2021, "referenceCount": 19, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "104779398", "name": "Delia Magherescu"}]}}, {"contexts": ["The process of generating musters centrally has inadvertently yielded more agency to the street-level bureaucrats, who are able to hide behind the computer or \u201cfoot dragging\u201d after committing transgressions, a finding other have found in different contexts [12, 14]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "caa9e85907c065a6b27c71e55ff662952995bdef", "externalIds": {"DBLP": "journals/pacmhci/Veeraraghavan21", "DOI": "10.1145/3449285"}, "url": "https://www.semanticscholar.org/paper/caa9e85907c065a6b27c71e55ff662952995bdef", "title": "Cat and Mouse Game", "abstract": "This article uses findings from a field study of the world's largest guaranteed employment scheme (NREGA) in India to understand how digital technology mediates work relations and power dynamics within a bureaucracy. In this initiative, upper-level bureaucrats in the south Indian state of Andhra Pradesh built a digital network to remove local discretion at the \"last mile\" of an implementation of NREGA. I show how digital infrastructure affords actors at both the first and last mile opportunities to modify software to control as well as subvert certain practices. This article refers to this dialectic phenomenon as \"governance by patching\" and defines it as a socio-technical instantiation of a top-down process that focuses on small changes, iterative, and political process for positive change. Governance by patching is, therefore, neither a purely technical process nor an exclusively administrative one. Rather, it refers to the ability to fix unanticipated problems that arise in the implementation of governance programs by altering the socio-technical systems. The struggle for power continues, but on the new digital terrain.", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2021, "referenceCount": 127, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3215459", "name": "R. Veeraraghavan"}]}}, {"contexts": ["\u2026logic and biases (O\u2019Neil 2016; Eubanks 2018; Noble 2018; Benjamin 2019a), a second perspective foregrounds the \u201cloose coupling\u201d between technologies and on-the-ground implementations (Willis, Mastrofski, and Weisburd 2007; Couldry and Powell 2014; Christin 2017; Brayne and Christin, forthcoming).", "\u2026various aspects of life including health (Ruckenstein and Dow Sch\u00fcll 2017), education (Espeland and Sauder 2016; Williamson 2017), cultural consumption (Prey 2016), business (Lycett 2013), personal information (Jens-Erik 2016), and legal institutions (Brayne 2020; Christin 2017; Joh 2016)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "d681a5ae4efca8fb6b33083211e9f06b9194df88", "externalIds": {"MAG": "3184434053", "DOI": "10.1017/lsi.2021.10"}, "url": "https://www.semanticscholar.org/paper/d681a5ae4efca8fb6b33083211e9f06b9194df88", "title": "The Datafication of Law: How Technology Encodes Carceral Power and Affects Judicial Practice in the United States", "abstract": "This inquiry explores how data analyses about US Federal sentences have transformed sentencing practice beginning in the mid-1980s. I consider this inquiry an early case of the datafication of law, a pervasive process that translates legal practice into data and embeds it in digital networks so it can be tracked and analyzed in real time. To explore datafication historically and in relation to legal practice and power, I consider it not as an objective and passive undertaking but, rather, as an ideological and performative process that encodes and enacts normative presumptions and desirable futures. The empirical inquiry traverses \u201clevels of analysis\u201d and thus bridges prominent perspectives in sociolegal research. In so doing, I identify four mechanisms that mediate \u201clarge-scale\u201d processes and \u201clocal\u201d practices: field assembly, symbolic projection, material inscription, and boundaries spanning. Substantively, I show how datafication has not simply described, but also transformed, sentencing practice according to a colorblind-carceral imaginary that strives to fix the present in place. By relentlessly translating decisions into data forms that derive from this carceral imaginary, datafication affects judicial action and partakes in sustaining legacies of oppression. Yet, like other technologies, datafication also reveals dialectic dimensions in opening up to new actors and subjecting its ideological underpinnings to contestation and change.", "venue": "Law & Social Inquiry", "year": 2021, "referenceCount": 291, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "1470878445", "name": "Gil Rothschild\u2010Elyassi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "d7fbb0293d5a84ca0388922f731186174863bd69", "externalIds": {"MAG": "3141797215", "DOI": "10.1007/S10991-021-09276-1"}, "url": "https://www.semanticscholar.org/paper/d7fbb0293d5a84ca0388922f731186174863bd69", "title": "Promoting the Rights of Victims in Under-Resourced Places by Using Science and Technology That Can be Used by Ordinary People, to Deal with Human Rights Violations: Bolstering the Right to the Truth", "abstract": "This article argues that while the right to the truth has come to the fore over the last few decades, victims around the world have not really felt its practical effect. It is argued that for the right to have real impact, human rights violations need to be documented and investigated, and the victims identified. This has, however, been limited in the past for a variety of reasons, including the inability to document violations to the extent needed. The article therefore considers how scientific and technological tools can help with this. It is argued that while the right to the truth has been assisted by the advent of DNA analysis, this tool is often not available in large parts of the world because of a lack of resources. Thus, it is argued that other types of techniques can, and must, be used to identify victims of human rights abuses. The article considers how ordinary people and NGOs can use a range of other tools, including a variety of apps and social media, to collect evidence of human rights violations, find people and fight impunity. The article also discusses why there ought therefore to be a greater reliance on open-source information and how it can be used to improve documentation and investigations of human rights violations. Examples that best embody the advantages and disadvantages of these scientific and technological tools are provided, as well as ideas on how to overcome the challenges they present.", "venue": "", "year": 2021, "referenceCount": 115, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "2060534815", "name": "J. Sarkin"}]}}, {"contexts": ["Pointing in a similar direction, studies on news entrepreneurs (Klawitter and Hargittai 2018), criminal justice (Brayne and Christin 2020; Christin 2017) and journalism (Wu, Tandoc, and Salmon 2019) provide insights into the ways by which such professionals cope with algorithmic systems (see\u2026"], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "db890e7196f28673e688ce5c0bf95392c963b72a", "externalIds": {"DBLP": "journals/tis/Dogruel21", "DOI": "10.1080/01972243.2021.1949768"}, "url": "https://www.semanticscholar.org/paper/db890e7196f28673e688ce5c0bf95392c963b72a", "title": "Folk theories of algorithmic operations during Internet use: A mixed methods study", "abstract": "Abstract We used the folk theory perspective to investigate Internet users\u2019 understanding of algorithms during their Internet use. Empirically, we conducted a mixed-method study. First, we carried out semi-structured in-person interviews with 30 German Internet users. Our analysis of these interviews enabled us to identity five folk theories \u2013 economic orientation theory, personal interaction theory, popularity theory, categorization theory, and algorithmic thinking theory. In a second step, we created a standardized survey questionnaire with 19 illustrative statements for these five folk theories, relying on participants\u2019 explanations in the interviews to develop statements that reflected lay users\u2019 ideas as much as possible. Participants (N\u2009=\u2009331) were recruited through a commercial online access panel using quota criteria for age, gender, and education level to have a sample representative of the German population. Our survey findings indicate the prevalence of such folk theories among a broader population of Internet users, except for the algorithmic thinking theory, which is likely due to it being based on inaccurate assumptions about algorithms\u2019 capabilities.", "venue": "Inf. Soc.", "year": 2021, "referenceCount": 82, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2420895", "name": "Leyla Dogruel"}]}}, {"contexts": ["Most traditional news media outlets depend on the number of visitors, clicks, subscription methods, and advertisement to earn revenue [18, 72].", "Significant work has considered the unique challenges and opportunities of data journalism [12, 18, 19, 25, 31, 45, 59, 95]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "de3c379b91bb95680e773cd68609b23f85872788", "externalIds": {"DBLP": "journals/pacmhci/ShowkatB21", "DOI": "10.1145/3479534"}, "url": "https://www.semanticscholar.org/paper/de3c379b91bb95680e773cd68609b23f85872788", "title": "Where Do Stories Come From? Examining the Exploration Process in Investigative Data Journalism", "abstract": "Investigative data journalists work with a variety of data sources to tell a story. Though prior work has indicated that there is a close relationship between journalists' data work practices and that of data scientists. However, these relationships and data work practices are not empirically examined, and understanding them is crucial to inform the design of tools that are used by different groups of people including data scientists and data journalists. Thus, to bridge this gap, we studied investigative reporters' data work practices with one non-profit investigative newsroom. Our study design includes two activities: 1) semi-structured interviews with journalists, and 2) a sketching activity allowing journalists to depict examples of their work practices. By analyzing these data and synthesizing them across related prior work, we propose the major phases in the data-driven investigative journalism story idea generation process. Our study findings show that the journalists employ a collection of multiple, iterative, cyclic processes to identify journalistically \"interesting'' story ideas. These processes both significantly resemble and show subtle nuanced differences with data science work practices identified in prior research. We further verified our proposal through a member check with key informants. This work offers three primary contributions. First, it provides a close glimpse into the main phases of investigative journalists' data-driven story idea generation technique. Second, it complements prior work studying formal data science practices by examining data-driven investigative journalists, whose primary expertise lies outside computing. Third, it identifies particular points in the data exploration processes that would benefit from design interventions and suggests future research directions.", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2021, "referenceCount": 188, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2635624", "name": "Dilruba Showkat"}, {"authorId": "144392313", "name": "E. Baumer"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "e815d5c70d44f7fe49fdaa356466a9c363b0e715", "externalIds": {"DOI": "10.1075/dapsac.94.c2"}, "url": "https://www.semanticscholar.org/paper/e815d5c70d44f7fe49fdaa356466a9c363b0e715", "title": "Chapter 2. Online headline testing at a Belgian broadsheet", "abstract": null, "venue": "Participation, Engagement and Collaboration in Newsmaking", "year": 2021, "referenceCount": 54, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "70611816", "name": "Astrid Vandendaele"}, {"authorId": "38953838", "name": "J. Declercq"}, {"authorId": "35663982", "name": "Geert Jacobs"}, {"authorId": "2126895881", "name": "Sofie Verkest"}]}}, {"contexts": ["Journalists too have resisted algorithmic evaluation of their work by manipulating the variables they enter into evaluation systems so as to obtain the score that they desired (Christin, 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "e848c649c877dfb0de98232eddb1b453c72a992f", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/e848c649c877dfb0de98232eddb1b453c72a992f", "title": "Consequences for Work Organisation and Working Conditions", "abstract": "The use of software algorithms to automate organisational functions traditionally carried out by human managers has been termed \u2018algorithmic management\u2019 and identified in both platform work and conventional employment settings. Algorithmic management has been researched in greatest detail in the settings of platform work and warehousing but also noted to a lesser extent in retail, manufacturing, marketing, consultancy, banking, hotels, call centres, and among journalists, lawyers and the police. This working paper reviews industry examples from the above sectors along with more detailed case studies of platform work. Doing so enables the outlining of the main ways in which algorithms are deployed to automate workforce direction, evaluation and discipline. A new framework is presented for differentiating algorithmic management from algorithmic assistance and whether it constitutes partial, conditional, high, or full automation. The working paper also highlights some potential consequences of algorithmic management for work organisation and working conditions. In particular, the existing evidence suggests that algorithmic management may accelerate and expand precarious fissured employment relations (via outsourcing, franchising, temporary work agencies, labour brokers and digital labour platforms). It may also worsen working conditions by increasing standardisation and reducing opportunities for discretion and intrinsic skill use. Evidence from platform work and logistics highlights the danger of algorithmic management intensifying work effort, creating new sources of algorithmic insecurity and fuelling workplace resistance. Finally, the implications for policy are considered and remedies to the potential harms of algorithmic management considered.", "venue": "", "year": 2021, "referenceCount": 93, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "145231219", "name": "A. Wood"}]}}, {"contexts": ["While much work has studied the technological aspects of the algorithms themselves, comparatively less discussion has focused on \u201calgorithms in practice\u201d [9], and the impact of these systems on user behaviors and beliefs.", "A wide range of algorithms, from those used in the criminal justice system [9] to online market places [24] to search algorithms [30] are coming under scrutiny, and for good reason."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "ec81efd7b87966d66a14d7482f2a05a826763c39", "externalIds": {"DBLP": "journals/pacmhci/MetaxaGGHL21", "DOI": "10.1145/3449100"}, "url": "https://www.semanticscholar.org/paper/ec81efd7b87966d66a14d7482f2a05a826763c39", "title": "An Image of Society", "abstract": "Algorithmically-mediated content is both a product and producer of dominant social narratives, and it has the potential to impact users' beliefs and behaviors. We present two studies on the content and impact of gender and racial representation in image search results for common occupations. In Study 1, we compare 2020 workforce gender and racial composition to that reflected in image search. We find evidence of underrepresentation on both dimensions: women are underrepresented in search at a rate of 42% women for a field with 50% women; people of color are underrepresented with 16% in search compared to an occupation with 22% people of color (the latter being proportional to the U.S. workforce). We also compare our gender representation data with that collected in 2015 by Kay et al., finding little improvement in the last half-decade. In Study 2, we study people's impressions of occupations and sense of belonging in a given field when shown search results with different proportions of women and people of color. We find that both axes of representation as well as people's own racial and gender identities impact their experience of image search results. We conclude by emphasizing the need for designers and auditors of algorithms to consider the disparate impacts of algorithmic content on users of marginalized identities.", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2021, "referenceCount": 69, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1393229896", "name": "D. Metaxa"}, {"authorId": "2062566733", "name": "M. Gan"}, {"authorId": "153089934", "name": "Summer Goh"}, {"authorId": "1697703", "name": "J. Hancock"}, {"authorId": "9522307", "name": "J. Landay"}]}}, {"contexts": ["technology firms, who make claims about the capabilities of AI that are, at best, overly optimistic [6, 23, 28]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "f5c13fb738621f39a7a16b367366cae8cc71ec68", "externalIds": {"DBLP": "conf/aies/RazBGTKDHKY21", "DOI": "10.1145/3461702.3462627"}, "url": "https://www.semanticscholar.org/paper/f5c13fb738621f39a7a16b367366cae8cc71ec68", "title": "Face Mis-ID: An Interactive Pedagogical Tool Demonstrating Disparate Accuracy Rates in Facial Recognition", "abstract": "This paper reports on the making of an interactive demo to illustrate algorithmic bias in facial recognition. Facial recognition technology has been demonstrated to be more likely to misidentify women and minoritized people. This risk, among others, has elevated facial recognition into policy discussions across the country, where many jurisdictions have already passed bans on its use. Whereas scholarship on the disparate impacts of algorithmic systems is growing, general public awareness of this set of problems is limited in part by the illegibility of machine learning systems to non-specialists. Inspired by discussions with community organizers advocating for tech fairness issues, we created the Face Mis-ID Demo to reveal the algorithmic functions behind facial recognition technology and to demonstrate its risks to policymakers and members of the community. In this paper, we share the design process behind this interactive demo, its form and function, and the design decisions that honed its accessibility, toward its use for improving legibility of algorithmic systems and awareness of the sources of their disparate impacts.", "venue": "AIES", "year": 2021, "referenceCount": 41, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1451643902", "name": "Daniella Raz"}, {"authorId": "2051528428", "name": "Corinne Bintz"}, {"authorId": "120844932", "name": "Vivian Guetler"}, {"authorId": "2070778588", "name": "Aaron Tam"}, {"authorId": "2495939", "name": "Michael A. Katell"}, {"authorId": "1845779", "name": "Dharma Dailey"}, {"authorId": "11484361", "name": "Bernease Herman"}, {"authorId": "143782314", "name": "P. Krafft"}, {"authorId": "47533677", "name": "Meg Young"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "f66fad34413cde38d804bb1d9de0c3779b8481f3", "externalIds": {"DOI": "10.3384/vs.2001-5992.2021.8.1.33-66"}, "url": "https://www.semanticscholar.org/paper/f66fad34413cde38d804bb1d9de0c3779b8481f3", "title": "Valuation Constellations", "abstract": "The focus on situated practices in current valuation studies becomes an obstacle when situations are too narrowly defined, when moments of valuation are treated as isolated events and especially when the interconnectedness of moments across situations and social fields is neglected. In order to overcome these limitations, we propose the concept of valuation constellations (Meier et al. 2016). Based on the literature on valuation the concept distinguishes positions and their relations, rules, and infrastructures. We present these three components of constellations and demonstrate the potential of the concept regarding three analytical puzzles of valuation analysis: historical change of valuation processes, the definition and solution of valuation problems, and the legitimacy of valuations. Each of the puzzles is illustrated with an empirical case, i.e. dating platforms and apps, higher education, and amateur reviewing. Going beyond situationalism, the valuation constellations perspective is key to understanding interconnected valuation processes.", "venue": "", "year": 2021, "referenceCount": 146, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "117670888", "name": "D\u00e9sir\u00e9e Waibel"}, {"authorId": "2066663790", "name": "Thorsten Peetz"}, {"authorId": "143983018", "name": "F. Meier"}]}}, {"contexts": ["In many of these contexts, these works prompt us to think about ways that we can support that subversive act [22]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "fa2607ea71f2f30714471cd535b9274cb2edff7e", "externalIds": {"DBLP": "conf/chi/Alkhatib21", "DOI": "10.1145/3411764.3445740"}, "url": "https://www.semanticscholar.org/paper/fa2607ea71f2f30714471cd535b9274cb2edff7e", "title": "To Live in Their Utopia: Why Algorithmic Systems Create Absurd Outcomes", "abstract": "The promise AI\u2019s proponents have made for decades is one in which our needs are predicted, anticipated, and met - often before we even realize it. Instead, algorithmic systems, particularly AIs trained on large datasets and deployed to massive scales, seem to keep making the wrong decisions, causing harm and rewarding absurd outcomes. Attempts to make sense of why AIs make wrong calls in the moment explain the instances of errors, but how the environment surrounding these systems precipitate those instances remains murky. This paper draws from anthropological work on bureaucracies, states, and power, translating these ideas into a theory describing the structural tendency for powerful algorithmic systems to cause tremendous harm. I show how administrative models and projections of the world create marginalization, just as algorithmic models cause representational and allocative harm. This paper concludes with a recommendation to avoid the absurdity algorithmic systems produce by denying them power.", "venue": "CHI", "year": 2021, "referenceCount": 94, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3153970", "name": "A. Alkhatib"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0308489bc170419bcfbdf778d4603f8695de911a", "externalIds": {"DBLP": "conf/sigcpr/EisserTB20", "MAG": "3033479879", "DOI": "10.1145/3378539.3393866"}, "url": "https://www.semanticscholar.org/paper/0308489bc170419bcfbdf778d4603f8695de911a", "title": "Automation Anxiety as a Barrier to Workplace Automation: An Empirical Analysis of the Example of Recruiting Chatbots in Germany", "abstract": "Workplace automation substantially changes the overall working environment. Through artificial intelligence and robotics, more and more complex tasks can be automated. Various jobs could undergo significant changes or even cease to exist due to these technological advancements. One area affected by these developments is the recruitment in enterprises. Chatbot systems have the potential to automate the communication processes within recruiting. This automation could cause automation anxiety perceived by recruiters due to their job being at risk of automation. Within this study, the influence of automation anxiety perceived by recruiters on the acceptance of chatbot technology is assessed. Methodologically, an adaptation of the Technology Acceptance Model (TAM) is proposed for the assessment of recruiting chatbot acceptance by exchanging the original construct computer anxiety with the more focused construct automation anxiety (AANX). The completed pre-study as preparatory work for a more comprehensive recruiting acceptance study presented in this paper resulted in 83 participants. As a preliminary result, AANX is supported as a construct significantly influencing recruiters' perceived level of chatbot acceptance through its positive relationships with job relevance and perceived usefulness. However, recruiters seem to ostensibly focus on the opportunities that chatbot systems offer, which are perceived as useful and positive and only sporadically on potential concerns of this technology.", "venue": "SIGMIS-CPR", "year": 2020, "referenceCount": 42, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "116829205", "name": "Judith Ei\u00dfer"}, {"authorId": "1742032227", "name": "Mario Torrini"}, {"authorId": "152510602", "name": "Stephan B\u00f6hm"}]}}, {"contexts": ["\u2026et al., 2016), journalism (Anderson, 2013; D\u00f6rr, 2016), finance (Lenglet, 2011; Pasquale, 2015), security (Amoore, 2009) and juridical systems (Christin, 2017), social scientists have documented how algorithms are restructuring the ways in which key democratic institutions and organizations\u2026"], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "032df5e7407832f9466a2ba299105505dcd38cb4", "externalIds": {"MAG": "3010781206", "DOI": "10.1080/1369118X.2020.1736124"}, "url": "https://www.semanticscholar.org/paper/032df5e7407832f9466a2ba299105505dcd38cb4", "title": "To be or not to be algorithm aware: a question of a new digital divide?", "abstract": "ABSTRACT Algorithms are an increasingly important element of internet infrastructure in that they are used to make decisions about everything from mundane music recommendations through to more profound and oftentimes life changing ones such as policing, health care or social benefits. Given algorithmic systems\u2019 impact and sometimes harm on people\u2019s everyday life, information access and agency, awareness of algorithms has the potential to be a critical issue. We, therefore, ask whether having awareness of algorithms or not corresponds to a new reinforced digital divide. This study examines levels of awareness and attitudes toward algorithms across the population of the highly digitized country of Norway. Our exploratory research finds clear demographic differences regarding levels of algorithms awareness. Furthermore, attitudes to algorithm driven recommendations (e.g., YouTube and Spotify), advertisements and content (e.g., personalized news feeds in social media and online newspaper) are associated with both the level of algorithm awareness and demographic variables. A cluster analysis facilitates an algorithm awareness typology of six groups: the unaware, the uncertain, the affirmative, the neutral, the sceptic and the critical.", "venue": "", "year": 2020, "referenceCount": 76, "citationCount": 25, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "116813442", "name": "A. Gran"}, {"authorId": "2071561863", "name": "Peter Booth"}, {"authorId": "38516982", "name": "Taina Bucher"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0412076e1004d030ac02de77bc44cc7d92b13ab9", "externalIds": {"MAG": "2997335426", "DBLP": "journals/corr/abs-2001-00973", "ArXiv": "2001.00973", "DOI": "10.1145/3351095.3372873"}, "url": "https://www.semanticscholar.org/paper/0412076e1004d030ac02de77bc44cc7d92b13ab9", "title": "Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing", "abstract": "Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.", "venue": "FAT*", "year": 2020, "referenceCount": 106, "citationCount": 116, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "81316798", "name": "Inioluwa Deborah Raji"}, {"authorId": "152735378", "name": "A. Smart"}, {"authorId": "2109593191", "name": "Rebecca N. White"}, {"authorId": "118707418", "name": "Margaret Mitchell"}, {"authorId": "2076288", "name": "Timnit Gebru"}, {"authorId": "2083807", "name": "B. Hutchinson"}, {"authorId": "1452730703", "name": "Jamila Smith-Loud"}, {"authorId": "1477280273", "name": "Daniel Theron"}, {"authorId": "80940648", "name": "P. Barnes"}]}}, {"contexts": ["The uses and manifestations of AI take shape through complex interrelations between users and machines (Christin, 2017; Crawford & Joler, 2018)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "150298b517c930947ca0fc3d69f25505297ab623", "externalIds": {"MAG": "3001465270", "DOI": "10.1080/1369118X.2020.1713842"}, "url": "https://www.semanticscholar.org/paper/150298b517c930947ca0fc3d69f25505297ab623", "title": "Attributions of ethical responsibility by Artificial Intelligence practitioners", "abstract": "ABSTRACT Systems based on Artificial Intelligence (AI) are increasingly normalized as part of work, leisure, and governance in contemporary societies. Although ethics in AI has received significant attention, it remains unclear where the burden of responsibility lies. Through twenty-one interviews with AI practitioners in Australia, this research seeks to understand how ethical attributions figure into the professional imagination. As institutionally embedded technical experts, AI practitioners act as a connective tissue linking the range of actors that come in contact with, and have effects upon, AI products and services. Findings highlight that practitioners distribute ethical responsibility across a range of actors and factors, reserving a portion of responsibility for themselves, albeit constrained. Characterized by imbalances of decision-making power and technical expertise, practitioners position themselves as mediators between powerful bodies that set parameters for production; users who engage with products once they leave the proverbial workbench; and AI systems that evolve and develop beyond practitioner control. Distributing responsibility throughout complex sociotechnical networks, practitioners preclude simple attributions of accountability for the social effects of AI. This indicates that AI ethics are not the purview of any singular player but instead, derive from collectivities that require critical guidance and oversight at all stages of conception, production, distribution, and use.", "venue": "", "year": 2020, "referenceCount": 73, "citationCount": 21, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "117258238", "name": "W. Orr"}, {"authorId": "152658412", "name": "Jenny L. Davis"}]}}, {"contexts": ["The focus upon process was not simply a focus upon \u2018doings\u2019, but was a method for reorienting our ethnographic focus towards the use of the algorithm and associated practices (Christin, 2017), and the manner in which the actions of both technologies and users were coordinated within these settings."], "isInfluential": false, "intents": ["methodology"], "citingPaper": {"paperId": "16a0cae293fa3204014584c673771ba01e472de8", "externalIds": {"MAG": "2991577617", "DOI": "10.1177/0011392120907638"}, "url": "https://www.semanticscholar.org/paper/16a0cae293fa3204014584c673771ba01e472de8", "title": "Dismembering organisation: The coordination of algorithmic work in healthcare", "abstract": "Algorithms are increasingly being adopted in healthcare settings, promising increased safety, productivity and efficiency. The growing sociological literature on algorithms in healthcare shares an assumption that algorithms are introduced to \u2018support\u2019 decisions within an interactive order that is predominantly human-oriented. This article presents a different argument, calling attention to the manner in which organisations can end up introducing a non-negotiable disjuncture between human-initiated care work and work that supports algorithms, which the authors call algorithmic work. Drawing on an ethnographic study, the authors describe how two hospitals in England implemented an Acute Kidney Injury (AKI) algorithm and analyse \u2018interruptions\u2019 to the algorithm\u2019s expected performance. When the coordination of algorithmic work occludes care work, the study finds a \u2018dismembered\u2019 organisation that is algorithmically-oriented rather than human-oriented. In the discussion, the authors examine the consequences of coordinating human and non-human work in each hospital and conclude by urging sociologists of organisation to attend to the importance of the formal in algorithmic work. As the use of algorithms becomes widespread, the analysis provides insight into how organisations outside of healthcare can also end up severing tasks from human experience when algorithmic automation is introduced.", "venue": "", "year": 2020, "referenceCount": 88, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "144822115", "name": "Simon Bailey"}, {"authorId": "50035613", "name": "Dean C. Pierides"}, {"authorId": "78306056", "name": "Adam Brisley"}, {"authorId": "48791297", "name": "C. Weisshaar"}, {"authorId": "120366038", "name": "T. Blakeman"}]}}, {"contexts": ["By considering in empirical detail how \u201calgorithms are lived with\u201d (Beer, 2017:4) and imagined, gamed or contested by their users (Christin, 2017; Crawford, 2016), it is possible to balance pre-eminent narratives of the \u201cpowerful [algorithms] that rule, sort, govern, shape, or otherwise control our\u2026"], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "1d8e8be4ee1c1cca533722386dfa6f6f2eb1c4ed", "externalIds": {"MAG": "3083248133", "DOI": "10.5194/gh-75-259-2020"}, "url": "https://www.semanticscholar.org/paper/1d8e8be4ee1c1cca533722386dfa6f6f2eb1c4ed", "title": "Foams of togetherness in the digital age: Sloterdijk, software sorting and Foursquare", "abstract": "Abstract. The article brings together Peter Sloterdijk's theory of spheres and literatures on the socio-spatial implications of the functioning of software. By examining the growing personalization of search results for\nrecreational places on spatial media like Foursquare, we make the case for\nSloterdijk's conceptualization of \u201cfoam\u201d offering an interesting\ncontribution to the analysis and critique of contemporary algorithmic life,\nin particular with regard to the liquidity and fragility of the forms of\ntogetherness in \u201cco-isolation\u201d created by such applications. In emphasizing the impermanence and ambiguities of these fleeting mediations,\nthe article also points to the politics of these algorithmic foams, whose\nlogics of categorization and socio-spatial sorting become increasingly\ndifficult to understand, politically address or challenge.", "venue": "", "year": 2020, "referenceCount": 71, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "116751505", "name": "S. Widmer"}, {"authorId": "66682367", "name": "F. Klauser"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "267cb6b0d92d0e879f9a5603afdb5510848ac592", "externalIds": {"MAG": "3046850659", "DOI": "10.1111/gwao.12511"}, "url": "https://www.semanticscholar.org/paper/267cb6b0d92d0e879f9a5603afdb5510848ac592", "title": "Equal pay index for men and women: The performative power of quantification conventions", "abstract": "Several methods of calculation may be used to quantify the gender pay gap. The sociology of quantification shows how the construction of indicators may be employed to define certain notions. Literature on the performativity of quantification also highlights the effect that acts of calculation can have upon reality. This article combines these fields as it examines the illocutionary performative power of the \u201cequal pay index\u201d, which was developed by the French government in 2018, and which companies are legally obliged to publish on an annual basis. The article therefore draws on different types of documents: leaflets explaining the construction of the index, government\u2010issued communications, statements given by members of the government and companies\u2019 own communications. With the aid of this index, the government can provide more precise definitions of the notion of equal pay and the field of action of companies. The novelty and benefits of the current case lie in the pairing of a relatively classic tool for commensuration and a subject\u2014equal pay\u2014that has suffered from a vague definition for decades. Furthermore, this article offers a conjoint study of the manner in which the index is built and the emerging discourse around it.", "venue": "", "year": 2020, "referenceCount": 75, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "2080830037", "name": "Clotilde Coron"}]}}, {"contexts": ["For example, consider algorithmic risk assessment (Angwin et al., 2016; Christin, 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "29e13fd8977ee79297c2f0a8a317155cadaa36fd", "externalIds": {"DBLP": "journals/hhci/BaumerSMZSM20", "MAG": "3019215050", "DOI": "10.1080/07370024.2020.1734460"}, "url": "https://www.semanticscholar.org/paper/29e13fd8977ee79297c2f0a8a317155cadaa36fd", "title": "Topicalizer: reframing core concepts in machine learning visualization by co-designing for interpretivist scholarship", "abstract": "Topicalizer: reframing core concepts in machine learning visualization by co-designing for interpretivist scholarship Eric P. S. Baumer, Drew Siedel, Lena McDonnell, Jiayun Zhong, Patricia Sittikul & Micki McGee To cite this article: Eric P. S. Baumer, Drew Siedel, Lena McDonnell, Jiayun Zhong, Patricia Sittikul & Micki McGee (2020): Topicalizer: reframing core concepts in machine learning visualization by co-designing for interpretivist scholarship, Human\u2013Computer Interaction, DOI: 10.1080/07370024.2020.1734460 To link to this article: https://doi.org/10.1080/07370024.2020.1734460", "venue": "Hum. Comput. Interact.", "year": 2020, "referenceCount": 161, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144392313", "name": "E. Baumer"}, {"authorId": "1729290159", "name": "Drew Siedel"}, {"authorId": "1729284127", "name": "Lena McDonnell"}, {"authorId": "2114795670", "name": "Jiayun Zhong"}, {"authorId": "1729284129", "name": "Patricia Sittikul"}, {"authorId": "118156953", "name": "M. McGee"}]}}, {"contexts": ["Interviews-based studies on algorithms in everyday life include Bucher (2016), Christin (2017), and Hong and Chen (2014).", "This study answers calls from Beer (2009), Bucher (2012), Christin (2017), Kitchin (2017), and van Dijck (2013), and others to explore algorithms in everyday life in different social contexts and contributes to double articulation analyses, integrating content and social discourses with their\u2026", "Some scholars have started conducting empirical research on the interaction between use and algorithmic responses, e.g., logics that generate content on news feeds (Bucher 2012, 2016; Christin 2017; van Dijck 2013).", "However, scholarship has been challenging from both a theoretically (Beer 2009; Lash 2007; Thrift 2005) and empirically (Bucher 2012, 2016; Hong and Chen 2014; Christin 2017).", "Moreover, existing studies look at algorithms in everyday life from the algorithm side of the equation, with a few studies complementing it with observations of everyday users, like Bucher (2012, 2016) and Christin (2017)."], "isInfluential": true, "intents": ["methodology", "background"], "citingPaper": {"paperId": "3431f13465d052ccf7d04e6278020f7045b682ba", "externalIds": {"MAG": "3006650559", "DBLP": "journals/tis/Leong20", "DOI": "10.1080/01972243.2019.1709930"}, "url": "https://www.semanticscholar.org/paper/3431f13465d052ccf7d04e6278020f7045b682ba", "title": "Domesticating algorithms: An exploratory study of Facebook users in Myanmar", "abstract": "Abstract This exploratory study investigates the encounters and everyday experiences with the Facebook algorithm of 18 informants in Yangon, Myanmar. It draws on domestication theory and research on algorithms to understand how users come to use and respond to Facebook. Findings showed that their particular perception of Facebook algorithm\u2014Friends funnel information\u2014informs their domestication process, wherein they add strangers as Friends to draw more information flows to their News Feeds.", "venue": "Inf. Soc.", "year": 2020, "referenceCount": 125, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology", "Computer Science"], "authors": [{"authorId": "144917239", "name": "Lorian Leong"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "39abb8325ca253f1569246ee5aeb0d7c5cf5a087", "externalIds": {"MAG": "3042472959", "DOI": "10.1080/17512786.2020.1791231"}, "url": "https://www.semanticscholar.org/paper/39abb8325ca253f1569246ee5aeb0d7c5cf5a087", "title": "Quantifying Quality: Negotiating Audience Participation and the Value of a Digital Story at NRK", "abstract": "The use of metrics and analytics at Norway's national broadcaster, NRK, allows the audience to participate in the news process, impacting story promotion, selection, and formatting. Cognizant of th...", "venue": "", "year": 2020, "referenceCount": 82, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "2050403954", "name": "Nicole Blanchett"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "3d7da9c300b65fa320ecd99f7917c06033923d76", "externalIds": {"MAG": "3168292979", "DOI": "10.2139/ssrn.3687843"}, "url": "https://www.semanticscholar.org/paper/3d7da9c300b65fa320ecd99f7917c06033923d76", "title": "Making Sense of Algorithms: Relational Perception of Contact Tracing and Risk Assessment during COVID-19", "abstract": "Governments and citizens of nearly every nation have been compelled to respond to COVID-19. Many measures have been adopted, including contact tracing and risk assessment algorithms, whereby citizen whereabouts are monitored to trace contact with other infectious individuals in order to generate a risk status via algorithmic evaluation. Based on 38 in-depth interviews, we investigate how people make sense of Health Code (jiankangma), the Chinese contact tracing and risk assessment algorithmic sociotechnical assemblage. We probe how people accept or resist Health Code by examining their ongoing, dynamic, and relational interactions with it. Participants display a rich variety of attitudes towards privacy and surveillance, ranging from fatalism to the possibility of privacy to trade-offs for surveillance in exchange for public health, which is mediated by the perceived effectiveness of Health Code and changing views on the intentions of institutions who deploy it. We show how perceived competency varies not just on how well the technology works, but on the social and cultural enforcement of various non-technical aspects like quarantine, citizen data inputs, and cell reception. Furthermore, we illustrate how perceptions of Health Code are nested in people\u2019s broader interpretations of disease control at the national and global level, and unexpectedly strengthen the Chinese authority\u2019s legitimacy. None of the Chinese public, Health Code, or people\u2019s perceptions toward Health Code are predetermined, fixed, or categorically consistent, but are co-constitutive and dynamic over time. We conclude with a theorization of a relational perception and methodological reflections to study algorithmic sociotechnical assemblages beyond COVID-19.", "venue": "", "year": 2020, "referenceCount": 48, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "2107892115", "name": "Chuncheng Liu"}, {"authorId": "2089380010", "name": "Ross Graham"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "4b032602148726c4fd254a165db4d1885248a8ca", "externalIds": {"DBLP": "conf/chi/PaakkonenNHL20", "MAG": "3029690672", "DOI": "10.1145/3313831.3376780"}, "url": "https://www.semanticscholar.org/paper/4b032602148726c4fd254a165db4d1885248a8ca", "title": "Bureaucracy as a Lens for Analyzing and Designing Algorithmic Systems", "abstract": "Scholarship on algorithms has drawn on the analogy between algorithmic systems and bureaucracies to diagnose shortcomings in algorithmic decision-making. We extend the analogy further by drawing on Michel Crozier's theory of bureaucratic organizations to analyze the relationship between algorithmic and human decision-making power. We present algorithms as analogous to impartial bureaucratic rules for controlling action, and argue that discretionary decision-making power in algorithmic systems accumulates at locations where uncertainty about the operation of algorithms persists. This key point of our essay connects with Alkhatib and Bernstein's theory of 'street-level algorithms', and highlights that the role of human discretion in algorithmic systems is to accommodate uncertain situations which inflexible algorithms cannot handle. We conclude by discussing how the analysis and design of algorithmic systems could seek to identify and cultivate important sources of uncertainty, to enable the human discretionary work that enhances systemic resilience in the face of algorithmic errors.", "venue": "CHI", "year": 2020, "referenceCount": 131, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "46255424", "name": "Juho P\u00e4\u00e4kk\u00f6nen"}, {"authorId": "1879192", "name": "Matti Nelimarkka"}, {"authorId": "51391714", "name": "Jesse Haapoja"}, {"authorId": "1805999", "name": "Airi Lampinen"}]}}, {"contexts": ["Echoing a growing scholarly focus on people\u2019s perception of algorithms in practice (Amelang and Bauer, 2019; Brayne and Christin, 2020; Bucher, 2017; Christin, 2017), we demonstrate how people perceive Health Code along three axes: privacy issues, technology use, and implementation.", "\u2026must treat them as dynamic and unstable objects, ones that are enacted not only through an automated digital process but by the perceptions, imaginaries, and practices of the individuals who interact with them in their everyday lives (Christin, 2017; Introna, 2016; Liu, 2020; Seaver, 2017).", "(Bourdieu, 1998: 4\u20135)\nAngele Christin (2017) has conducted studies in this vein, addressing how people make sense of an algorithm in various contexts.", "Specific, dynamic imaginaries of the algorithm were cultivated over time that were \u201cmobilized differently to comply with or resist algorithmic technologies\u201d (Christin, 2017: 11)."], "isInfluential": true, "intents": ["background"], "citingPaper": {"paperId": "4fc321ffaafa7837cc1841756f86055953cfccab", "externalIds": {"MAG": "3118801830", "DOI": "10.1177/2053951721995218"}, "url": "https://www.semanticscholar.org/paper/4fc321ffaafa7837cc1841756f86055953cfccab", "title": "Making sense of algorithms: Relational perception of contact tracing and risk assessment during COVID-19", "abstract": "Governments and citizens of nearly every nation have been compelled to respond to COVID-19. Many measures have been adopted, including contact tracing and risk assessment algorithms, whereby citizen whereabouts are monitored to trace contact with other infectious individuals in order to generate a risk status via algorithmic evaluation. Based on 38 in-depth interviews, we investigate how people make sense of Health Code (jiankangma), the Chinese contact tracing and risk assessment algorithmic sociotechnical assemblage. We probe how people accept or resist Health Code by examining their ongoing, dynamic, and relational interactions with it. Participants display a rich variety of attitudes toward privacy and surveillance, ranging from fatalism to the possibility of privacy to trade-offs for surveillance in exchange for public health, which is mediated by the perceived effectiveness of Health Code and changing views on the intentions of institutions who deploy it. We show how perceived competency varies not just on how well the technology works, but on the social and cultural enforcement of various non-technical aspects like quarantine, citizen data inputs, and cell reception. Furthermore, we illustrate how perceptions of Health Code are nested in people\u2019s broader interpretations of disease control at the national and global level, and unexpectedly strengthen the Chinese authority\u2019s legitimacy. None of the Chinese public, Health Code, or people\u2019s perceptions toward Health Code are predetermined, fixed, or categorically consistent, but are co-constitutive and dynamic over time. We conclude with a theorization of a relational perception and methodological reflections to study algorithmic sociotechnical assemblages beyond COVID-19.", "venue": "", "year": 2021, "referenceCount": 68, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2107892115", "name": "Chuncheng Liu"}, {"authorId": "2089380010", "name": "Ross Graham"}]}}, {"contexts": ["Such a comparative approach can also shed light on complex and opaque algorithmic systems (Anderson and Kreiss 2013; Christin 2017; Griesbach et al. 2019)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "58884a63fed93e6c0400ba2b59b4961ee58ee9ce", "externalIds": {"MAG": "3049046209", "DOI": "10.1007/s11186-020-09411-3"}, "url": "https://www.semanticscholar.org/paper/58884a63fed93e6c0400ba2b59b4961ee58ee9ce", "title": "The ethnographer and the algorithm: beyond the black box", "abstract": "A common theme in social science studies of algorithms is that they are profoundly opaque and function as \u201cblack boxes.\u201d Scholars have developed several methodological approaches in order to address algorithmic opacity. Here I argue that we can explicitly enroll algorithms in ethnographic research, which can shed light on unexpected aspects of algorithmic systems\u2014including their opacity. I delineate three meso-level strategies for algorithmic ethnography. The first, algorithmic refraction, examines the reconfigurations that take place when computational software, people, and institutions interact. The second strategy, algorithmic comparison, relies on a similarity-and-difference approach to identify the instruments\u2019 unique features. The third strategy, algorithmic triangulation, enrolls algorithms to help gather rich qualitative data. I conclude by discussing the implications of this toolkit for the study of algorithms and future of ethnographic fieldwork.", "venue": "", "year": 2020, "referenceCount": 178, "citationCount": 22, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3379909", "name": "Ang\u00e8le Christin"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "59464f18d48f71787dcfd6f2a3dc794cef3c010d", "externalIds": {"MAG": "3006396560", "DOI": "10.1108/jcp-09-2019-0032"}, "url": "https://www.semanticscholar.org/paper/59464f18d48f71787dcfd6f2a3dc794cef3c010d", "title": "Evaluating the use of data-based offender profiling by researchers, practitioners and investigative journalists to address unresolved serial homicides", "abstract": "The purpose of this article is to improve the use of evidence-based practice and research utilization in the offender profiling process. The use of offender profiling has been met with increasing resistance given its exaggerated accuracy. The \u201cInvestigative Journalist/Expert Field Micro Task Force\u201d model, a collaborative method that incorporates offender profiling and is designed to address unresolved serial homicides, is introduced and evaluated alongside recommendations on attaining adherence.,The model was field tested in 17 instances. The measures used by the Federal Bureau of Investigation to gauge the usefulness of their case consultations, whether their input helped catch the offender, offer new leads, move the case forward, provide new avenues or give new ideas, were used to evaluate the model.,The model established likely patterns of serial murder activity among strangulations of women in Chicago, Cleveland, and Panama and resulted in convictions of suspects in Louisiana and Kansas City. This model is valuable when used to parse modern-day offenders from those who committed unresolved homicides as the latter display different behaviors that can make investigations difficult endeavors. Results from the field tests mirror those from the literature in that profiling alone did not result in the capture of serial killers. Instead, profiling was used in conjunction with other efforts and mainly as a means to keep the investigation moving forward.,Unresolved homicides are at a point of crisis and represent a significant but largely unaddressed societal problem. The success of this model may compel law enforcement to restore faith in offender profiling.", "venue": "", "year": 2020, "referenceCount": 80, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "14543785", "name": "E. Yaksic"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5a451ddc59e80beb159c3363faff2f5598e4ab1e", "externalIds": {"MAG": "3113581731", "DOI": "10.1080/1369118x.2020.1863999"}, "url": "https://www.semanticscholar.org/paper/5a451ddc59e80beb159c3363faff2f5598e4ab1e", "title": "\u2018I\u2019m still the master of the machine.\u2019 Internet users\u2019 awareness of algorithmic decision-making and their perception of its effect on their autonomy", "abstract": "Algorithms are an integral part of our everyday lives and shape the selection and presentation of information and communication on the internet. At the same time, media users are faced with a lack ...", "venue": "", "year": 2020, "referenceCount": 38, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2420895", "name": "Leyla Dogruel"}, {"authorId": "2091741960", "name": "Dominique Facciorusso"}, {"authorId": "1844292960", "name": "Birgit Stark"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "5a8a5eb3afa98dd22d86c6142855345f949a1cc4", "externalIds": {"MAG": "3028565296", "DOI": "10.1080/1369118x.2020.1761860"}, "url": "https://www.semanticscholar.org/paper/5a8a5eb3afa98dd22d86c6142855345f949a1cc4", "title": "The (in)credibility of algorithmic models to non-experts", "abstract": "The rapid development and dissemination of data analysis techniques permits the creation of ever more intricate algorithmic models. Such models are simultaneously the vehicle and outcome of quantif...", "venue": "", "year": 2020, "referenceCount": 46, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "51303123", "name": "D. Kolkman"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "65ea5b4545eb574c67474123cafb53214f112331", "externalIds": {"MAG": "3091893025", "DBLP": "journals/jms/PesqueiraSR20", "PubMedCentral": "7544557", "DOI": "10.1007/s10916-020-01665-9", "PubMed": "33034743"}, "url": "https://www.semanticscholar.org/paper/65ea5b4545eb574c67474123cafb53214f112331", "title": "Big Data Skills Sustainable Development in Healthcare and Pharmaceuticals", "abstract": "Big Data technology is one of the most promising organizational processes within the Healthcare and Pharmaceutical industry and crucial for any company that wants to preserve the competitive advantage in the market, where most of the organizational structures are already struggling with the right skills and knowledge to fully support existing business needs for storing and processing and even analyzing information. This paper aims to examine the extent to which new Big Data technology and data-related processes are developing different professionals skills and competencies within the Healthcare and Pharmaceutical industries, and creating sustainable development in addressing critical organizational challenges in recruiting, retaining, and discover professional skills that can fully support the advances and exponential growth of Big Data technology benefits. This research paper also highlights the significant aspects of Big Data in professional technical and process oriented skills development, and the influence it has on organizational business processes including how various internal functions will need to adapt to new circumstances with renewed competency and skills development programs for departments that are strongly connected to the business and analytical needs. We conducted a focus group with twenty-five industry based professionals\u2019 ranges from analysts to executive directors to better assess the necessary knowledge to answer the proposed research questions: (1) which professional skills can big data influence in employee development and (2) how can organizations adapt their employee skills to big data. Regarding the key research limitations/implications most of the article and research was built on the foundation of the literature review and the performed focus group. The conceptual recommendations and observations presented provide solid empirical evidence but should be subjected to more comprehensive, large-scale empirical testing and validation. It\u2019s recommended for future research a more extensive sample of companies, organizations, and interviewees. Studying a broader set of similar research questions in more homogeneous organizations could provide deeper insights into the process, governance, and stakeholder dimensions of Big Data within specific contexts. Therefore this study contributes to explore in-depth and systematically to what extent Big Data technology and processes are currently influencing the healthcare and pharmaceuticals industries where to the best of the authors\u2019 knowledge, it is the first focus group dealing with the presented research questions.", "venue": "Journal of Medical Systems", "year": 2020, "referenceCount": 73, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Business", "Medicine", "Computer Science"], "authors": [{"authorId": "150916789", "name": "A. Pesqueira"}, {"authorId": "144339427", "name": "M. Sousa"}, {"authorId": "145786399", "name": "\u00c1. Rocha"}]}}, {"contexts": ["\u2026\u2022 technology \u2022 case study \u2022 ethnography\nAs the world of work changes, workers must find innovative ways of acting skillfully in the midst of a new and shifting set of problems in order to deliver value (e.g., Christin 2017, Pine and Mazmanian 2017, Beane 2019, and Myers and Kellogg 2020)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "66c51dbe154f1d8fe7e05494833fe12c045ec050", "externalIds": {"MAG": "3092685420", "DOI": "10.1287/orsc.2020.1374"}, "url": "https://www.semanticscholar.org/paper/66c51dbe154f1d8fe7e05494833fe12c045ec050", "title": "Moving Violations: Pairing an Illegitimate Learning Hierarchy with Trainee Status Mobility for Acquiring New Skills When Traditional Expertise Erodes", "abstract": "We explore how members of a community of practice learn new tools and techniques when environmental shifts undermine existing expertise. In our 20-month comparative field study of medical assistant...", "venue": "", "year": 2020, "referenceCount": 208, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "2537130", "name": "Katherine C. Kellogg"}, {"authorId": "108764333", "name": "Jenna E. Myers"}, {"authorId": "12602582", "name": "L. Gainer"}, {"authorId": "145537261", "name": "S. Singer"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "68667ea7d42029447102037af7ab62ee02d19744", "externalIds": {"DOI": "10.5117/9789462986190_intro"}, "url": "https://www.semanticscholar.org/paper/68667ea7d42029447102037af7ab62ee02d19744", "title": "Introduction", "abstract": "The introduction chapter positions algorithmic information ordering as a\n central practice and technology in contemporary digital infrastructures, a\n set of techniques that serve as \u2018levers on reality\u2019 (Goody). While algorithms\n used in concrete systems may often be hard to scrutinize, they draw on\n widely available software modules and well-documented principles that\n make them amendable to humanistic analysis. The chapter introduces\n Gilbert Simondon\u2019s mechanology and provides an overview of the structure\n and argument of the book.", "venue": "Engines of Order", "year": 2020, "referenceCount": 10, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "145365724", "name": "Bernhard Rieder"}]}}, {"contexts": ["Yet, although a labor process perspective draws attention to how algorithmic control can result in negative outcomes for workers, studies have also shown that there is variation in worker outcomes across organizations and individuals (Christin, 2017; Griesbach et al., 2019; Lehdonvirta, 2018).", "Similarly, Christin (2017) shows that judges and prosecutors resented the opacity of predictive algorithms called riskassessment tools because they found them to be unintelligible.", "And,Christin (2017) demonstrates that web journalists and legal professionals engaged in foot-dragging (ignoring risk scores and analytics systems in their daily work), gaming (manipulating the variables they entered in algorithmic systems to obtain the score that they desired), and open critique\u2026"], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "778004fb4e0b7ab7632199ab654742d01e57aaba", "externalIds": {"MAG": "2990520658", "DOI": "10.5465/annals.2018.0174"}, "url": "https://www.semanticscholar.org/paper/778004fb4e0b7ab7632199ab654742d01e57aaba", "title": "Algorithms at Work: The New Contested Terrain of Control", "abstract": "The widespread implementation of algorithmic technologies in organizations prompts questions about how algorithms may reshape organizational control. We use Edwards\u2019 (1979) perspective of \u201cconteste...", "venue": "", "year": 2020, "referenceCount": 461, "citationCount": 150, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2537130", "name": "Katherine C. Kellogg"}, {"authorId": "50521961", "name": "Melissa A. Valentine"}, {"authorId": "3379909", "name": "Ang\u00e8le Christin"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7a0b2f44baa18b586bea802b0e128d49052bdf8b", "externalIds": {"MAG": "3038656275", "DOI": "10.1080/13510347.2020.1714595"}, "url": "https://www.semanticscholar.org/paper/7a0b2f44baa18b586bea802b0e128d49052bdf8b", "title": "Scraping the demos. Digitalization, web scraping and the democratic project", "abstract": "ABSTRACT Scientific, political and bureaucratic elites use epistemic practices like \u201cbig data analysis\u201d and \u201cweb scraping\u201d to create representations of the citizenry and to legitimize policymaking. I develop the concept of \u201cdemos scraping\u201d for these practices of gaining information about citizens (the \u201cdemos\u201d) through automated analysis of digital trace data which are re-purposed for political means. This article critically engages with the discourse advocating demos scraping and provides a conceptual analysis of its democratic implications. It engages with the promise of demos scraping advocates to reduce the gap between political elites and citizens and highlights how demos scraping is presented as a superior form of accessing the \u201cwill of the people\u201d and to increase democratic legitimacy. This leads me to critically discuss the implications of demos scraping for political representation and participation. In its current form, demos scraping is technocratic and de-politicizing; and the larger political and economic context in which it takes place makes it unlikely that it will reduce the gap between elites and citizens. From the analytic perspective of a post-democratic turn, demos scraping is an attempt of late modern and digitalized societies to address the democratic paradox of increasing citizen expectations coupled with a deep legitimation crisis.", "venue": "", "year": 2020, "referenceCount": 165, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "2094412391", "name": "Lena Ulbricht"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7be6018a042f37510b0a0472d2b35d261201794e", "externalIds": {"MAG": "3082478199", "DOI": "10.1007/978-3-030-52470-8_2"}, "url": "https://www.semanticscholar.org/paper/7be6018a042f37510b0a0472d2b35d261201794e", "title": "A Theoretical Framework for the Discussion on AI and Criminal Law", "abstract": "This chapter presents the background and the scope of the study and justifies the structure of the inquiry. In particular, the focus is on the importance to ground the reflection in the specificities of the European context.", "venue": "", "year": 2020, "referenceCount": 59, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "101258582", "name": "S. Quattrocolo"}]}}, {"contexts": ["Actors develop many strategies of resistance, including foot-dragging, gaming, and open critique (Christin, 2017).", "The examples were chosen to present significant variation in terms of scale, kind of data, and motivation behind the technology (see Christin, 2017).", "Thus, Chartbeat has different effects depending on the newsroom, and COMPAS is used differently depending on the jurisdiction under consideration, as I have explored elsewhere (Christin, 2017, 2018)."], "isInfluential": false, "intents": ["methodology", "background"], "citingPaper": {"paperId": "81648c4c0dc81b018ffc55af941f12d6d425adac", "externalIds": {"MAG": "3006041660"}, "url": "https://www.semanticscholar.org/paper/81648c4c0dc81b018ffc55af941f12d6d425adac", "title": "What Data Can Do: A Typology of Mechanisms", "abstract": "This article offers an analytical framework for understanding the effects of data on the social world. Specifically, I ask: What happens when new data \u2014digital or not\u2014is introduced in a given context? Drawing on a mix of historical and contemporary examples, I provide a typology of 5 mechanisms: tracking, homogenizing, triaging, nudging, and valuating. I then demonstrate how this framework can change how we understand two empirical cases involving data-driven software programs, one in Web journalism and the other in criminal justice. Based on this analysis, the article makes three main points. First, at a time of rapid technological development, we should pay particular attention to what is not changing with digitization. Second, we need further theoretical integration in the rapidly growing field of critical data studies. Third, I suggest that the umbrella concept of \u201cdata\u201d should be broken down into smaller and more manageable components depending on the mechanisms scholars are interested in studying.", "venue": "", "year": 2020, "referenceCount": 76, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "3379909", "name": "Ang\u00e8le Christin"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "90a9b0812d843b229cc0055e88350de2bebcff0c", "externalIds": {"MAG": "3005834607", "DOI": "10.1080/01442872.2020.1724928"}, "url": "https://www.semanticscholar.org/paper/90a9b0812d843b229cc0055e88350de2bebcff0c", "title": "Datafied child welfare services: unpacking politics, economics and power", "abstract": "ABSTRACT This article analyses three distinct child welfare data systems in England. We focus on child welfare as a contested area in public services where data systems are being used to inform decision-making and transforming governance. We advance the use of \u201cdata assemblage\u201d as an analytical framework to detail how key political and economic factors influence the development of these data systems. We provide an empirically grounded demonstration of why child welfare data systems must not be considered neutral decision aid tools. We identify how systems of thought, ownership structures, policy agendas, organizational practices, and legal frameworks influence these data systems. We find similarities in the move toward greater sharing of sensitive data, but differences in attitudes toward public-private partnerships, rights and uses of prediction. There is a worrying lack of information available about the impacts of these systems on those who are subject to them \u2013 particularly in relation to predictive data systems. We argue for policy debates to go beyond technical fixes and privacy concerns to engage with fundamental questions about the power dynamics and rights issues linked to the expansion of data sharing in this sector as well as whether predictive data systems should be used at all.", "venue": "", "year": 2020, "referenceCount": 63, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "39785277", "name": "J. Redden"}, {"authorId": "2014271", "name": "Lina Dencik"}, {"authorId": "121717977", "name": "Harry Warne"}]}}, {"contexts": ["Email: anne.kaun@sh.se\ndiscussions have also reached the juridical and legal context (Christin, 2017).", "discussions have also reached the juridical and legal context (Christin, 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "9f4780b3e77b49423a89fd535db80c184b51be75", "externalIds": {"DBLP": "journals/nms/KaunS20", "MAG": "3083612208", "DOI": "10.1177/1461444820914865"}, "url": "https://www.semanticscholar.org/paper/9f4780b3e77b49423a89fd535db80c184b51be75", "title": "Doing time, the smart way? Temporalities of the smart prison", "abstract": "The article engages with the notion of the smart prison to develop an understanding of emerging temporalities of digital technologies. The prison context serves here as a magnifying glass that makes certain contradictions and paradoxes of the digital imperative visible. Starting with a brief discussion of smart technology discourses, the article explores the temporalities of real-timeness, prediction and pre-emption that are entangled with digital technologies. Analysing the Spartan RFID tracking tool, the use of algorithms in prison administration and a mobile phone application used in Swedish probation, the article identifies a desynchronization between the temporalities of the incarcerated individuals\u2019 lived experience and the (imagined) temporalities of the smart prison. The findings point to developments that are relevant for the smart, digital society beyond the prison walls.", "venue": "New Media Soc.", "year": 2020, "referenceCount": 64, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Sociology"], "authors": [{"authorId": "2452684", "name": "Anne Kaun"}, {"authorId": "3181432", "name": "Fredrik Stiernstedt"}]}}, {"contexts": ["Initial arguments for the introduction of algorithms in organisational settings centred on their capacity to make judgements with greater objectivity than human counterparts (Christin, 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "a81fe446c4eb3af179e2a2a196331e583421e27c", "externalIds": {"MAG": "3034749153", "DOI": "10.1177/0170840620937900"}, "url": "https://www.semanticscholar.org/paper/a81fe446c4eb3af179e2a2a196331e583421e27c", "title": "Algorithmic Surveillance in the Gig Economy: The Organization of Work through Lefebvrian Conceived Space", "abstract": "Workplace surveillance is traditionally conceived of as a dyadic process, with an observer and an observee. In this paper, I discuss the implications of an emerging form of workplace surveillance: surveillance with an algorithmic, as opposed to human, observer. Situated within the on-demand food-delivery context, I draw upon Henri Lefebvre\u2019s spatial triad to provide in-depth conceptual examination of how platforms rely on conceived space, namely the virtual reality generated by data capture, while neglecting perceived and lived space in the form of the material embodied reality of workers. This paper offers a two-fold contribution. First, it applies Henri Lefebvre\u2019s spatial triad to the techno-centric digital cartography used by platform-mediated organisations, assessing spatial power dynamics and opportunities for resistance. Second, this paper advances organisational research into workplace surveillance in situations where the observer and decision-maker can be a non-human agent.", "venue": "", "year": 2020, "referenceCount": 160, "citationCount": 20, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "34633345", "name": "G. Newlands"}]}}, {"contexts": ["Many debates about COMPAS and related systems have placed less attention on the models as an element of a broader socio-technical software systemwithin organizations, although there are notable exceptions like Christin\u2019s ethnographic work on algorithmic decision making in criminal justice, who raises similar organizational issues [15, 16].", "Christin [15] also uses \u201cdecoupling\u201d to discuss algorithms in an organizational context, in an allied but somewhat different use."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "aac29b1b8627f56a258c52e95092a3fdd863b137", "externalIds": {"DBLP": "journals/pacmhci/HalfakerG20", "MAG": "3094328607", "ArXiv": "1909.05189", "DOI": "10.1145/3415219"}, "url": "https://www.semanticscholar.org/paper/aac29b1b8627f56a258c52e95092a3fdd863b137", "title": "ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia", "abstract": "Algorithmic systems -- from rule-based bots to machine learning classifiers -- have a long history of supporting the essential work of content moderation and other curation work in peer production projects. From counter-vandalism to task routing, basic machine prediction has allowed open knowledge projects like Wikipedia to scale to the largest encyclopedia in the world, while maintaining quality and consistency. However, conversations about how quality control should work and what role algorithms should play have generally been led by the expert engineers who have the skills and resources to develop and modify these complex algorithmic systems. In this paper, we describe ORES: an algorithmic scoring service that supports real-time scoring of wiki edits using multiple independent classifiers trained on different datasets. ORES decouples several activities that have typically all been performed by engineers: choosing or curating training data, building models to serve predictions, auditing predictions, and developing interfaces or automated agents that act on those predictions. This meta-algorithmic system was designed to open up socio-technical conversations about algorithmic systems in Wikipedia to a broader set of participants. In this paper, we discuss the theoretical mechanisms of social change ORES enables and detail case studies in participatory machine learning around ORES from the 4 years since its deployment.", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2019, "referenceCount": 136, "citationCount": 28, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Sociology"], "authors": [{"authorId": "1793302", "name": "Aaron Halfaker"}, {"authorId": "143984380", "name": "R. Geiger"}]}}, {"contexts": ["Christin (2017) studies how algorithmic decision-making is applied differently in practice within journalism and criminal justice compared to the original intentions of the AI systems."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "ab51a54f53540ea7ce23cdd3d05fe0ce625cb972", "externalIds": {"MAG": "3022967462", "DOI": "10.1080/1369118X.2020.1751866"}, "url": "https://www.semanticscholar.org/paper/ab51a54f53540ea7ce23cdd3d05fe0ce625cb972", "title": "Building truths in AI: Making predictive algorithms doable in healthcare", "abstract": "ABSTRACT Increasingly, artificial intelligent (AI) algorithms are being applied to automatically assist or automate decisions. Such statistical models have been criticized in the existing literature especially for producing cultural biases and for challenging our notions of knowledge. However, few studies have contributed to an essential understanding of the way in which algorithms are designed with particular truths to enable systematic decision-making. Drawing on an ethnographic study in a Scandinavian AI company, this article analyzes how truth is built through layered interpretative practices in applied AI for healthcare, and critically assesses how such practices shed light on the pragmatic notion of truth(s) in AI. The study identifies five practices that all show difficulty in modeling fuzzy patient conditions into one firm truth. The key contribution is that truth goes from being a process of discovering a more \u2018right\u2019 truth to become a process of reinventing the existing truth and healthcare practice. These findings suggest that truth in applied AI is a key devise for making predictive algorithms a viable business, and that developers are in a favorable position to make not only AI doable but also the very truth they intend to find and model. The study in this way shows how change is an inherent part of making AI systems, and that centralizing truth practices is a fruitful way of analyzing such changes and developers\u2019 agency. We argue for analytical awareness of how AI truth practices may prompt a world that is fit to algorithms rather than a world to which algorithms are fit.", "venue": "", "year": 2020, "referenceCount": 82, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "65761004", "name": "A. Henriksen"}, {"authorId": "30827227", "name": "Anja Bechmann"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "abae888214a50f43bbc98fdcd521ca0a5effbdb3", "externalIds": {"MAG": "3039055470", "DOI": "10.1016/j.obhdp.2020.03.008"}, "url": "https://www.semanticscholar.org/paper/abae888214a50f43bbc98fdcd521ca0a5effbdb3", "title": "When eliminating bias isn\u2019t fair: Algorithmic reductionism and procedural justice in human resource decisions", "abstract": "Abstract The perceived fairness of decision-making procedures is a key concern for organizations, particularly when evaluating employees and determining personnel outcomes. Algorithms have created opportunities for increasing fairness by overcoming biases commonly displayed by human decision makers. However, while HR algorithms may remove human bias in decision making, we argue that those being evaluated may perceive the process as reductionistic, leading them to think that certain qualitative information or contextualization is not being taken into account. We argue that this can undermine their beliefs about the procedural fairness of using HR algorithms to evaluate performance by promoting the assumption that decisions made by algorithms are based on less accurate information than identical decisions made by humans. Results from four laboratory experiments (N\u00a0=\u00a0798) and a large-scale randomized experiment in an organizational setting (N\u00a0=\u00a01654) confirm this hypothesis. Theoretical and practical implications for organizations using algorithms and data analytics are discussed.", "venue": "", "year": 2020, "referenceCount": 74, "citationCount": 30, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "2054845608", "name": "David T. Newman"}, {"authorId": "3380591", "name": "Nathanael J. Fast"}, {"authorId": "15428149", "name": "Derek J. Harmon"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b80283e61cec17b486bf78985411114e3526796e", "externalIds": {"MAG": "3081817064", "DOI": "10.1007/978-3-030-52470-8_1"}, "url": "https://www.semanticscholar.org/paper/b80283e61cec17b486bf78985411114e3526796e", "title": "Approaching the Unknown: Some\u00a0Preliminary Words", "abstract": "The chapter presents the outline of the book, justifies the purposes of it and sets the boundaries of this enquiry. It frames the work into the methodological limits of a theoretical reconstruction of the topic.", "venue": "", "year": 2020, "referenceCount": 32, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "101258582", "name": "S. Quattrocolo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b9fc17bd609f42c73270220a4ce95385e2d9dd55", "externalIds": {"MAG": "3003545719", "ArXiv": "1908.09092", "DBLP": "journals/corr/abs-1908-09092", "DOI": "10.1145/3351095.3372839"}, "url": "https://www.semanticscholar.org/paper/b9fc17bd609f42c73270220a4ce95385e2d9dd55", "title": "Fairness warnings and fair-MAML: learning fairly with minimal data", "abstract": "Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.", "venue": "FAT*", "year": 2019, "referenceCount": 55, "citationCount": 17, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "153794305", "name": "Dylan Slack"}, {"authorId": "34597147", "name": "Sorelle A. Friedler"}, {"authorId": "1388056259", "name": "Emile Givental"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "bf3edc7af7c1d647e2b4c1ba55256ec21336568c", "externalIds": {"MAG": "3106540224", "DOI": "10.1177/1461444820972389"}, "url": "https://www.semanticscholar.org/paper/bf3edc7af7c1d647e2b4c1ba55256ec21336568c", "title": "Proactive ephemerality: How journalists use automated and manual tweet deletion to minimize risk and its consequences for social media as a public archive", "abstract": "Despite their ephemeral constantly changing nature, social media constitute an archive of public discourse. In this study, we examine when, how, and why journalists practice proactive ephemerality,...", "venue": "", "year": 2020, "referenceCount": 49, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "144817980", "name": "Sharon Ringel"}, {"authorId": "34984316", "name": "Roei Davidson"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "cb38c726a1d9a79e8b542b5f76fa7e3276f58f88", "externalIds": {"DOI": "10.5070/lp61150258"}, "url": "https://www.semanticscholar.org/paper/cb38c726a1d9a79e8b542b5f76fa7e3276f58f88", "title": "Learning Like a State: Statecraft in the Digital Age", "abstract": "What does it mean to sense, see, and act like a state in the digital age? We examine the changing phenomenology, governance, and capacity of the state in the era of big data and machine learning. Our argument is threefold. First, what we call the dataist state may be less accountable than its predecessor, despite its promise of enhanced transparency and accessibility. Second, a rapid expansion of the data collection mandate is fueling a transformation in political rationality, in which data affordances increasingly drive policy strategies. Third, the turn to dataist statecraft facilitates a corporate reconstruction of the state. On the one hand, digital firms attempt to access and capitalize on data \u201cminted\u201d by the state. On the other hand, firms compete with the state in an effort to reinvent traditional public functions. Finally, we explore what it would mean for this dataist state to \u201csee like a citizen\u201d instead.", "venue": "Journal of Law and Political Economy", "year": 2020, "referenceCount": 179, "citationCount": 9, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "145566388", "name": "M. Fourcade"}, {"authorId": null, "name": "Jeffrey Gordon"}]}}, {"contexts": ["\u2026alarms about algorithmic power (e.g. Pariser, 2011; Pasquale, 2016) have been supplemented with nuanced organizational or meso-level accounts of how algorithms are deployed in particular occupational spheres, including journalism (Christin, 2017; Gillespie, 2018; Petre, 2015, 2018; Zamith, 2018)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "d333bcf369e71fd9119ab0f971930a0cd08359c5", "externalIds": {"DBLP": "journals/nms/GravesA20", "MAG": "2951116596", "DOI": "10.1177/1461444819856916"}, "url": "https://www.semanticscholar.org/paper/d333bcf369e71fd9119ab0f971930a0cd08359c5", "title": "Discipline and promote: Building infrastructure and managing algorithms in a \u201cstructured journalism\u201d project by professional fact-checking groups", "abstract": "News organizations have adapted in various ways to a digital media environment dominated by algorithmic gatekeepers such as search engines and social networks. This article dissects a campaign to actively shape that environment led by professional fact-checking organizations. We trace the development of the Share the Facts \u201cwidget,\u201d a device designed to give fact-checks greater purchase in algorithmically governed media networks by driving adoption of a new data standard called ClaimReview. We show how \u201cstructured journalism\u201d gave journalists a language for the social and technical challenges involved, and how this infrastructural technology mediates between fact-checkers, audiences, and platform companies. We argue that this standard-setting initiative exhibits both promotional and disciplining facets, offering greater distribution and impact to journalists while also defining their work in specific ways. Crucially, in this case, this disciplining influence reflects internal professional-institutional agendas in an emerging subfield of journalism as much as the demands of platform companies.", "venue": "New Media Soc.", "year": 2020, "referenceCount": 60, "citationCount": 9, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3374926", "name": "L. Graves"}, {"authorId": "31669747", "name": "C. Anderson"}]}}, {"contexts": ["coupling\" whereby the everyday practices of the courtroom diverge from state and federal laws regarding pretrial release [9]."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "d5374f6e3ae5ee2a22e993b70ceb27c41ac34d05", "externalIds": {"DBLP": "conf/fat/BarabasDRD20", "MAG": "3003599720", "DOI": "10.1145/3351095.3372859"}, "url": "https://www.semanticscholar.org/paper/d5374f6e3ae5ee2a22e993b70ceb27c41ac34d05", "title": "Studying up: reorienting the study of algorithmic fairness around issues of power", "abstract": "Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of \"studying up\". We reflect on the contributions that the call to \"study up\" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation \"upward\". A case study from our own work illustrates what it looks like to reorient one's research questions \"up\" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that \"study up\". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.", "venue": "FAT*", "year": 2020, "referenceCount": 61, "citationCount": 24, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "32412279", "name": "Chelsea Barabas"}, {"authorId": "145194095", "name": "Colin Doyle"}, {"authorId": "1404353737", "name": "J. Rubinovitz"}, {"authorId": "2782537", "name": "Karthik Dinakar"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "d66ddf2ee8fbd06690707625d798e1df0abc7438", "externalIds": {"DOI": "10.1016/b978-0-08-102295-5.10509-8"}, "url": "https://www.semanticscholar.org/paper/d66ddf2ee8fbd06690707625d798e1df0abc7438", "title": "Algorithmic Governance", "abstract": null, "venue": "International Encyclopedia of Human Geography", "year": 2020, "referenceCount": 17, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "117496394", "name": "Joe Blankenship"}]}}, {"contexts": ["\u2026on human distrust of algorithms dates back to as early as Meehl (1954), and it has been confirmed across many contexts (Grove and Meehl 1996, Grove et al. 2000, Sanders and Manrodt 2003, Fildes and Goodwin 2007, Vrieze and Grove 2009, Dietvorst, Simmons, and Massey, 2016, Christin, 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "d9b53a54ef7a3c4e3372bef10f08a3d0691df4fc", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/d9b53a54ef7a3c4e3372bef10f08a3d0691df4fc", "title": "Algorithm-Augmented Work Performance and Domain Experience: The Countervailing Forces of Ability and Aversion", "abstract": "How does a knowledge worker\u2019s level of domain experience affect their algorithm-augmented work performance? We propose and test theoretical predictions that domain experience has countervailing effects on algorithm-augmented performance: on one hand, domain experience enhances a worker\u2019s ability to accurately assess the quality of an algorithmic tool\u2019s advice; on the other hand, highly experienced workers exhibit more aversion to algorithmic advice, relative to their own judgment. We exploit a within-subjects experiment in which corporate IT support workers were assigned to resolve similar problems both manually (using their own judgment) and using advice generated by an algorithmic tool. Relative to solving problems using their own judgment, we confirm an inverted U-shape between IT domain experience and performance for problems where the algorithmic tool generated advice. While low experience workers\u2019 propensity to reject accurate algorithmic advice appears to be driven by lack of ability to accurately assess the algorithm\u2019s advice, highly experienced workers appear to reject algorithmic advice due to algorithmic aversion.", "venue": "", "year": 2020, "referenceCount": 78, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2106735100", "name": "Ryan Allen"}, {"authorId": "144354441", "name": "Prithwiraj Choudhury"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "dbf2a4d2fc974b70612b285addd591e5b92f4a4d", "externalIds": {"DOI": "10.1002/9781119721765.refs"}, "url": "https://www.semanticscholar.org/paper/dbf2a4d2fc974b70612b285addd591e5b92f4a4d", "title": "References", "abstract": null, "venue": "Quantifying Human Resources", "year": 2020, "referenceCount": 178, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": null, "authors": []}}, {"contexts": ["This in-depth qualitative material is essential to move beyond hype and commercial statements made by Big Data analytics companies; it allows to understand \u201calgorithms in practice\u201d (Christin, 2017), i.e. the material settings in which they are conceived, experimented and interpreted.", "I thus study predictive marketing as an activity, inscribed in an ecology of practices, rooted within organized work collectives (Christin, 2017; Jaton, 2017).", "\u2026why algorithms can predict consumer behavior is because their conception and interpretation are continuously nurtured by exogenous epistemologies of the consumer, produced at the interplay of working collectives, routines and inherited data infrastructures (Bowker and Star, 1999; Christin, 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "e14d712248b890f71070d25414cf83ad3cad4a8d", "externalIds": {"MAG": "3086840123", "DOI": "10.1177/2053951720951581"}, "url": "https://www.semanticscholar.org/paper/e14d712248b890f71070d25414cf83ad3cad4a8d", "title": "Mass personalization: Predictive marketing algorithms and the reshaping of consumer knowledge", "abstract": "This paper focuses on the conception and use of machine-learning algorithms for marketing. In the last years, specialized service providers as well as in-house data scientists have been increasingly using machine learning to predict consumer behavior for large companies. Predictive marketing thus revives the old dream of one-to-one, perfectly adjusted selling techniques, now at an unprecedented scale. How do predictive marketing devices change the way corporations know and model their customers? Drawing from STS and the sociology of quantification, I propose to study the original ambivalence that characterizes the promise of a mass personalization, i.e. algorithmic processes in which the precise adjustment of prediction to unique individuals involves the computation of massive datasets. By studying algorithms in practice, I show how the active embedding of local preexisting consumer knowledge and punctual de-personalization mechanisms are keys to the epistemic and organizational success of predictive marketing. This paper argues for the study of algorithms in their contexts and suggests new perspectives on algorithmic objectivity.", "venue": "", "year": 2020, "referenceCount": 116, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2080311622", "name": "Baptiste Kotras"}]}}, {"contexts": ["In the criminal justice setting, past work indeed suggests that judges may not apply the recommendations of risk assessments in a consistent manner (Christin, 2017; DeMichele et al., 2018; Stevenson, 2018)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "e35f3df7c1ce45f5327229586bfb712435b6ddd8", "externalIds": {"MAG": "3031124195", "DOI": "10.1111/rssa.12576"}, "url": "https://www.semanticscholar.org/paper/e35f3df7c1ce45f5327229586bfb712435b6ddd8", "title": "Simple rules to guide expert classifications", "abstract": "Judges, doctors and managers are among those decision makers who must often choose a course of action under limited time, with limited knowledge and without the aid of a computer. Because data-driven methods typically outperform unaided judgements, resourceconstrained practitioners can benefit from simple, statistically derived rules that can be applied mentally. In this work, we formalize long-standing observations about the efficacy of improper linear models to construct accurate yet easily applied rules. To test the performance of this approach, we conduct a large-scale evaluation in 22 domains and focus in detail on one: judicial decisions to release or detain defendants while they await trial. In these domains, we find that simple rules rival the accuracy of complex prediction models that base decisions on considerably more information. Further, comparing with unaided judicial decisions, we find that simple rules substantially outperform the human experts. To conclude, we present an analytical framework that sheds light on why simple rules perform as well as they do.", "venue": "", "year": 2020, "referenceCount": 74, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3163252", "name": "Jongbin Jung"}, {"authorId": "90265258", "name": "Connor Concannon"}, {"authorId": "33215252", "name": "Ravi Shroff"}, {"authorId": "143802734", "name": "Sharad Goel"}, {"authorId": "2463533", "name": "D. Goldstein"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "e8238b82b4b217865a10ab5e9ad505e5820dd849", "externalIds": {"MAG": "3009701942", "DOI": "10.1093/socpro/spaa004"}, "url": "https://www.semanticscholar.org/paper/e8238b82b4b217865a10ab5e9ad505e5820dd849", "title": "Technologies of Crime Prediction: The Reception of Algorithms in Policing and Criminal Courts", "abstract": "The number of predictive technologies used in the U.S. criminal justice system is on the rise. Yet there is little research to date on the reception of algorithms in criminal justice institutions. We draw on ethnographic fieldwork conducted within a large urban police department and a midsized criminal court to assess the impact of predictive technologies at different stages of the criminal justice process. We first show that similar arguments are mobilized to justify the adoption of predictive algorithms in law enforcement and criminal courts. In both cases, algorithms are described as more objective and efficient than humans\u2019 discretionary judgment. We then study how predictive algorithms are used, documenting similar processes of professional resistance among law enforcement and legal professionals. In both cases, resentment toward predictive algorithms is fueled by fears of deskilling and heightened managerial surveillance. Two practical strategies of resistance emerge: footdragging and data obfuscation. We conclude by discussing how predictive technologies do not replace, but rather displace discretion to less visible\u2014and therefore less accountable\u2014 areas within organizations, a shift which has important implications for inequality and the administration of justice in the age of big data. K E Y W O R D S : algorithms; prediction; policing; criminal courts; ethnography. In recent years, algorithms and artificial intelligence have attracted a great deal of scholarly and journalistic attention. Of particular interest is the development of predictive technologies designed to estimate the likelihood of a future event, such as the probability that an individual will default on a loan, the likelihood that a consumer will buy a specific product online, or the odds that a job candidate will have a long tenure in an organization. Predictive algorithms capture the imagination of scholars and journalists alike, in part because they raise the question of automated judgment: the replacement \u2013 or at least the augmentation \u2013 of human discretion by mechanical procedures. Nowhere are these The authors contributed equally and are listed alphabetically. We would like to thank the three anonymous reviewers, as well as the organizers and participants of the Willen Seminar at Barnard College in 2016, \u201cPunishment, Society, and Technology\u201d session of the LSA Annual Meeting in 2018, and \u201cInnovations and Technology in Studies of Crime and Social Control\u201d session of the ASA Annual Meeting in 2018 for their helpful comments and feedback. Please direct correspondence to Sarah Brayne at the Department of Sociology at the University of Texas at Austin, 305 E. 23rd Street, A1700, RLP 3.306, Austin, TX 78712; telephone (512) 475-8641; email sbrayne@utexas.edu. VC The Author(s) 2020. Published by Oxford University Press on behalf of the Society for the Study of Social Problems. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com. 1 Social Problems, 2020, 0, 1\u201317 doi: 10.1093/socpro/spaa004 Article D ow naded rom http/academ ic.p.com /socpro/advance-articleoi/10.1093/socpro/spaa004/5782114 by Snford U niersity user on 06 M arch 2020 questions more salient than in the context of criminal justice. Over recent decades, the U.S. criminal justice system has witnessed a proliferation of algorithmic technologies. Police departments now increasingly rely on predictive software programs to target potential victims and offenders and predict when and where future crimes are likely to occur (Brayne 2017; Ferguson 2017). Likewise, criminal courts use multiple predictive instruments, called \u201crisk-assessment tools,\u201d to assess the risk of recidivism or failure to appear in court among defendants (Hannah-Moffat 2018; Harcourt 2006; Monahan and Skeem 2016). Predictive technologies, in turn, raise many questions about fairness and inequality in criminal justice. On the positive side, advocates emphasize the benefits of using \u201csmart statistics\u201d to reduce crime and improve a dysfunctional criminal justice system characterized by racial discrimination and mass incarceration (Brantingham, Valasik, and Mohler 2018; Milgram 2012). On the negative side, critics argue that algorithms tend to embed bias and reinforce social and racial inequalities, rather than reducing them (Benjamin 2019; Eubanks 2018; O\u2019Neil 2016). They note that predictive algorithms draw on variables or proxies that are unfair and may be unconstitutional (Ferguson 2017; Starr 2014). Many point out that predictive algorithms may lead individuals to be surveilled and detained based on crimes they have not committed yet, frequently comparing these technologies to the science-fiction story Minority Report by Philip K. Dick and its movie adaptation, which evoke a dystopian future. To date, studies of criminal justice algorithms share three main characteristics. First, existing work tends to focus on the construction of algorithms, highlighting the proprietary aspect of most of these tools (which are often built by private companies) and criticizing their opacity (Angwin et al. 2016; Pasquale 2015; Wexler 2017). Second, they tend to treat the criminal justice system as a monolith, lumping together the cases of law enforcement, adjudication, sentencing, and community supervision (O\u2019Neil 2016; Scannell 2016). Third, and most importantly, most studies fail to analyze contexts of reception, implicitly assuming \u2013 usually without empirical data \u2013 that police officers, judges, and prosecutors rely uncritically on what algorithms direct them to do in their daily routines (Harcourt 2006; Hvistendahl 2016; Mohler et al. 2015; Uchida and Swatt 2013). In this article, we adopt a different perspective. Building on a growing body of literature that analyzes the impact of big data in criminal justice (Hannah-Moffat, Maurutto, and Turnbull 2009; Lageson 2017; Lum, Koper, and Willis 2017; Sanders, Weston, and Schott 2015; Stevenson and Doleac 2018), as well as existing ethnographic work on the uses of algorithms (Brayne 2017; Christin 2017; Levy 2015; Rosenblat and Stark 2016; Shestakovsky 2017), we focus on the reception of predictive algorithms in different segments of the criminal justice system. Drawing on two indepth ethnographic studies \u2013 one conducted in a police department and the other in a criminal court \u2013 we examine two questions. First, to what extent does the adoption of predictive algorithms affect work practices in policing and criminal courts? Second, how do practitioners respond to algorithmic technologies (i.e., do they embrace or contest them)? Based on this ethnographic material, this article provides several key findings. First, we document a widespread \u2013 albeit uneven \u2013 use of big data technologies on the ground. In policing, big data are used for both person-based and place-based predictive identification, in addition to risk management, crime analysis, and investigations. In criminal courts, multiple predictive instruments, complemented by digital case management systems, are employed to quantify the risk of the defendants. Second, similar arguments are used in policing and courts to justify the use of predictive technologies. In both cases, algorithms are presented as more rational and objective than \u201cgut feelings\u201d or discretionary judgments. Third, we find similar strategies of resistance, fueled by fears of experiential devaluation and increased managerial surveillance, among law enforcement and legal professionals\u2014most importantly, foot-dragging and data obfuscation. Despite these resemblances, we document important differences between our two cases. In particular, law enforcement officers were under more direct pressure to use the algorithms, whereas the legal professionals under consideration were able to keep their distance and ignore predictive technologies without consequences, a finding we relate to the 2 Brayne and Christin D ow naded rom http/academ ic.p.com /socpro/advance-articleoi/10.1093/socpro/spaa004/5782114 by Snford U niersity user on 06 M arch 2020 distinct hierarchical structures and levels of managerial oversight of the police department and criminal court we compared. We conclude by discussing the implications of these findings for research on technology and inequality in criminal justice. Whereas the current wave of critical scholarship on algorithmic bias often leans upon technological deterministic narratives in order to make social justice claims, here we focus on the social and institutional contexts within which such predictive systems are deployed and negotiated. In the process, we show that these tools acquire political nuance and meaning through practice, which can lead to unanticipated or undesirable outcomes: forms of workplace surveillance and the displacement of discretion to less accountable places. We argue that this sheds new light on the transformations of police and judicial discretion \u2013 with important consequences for social and racial inequality \u2013 in the age of big data. D E C I S I O N M A K I N G A C R O S S A V A R I E T Y O F D O M A I N S As a growing number of daily activities now take place online, an unprecedented amount of digital information is being collected, stored, and analyzed, making it possible to aggregate data across previously separate institutional settings. Harnessing this rapidly expanding corpus of digitized information, algorithms \u2013 broadly defined here as \u201c[a] formally specified sequence(s) of logical operations that provides step-by-step instructions for computers to act on data and thus automate decisions\u201d (Barocas et al. 2014) \u2013 are being used to guide decision-making across institutional domains as varied as education, journalism, credit, and criminal justice (Brayne 2017; Christin 2018; Fourcade and Healy 2017; O\u2019Neil 2016; Pasquale 2015). Advocates for algorithmic technologies argue that by relying on \u201cunbiased\u201d assessments, algorithms may help deploy resources more efficiently and objective", "venue": "", "year": 2020, "referenceCount": 118, "citationCount": 27, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "100470599", "name": "Sarah Brayne"}, {"authorId": "3379909", "name": "Ang\u00e8le Christin"}]}}, {"contexts": ["processes have been introduced into workplaces (Christin, 2017).", "\u2026perceived (and impossible) promises of the technologies themselves, but must situate the imperative logic of pre-emption in relation to the imaginaries, identities, practices and surroundings of those who rely on automated data processes in their work and lives (cf. Dencik, 2019; Christin, 2017).", "In grappling with the temporal logics of data analytics, we cannot confine ourselves to engaging solely with the perceived (and impossible) promises of the technologies themselves, but must situate the imperative logic of pre-emption in relation to the imaginaries, identities, practices and surroundings of those who rely on automated data processes in their work and lives (cf. Dencik, 2019; Christin, 2017).", "In part, they emerge from a potential existential threat to the livelihoods of police officers, familiar in many other settings where automation\nprocesses have been introduced into workplaces (Christin, 2017)."], "isInfluential": true, "intents": ["background"], "citingPaper": {"paperId": "fdd1e839da5572c7d102b428a3c6698ffe7c1fb5", "externalIds": {"DBLP": "journals/nms/AndrejevicDT20", "MAG": "2981402113", "DOI": "10.1177/1461444820913565"}, "url": "https://www.semanticscholar.org/paper/fdd1e839da5572c7d102b428a3c6698ffe7c1fb5", "title": "From pre-emption to slowness: Assessing the contrasting temporalities of data-driven predictive policing", "abstract": "Debates on the temporal shift associated with digitalization often stress notions of speed and acceleration. With the advent of big data and predictive analytics, the time-compressing features of digitalization are compounded within a distinct operative logic: that of pre-emption. The temporality of pre-emption attempts to project the past into a simulated future that can be acted upon in the present; a temporality of pure imminence. Yet, inherently paradoxical, pre-emption is marked by myriads of contrasts and frictions as it is caught between the supposedly all-encompassing knowledge of the data-processing \u2018Machine\u2019, and the daily reality of decision-making practices by relevant social actors. In this article, we explore the contrasting temporalities of automated data processing and predictive analytics, using policing as an illustrative example. Drawing on insights from two cases of predictive policing systems that have been implemented among UK police forces, we highlight the prevalence of counter-temporalities as predictive analytics is situated in institutional contexts and consider the conditions of possibility for agency and deliberation. Analysing these temporal tensions in relation to \u2018slowness\u2019 as a mode of resistance, the contextual examination of predictive policing advanced in the article provides a contribution to the formation of a deeper awareness of the politics of time in automated data processing; one that may serve to counter the imperative of pre-emption that, taken to the limit, seeks to foreclose the time for politics, action and life.", "venue": "New Media Soc.", "year": 2020, "referenceCount": 94, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50056732", "name": "M. Andrejevic"}, {"authorId": "2014271", "name": "Lina Dencik"}, {"authorId": "40858316", "name": "Emiliano Trer\u00e9"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "fe3876782e14092738f0253c34808eb0d2a98ecb", "externalIds": {"MAG": "3035958622", "DOI": "10.1080/10439463.2020.1788026"}, "url": "https://www.semanticscholar.org/paper/fe3876782e14092738f0253c34808eb0d2a98ecb", "title": "Rationing bytes: managing demand for digital forensic examinations", "abstract": "ABSTRACT With the growing sophistication and prevalence of digital devices such as mobile phones, computers, tablets, sat-navs, and domestic appliances, the extraction, analysis and interpretation of digital data has become increasingly central to intelligence gathering and criminal proceedings. However, the very extent of data available today challenges the ability of police agencies to turn seized devices into useful evidence. To date, most social science scholarship about forensics has concentrated on DNA profiling and its societal and ethical issues. In contrast, other forensic fields, including digital forensics, have had little analytical scrutiny. Based on unprecedented access to a forensic collaboration in England, this study addresses the question: In conditions of constrained resources, how do police agencies manage the insatiable demand for digital examinations? In doing so, we bring rationing classification schemes from healthcare studies into the field of criminology in order to characterise the techniques for reconciling demand with capacity. As detailed, formal attempts to ration demand are confounded by informal practices and procedures that can impact on the capacity of the workforce and the speed with which cases are processed. In addition, the rationing of digital devices has significant consequences for the definition and distribution of skills and expertise across criminal justice agencies.", "venue": "", "year": 2020, "referenceCount": 48, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "5641880", "name": "B. Rappert"}, {"authorId": "13069216", "name": "Hannah Wheat"}, {"authorId": "1398807747", "name": "Dana Wilson-Kovacs"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "0396cd8027b474e2fbed9358211eab539c7e7f1e", "externalIds": {"DBLP": "conf/chi/InkpenCCVB19", "MAG": "2942542459", "DOI": "10.1145/3290607.3299002"}, "url": "https://www.semanticscholar.org/paper/0396cd8027b474e2fbed9358211eab539c7e7f1e", "title": "Where is the Human?: Bridging the Gap Between AI and HCI", "abstract": "In recent years, AI systems have become both more powerful and increasingly promising for integration in a variety of application areas. Attention has also been called to the social challenges these systems bring, particularly in how they might fail or even actively disadvantage marginalised social groups, or how their opacity might make them difficult to oversee and challenge. In the context of these and other challenges, the roles of humans working in tandem with these systems will be important, yet the HCI community has been only a quiet voice in these debates to date. This workshop aims to catalyse and crystallise an agenda around HCI's engagement with AI systems. Topics of interest include explainable and explorable AI; documentation and review; integrating artificial and human intelligence; collaborative decision making; AI/ML in HCI Design; diverse human roles and relationships in AI systems; and critical views of AI.", "venue": "CHI Extended Abstracts", "year": 2019, "referenceCount": 52, "citationCount": 23, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1781500", "name": "K. Quinn"}, {"authorId": "2510840", "name": "Stevie Chancellor"}, {"authorId": "2583473", "name": "M. D. Choudhury"}, {"authorId": "33699440", "name": "Michael Veale"}, {"authorId": "144392313", "name": "E. Baumer"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "1aa414a820fdf1e5a846af09aa95caa5ed02a31f", "externalIds": {"DBLP": "journals/corr/abs-2004-07132", "MAG": "3017057981", "ArXiv": "2004.07132", "DOI": "10.2139/ssrn.3723046"}, "url": "https://www.semanticscholar.org/paper/1aa414a820fdf1e5a846af09aa95caa5ed02a31f", "title": "Hiring Fairly in the Age of Algorithms", "abstract": "Widespread developments in automation have reduced the need for human input. However, despite the increased power of machine learning, in many contexts these programs make decisions that are problematic. Biases within data and opaque models have amplified human prejudices, giving rise to such tools as Amazon's (now defunct) experimental hiring algorithm, which was found to consistently downgrade resumes when the word \"women's\" was added before an activity. This article critically surveys the existing legal and technological landscape surrounding algorithmic hiring. We argue that the negative impact of hiring algorithms can be mitigated by greater transparency from the employers to the public, which would enable civil advocate groups to hold employers accountable, as well as allow the U.S. Department of Justice to litigate. Our main contribution is a framework for automated hiring transparency, algorithmic transparency reports, which employers using automated hiring software would be required to publish by law. We also explain how existing regulations in employment and trade secret law can be extended by the Equal Employment Opportunity Commission and Congress to accommodate these reports.", "venue": "SSRN Electronic Journal", "year": 2020, "referenceCount": 22, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1633001501", "name": "Max Langenkamp"}, {"authorId": "2118819663", "name": "Allan Costa"}, {"authorId": "121779354", "name": "C. Y. Cheung"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "20452754227ec9d48ee3c68f45b6eb64b6766a74", "externalIds": {"MAG": "2981686111", "DOI": "10.1002/bdm.2155"}, "url": "https://www.semanticscholar.org/paper/20452754227ec9d48ee3c68f45b6eb64b6766a74", "title": "A systematic review of algorithm aversion in augmented decision making", "abstract": null, "venue": "Journal of Behavioral Decision Making", "year": 2020, "referenceCount": 98, "citationCount": 67, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145694598", "name": "Jason Burton"}, {"authorId": "152572847", "name": "Mari-Klara Stein"}, {"authorId": "34512911", "name": "T. Jensen"}]}}, {"contexts": ["Given the gap between marketing pitches promoting algorithmic systems and the realities of use, Christin (2017) calls for these to be \u2018\u2018decoupled\u2019\u2019 via ethnographic work on algorithmic systems use in practice to better understand their effects on the ground."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "29a1282e98b7e1830a77c0550a67bdda49ffee58", "externalIds": {"MAG": "2976766564", "DOI": "10.1177/2053951719868492"}, "url": "https://www.semanticscholar.org/paper/29a1282e98b7e1830a77c0550a67bdda49ffee58", "title": "Municipal surveillance regulation and algorithmic accountability", "abstract": "A wave of recent scholarship has warned about the potential for discriminatory harms of algorithmic systems, spurring an interest in algorithmic accountability and regulation. Meanwhile, parallel concerns about surveillance practices have already led to multiple successful regulatory efforts of surveillance technologies\u2014many of which have algorithmic components. Here, we examine municipal surveillance regulation as offering lessons for algorithmic oversight. Taking the 2017 Seattle Surveillance Ordinance as our primary case study and surveying efforts across five other cities, we describe the features of existing surveillance regulation; including procedures for describing surveillance technologies in detail, requirements for public engagement, and processes for establishing acceptable uses. Although the Seattle Surveillance Ordinance was not intended to address algorithmic accountability, we find these considerations to be relevant to the law\u2019s aim of surfacing disparate impacts of systems in use. We also find that in notable cases government employees did not identify regulated algorithmic surveillance technologies as reliant on algorithmic or machine learning systems, highlighting definitional gaps that could hinder future efforts toward algorithmic regulation. We argue that (i) finer-grained distinctions between types of information systems in the language of law and policy, and (ii) risk assessment tools integrated into their implementation would strengthen future regulatory efforts by rendering underlying algorithmic components more legible and accountable to political and community stakeholders.", "venue": "Big Data & Society", "year": 2019, "referenceCount": 71, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "47533677", "name": "Meg Young"}, {"authorId": "2495939", "name": "Michael A. Katell"}, {"authorId": "143782314", "name": "P. Krafft"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "2e3fd6c50bf301677a6bc1d93a37841c640e4e75", "externalIds": {"MAG": "2979893369", "DBLP": "journals/pacmhci/GreenC19", "DOI": "10.1145/3359152"}, "url": "https://www.semanticscholar.org/paper/2e3fd6c50bf301677a6bc1d93a37841c640e4e75", "title": "The Principles and Limits of Algorithm-in-the-Loop Decision Making", "abstract": "The rise of machine learning has fundamentally altered decision making: rather than being made solely by people, many important decisions are now made through an \"algorithm-in-the-loop'' process where machine learning models inform people. Yet insufficient research has considered how the interactions between people and models actually influence human decisions. Society lacks both clear normative principles regarding how people should collaborate with algorithms as well as robust empirical evidence about how people do collaborate with algorithms. Given research suggesting that people struggle to interpret machine learning models and to incorporate them into their decisions---sometimes leading these models to produce unexpected outcomes---it is essential to consider how different ways of presenting models and structuring human-algorithm interactions affect the quality and type of decisions made. This paper contributes to such research in two ways. First, we posited three principles as essential to ethical and responsible algorithm-in-the-loop decision making. Second, through a controlled experimental study on Amazon Mechanical Turk, we evaluated whether people satisfy these principles when making predictions with the aid of a risk assessment. We studied human predictions in two contexts (pretrial release and financial lending) and under several conditions for risk assessment presentation and structure. Although these conditions did influence participant behaviors and in some cases improved performance, only one desideratum was consistently satisfied. Under all conditions, our study participants 1) were unable to effectively evaluate the accuracy of their own or the risk assessment's predictions, 2) did not calibrate their reliance on the risk assessment based on the risk assessment's performance, and 3) exhibited bias in their interactions with the risk assessment. These results highlight the urgent need to expand our analyses of algorithmic decision making aids beyond evaluating the models themselves to investigating the full sociotechnical contexts in which people and algorithms interact.", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2019, "referenceCount": 91, "citationCount": 68, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "87122962", "name": "Ben Green"}, {"authorId": "117745152", "name": "Yiling Chen"}]}}, {"contexts": ["Indeed, both online journalists and marketers have interests in exploring reader expectations and preferences (about when and/or what information to pass along) through the lens of these numbers (Christin, 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "5c8da751872d658bbe62682fb61dc9b4eb5ac47c", "externalIds": {"MAG": "2984189769", "DOI": "10.1080/16522354.2019.1689766"}, "url": "https://www.semanticscholar.org/paper/5c8da751872d658bbe62682fb61dc9b4eb5ac47c", "title": "News website personalisation: the co-creation of content, audiences and services by online journalists and marketers", "abstract": "ABSTRACT This paper investigates news website personalisation from a socio-economic perspective to help understand the role of journalists and marketers in online service provision. It focuses on a Belgian company that publishes the two major financial and economic newspapers in the country. Debates and day-to-day practices regarding personalisation in the company show that two professional groups are involved in the qualification of media products (content and audiences) into media goods and audience segments. In a context where media organisations explore alternative forms of service provision, this study makes an original contribution by pointing out the increase shared roles between online journalists and marketers, especially regarding gatekeeping and audience segmentation practices.", "venue": "Journal of Media Business Studies", "year": 2019, "referenceCount": 103, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Business"], "authors": [{"authorId": "2094797467", "name": "Sylvain Malcorps"}]}}, {"contexts": ["This documented goal of attracting traffic (Christin 2017; 2018; Cohen 2018; Nelson\n2018) rests in ad revenue mechanisms that have evolved to prioritise impressions or clicks on a page (Banerjee 2017; Powers 2018; Wang 2018).", "\u2026into analytics\n45 Including Altheide 1976; Schultz 2007; Anderson 2011; Groves and Brown 2011; Usher 2013; Tandoc 2014; MacGregor 2007; 2014; Christin 2015; 2017; 2018; Bunce 2017; Duffy et al. 2017; Wolfgang 2018.\npractice at a variety of content providers in a rapidly changing media\u2026", "Further, Christin (2017) suggests that the more traditional ethnographic research designs that \u201csacrifice breadth for depth, focusing on a single, well-bounded site for long periods of time\u201d (p.7) are not conducive to studies based on quickly changing technology, whereas multi-sites can be."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "5caec8b4f775e5196f9a734de27d1e8ea4d63855", "externalIds": {"MAG": "2939335237"}, "url": "https://www.semanticscholar.org/paper/5caec8b4f775e5196f9a734de27d1e8ea4d63855", "title": "Metrics and analytics in the newsroom : an ethnographic study exploring how audience data are changing journalistic practice", "abstract": "The use of metrics and analytics is now embedded in and directly impacting newsroom practice and routines. As audience data are used to shape and promote content that help form societal narratives, development of best practice is crucial, not only to enhance fulsome public discourse but as a means of reputation building for media outlets fighting to retain relevance and public trust, both of which are intrinsically tied to revenue and/or funding. This thesis explores the potential conflicts between journalism\u2019s mandate to keep the public informed through quality, contextualised news coverage and the use of metrics and analytics to build scale and a sustainable business model. Empirical research is based on ethnographic observation in four news organisations on two continents in three different countries: Norway\u2019s national broadcaster, NRK, which has developed its own analytics system that uses both qualitative and quantitative data; The Canadian Press, Canada\u2019s national news agency, which is exploring ways to track how its content is being used with little direct access to audience data; The Hamilton Spectator, a local newspaper in Canada making the shift from print to digital; and a similarly sized and situated local paper in the United Kingdom, The Bournemouth Daily Echo. Participant observation and interviews were used to investigate how metrics and analytics impact newsroom routines; how journalists feel their work is impacted by the use of audience data; and how practices pertaining to the use of metrics and analytics are challenging the boundaries of journalism. The thesis employs a bricolage of theories within a sociological framework, through the lens of media logic, and draws on the author\u2019s own perspective of working in a newsroom and, currently, in an academic media faculty. The research provides observed examples of the ways in which changing boundaries are impacting definitions of journalism and who is a journalist; it proposes best practice for the use of metrics and analytics in newsrooms that might better situate media outlets to serve their communities and survive in a rapidly changing media landscape; it offers suggestions for media scholars on best practice to perform research that better reflects newsroom routines particular to the use of metrics and analytics; the thesis contributes a new gatekeeping model that identifies two primary channels related specifically to the use of metrics and analytics: promotional and developmental; finally, the thesis demonstrates how a bricolage of complementary theories and the selection of multiple sites of study might best support the reflexive investigation of complex social structures within a rapidly changing field.", "venue": "", "year": 2019, "referenceCount": 269, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "116903390", "name": "Nicole Blanchett Neheli"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "60b65ef108e37192c51eaf57407e645079e3c11d", "externalIds": {"MAG": "2956739106", "DOI": "10.14763/2019.2.1413"}, "url": "https://www.semanticscholar.org/paper/60b65ef108e37192c51eaf57407e645079e3c11d", "title": "The \u2018golden view\u2019: data-driven governance in the scoring society", "abstract": "Drawing on the first comprehensive investigation into the uses of data analytics in UK public services, this article outlines developments and practices surrounding the upsurge in data-driven forms of what we term \u2018citizen scoring\u2019. This refers to the use of data analytics in government for the purposes of categorisation, assessment and prediction at both individual and population level. Combining Freedom of Information requests and semi-structured interviews with public sector workers and civil society organisations, we detail the practices surrounding these developments and the nature of concerns expressed by different stakeholder groups as a way to elicit the heterogeneity, tensions and negotiations that shape the contemporary landscape of data-driven governance. Described by practitioners as a way to achieve a \u2018golden view\u2019 of populations, we argue that data systems need to be situated in this context in order to understand the wider politics of such a \u2018view\u2019 and the implications this has for state-citizen relations in the scoring society.", "venue": "Internet Policy Review", "year": 2019, "referenceCount": 28, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "2014271", "name": "Lina Dencik"}, {"authorId": "39785277", "name": "J. Redden"}, {"authorId": "2608327", "name": "A. Hintz"}, {"authorId": "121717977", "name": "Harry Warne"}]}}, {"contexts": ["Christin [15] studies the use of algorithms in the criminal justice and web journalism domains, finding that the intended and actual use of algorithms often differ, and that practitioners often develop strategies for minimizing the impact of algorithmic input in their daily work."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "62ee0d3b7baee5f1f8924b684504bfcbd7544d20", "externalIds": {"MAG": "2974075815", "DOI": "10.2139/ssrn.3465622"}, "url": "https://www.semanticscholar.org/paper/62ee0d3b7baee5f1f8924b684504bfcbd7544d20", "title": "Human Decision Making with Machine Assistance: An Experiment on Bailing and Jailing", "abstract": "Much of political debate focuses on the concern that machines might take over. Yet in many domains it is much more plausible that the ultimate choice and responsibility remain with a human decision-maker, but that she is provided with machine advice. A quintessential illustration is the decision of a judge to bail or jail a defendant. In multiple jurisdictions in the US, judges have access to a machine prediction about a defendant\u2019s recidivism risk. In our study, we explore how receiving machine advice influences people\u2019s bail decisions. \n \nWe run a vignette experiment with laypersons whom we test on a subsample of cases from the database of this prediction tool. In study 1, we ask them to predict whether defendants will recidivate before tried, and manipulate whether they have access to machine advice. We find that receiving machine advice has a small effect, which is biased in the direction of predicting no recidivism. \n \nIn the field, human decision makers sometimes have a chance, after the fact, to learn whether the machine has given good advice. In study 2, after each trial we inform participants of ground truth. This does not make it more likely that they follow the advice, despite the fact that the machine is (on average) slightly more accurate than real judges. This also holds if initially the advice is mostly correct, or if it initially is mostly to predict (no) recidivism. \n \nReal judges know that their decisions affect defendants\u2019 lives. They may also be concerned about reelection or promotion. Hence a lot is at stake. In study 3 we emulate high stakes by giving participants a financial incentive. An incentive to find the ground truth, or to avoid false positive or false negatives, does not make participants more sensitive to machine advice. But an incentive to follow the advice is effective.", "venue": "SSRN Electronic Journal", "year": 2019, "referenceCount": 74, "citationCount": 18, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "1380255001", "name": "Nina Grgic-Hlaca"}, {"authorId": "145574694", "name": "C. Engel"}, {"authorId": "1958921", "name": "K. Gummadi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "6d835df599995d0a3c6db26745d8667e2231371a", "externalIds": {"MAG": "2944420931", "DOI": "10.1080/1369118X.2019.1612933"}, "url": "https://www.semanticscholar.org/paper/6d835df599995d0a3c6db26745d8667e2231371a", "title": "The algorithm at work? Explanation and repair in the enactment of similarity in art data", "abstract": "ABSTRACT This paper examines the work practices involved in making data legible to machines and machine output legible to humans. The study is based on ethnographic research of a team of art experts at DNArt \u2013 a data classification system that features a growing database of art images, a classification scheme, a similarity matching algorithm, and a website that together serve as a consumer judgment device in an emerging online market for art. I analyze interactions from meeting observations, interviews, documentation, and online interaction data to show how non-technical art experts explain and repair sociotechnical breakdowns \u2013 when their expectations for similarity between art images and artists differ from the similarity relations produced by the algorithm. By repairing breakdowns, the art experts construct the algorithm anew, as a legitimate revealer of similarity in art. In doing so, the team's repair work is folded back into the black box of the algorithm, rendering it invisible and unacknowledged, sometimes even by the experts themselves.", "venue": "Information, Communication & Society", "year": 2020, "referenceCount": 42, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "33385802", "name": "S. Sachs"}]}}, {"contexts": ["Christin [15] studies the use of algorithms in the criminal justice and web journalism domains, finding that the intended and actual use of algorithms often differ, and that practitioners often develop strategies for minimizing the impact of algorithmic input in their daily work."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "7160abdb4fb0f3452ae8db470a481e930f3dd2eb", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/7160abdb4fb0f3452ae8db470a481e930f3dd2eb", "title": "Human Decision Making with Machine Advice : An Experiment on Bailing and Jailing", "abstract": "Much of political debate focuses on the concern that machines might take over. Yet in many domains it is much more plausible that the ultimate choice and responsibility remain with a human decision-maker, but that she is provided with machine advice. A quintessential illustration is the decision of a judge to bail or jail a defendant. In multiple jurisdictions in the US, judges have access to a machine prediction about a defendant\u2019s recidivism risk. In our study, we explore how receiving machine advice influences people\u2019s bail decisions. We run a vignette experiment with laypersons whom we test on a subsample of cases from the database of this prediction tool. In study 1, we ask them to predict whether defendants will recidivate before tried, and manipulate whether they have access to machine advice. We find that receiving machine advice has a small effect, which is biased in the direction of predicting no recidivism. In the field, human decision makers sometimes have a chance, after the fact, to learn whether the machine has given good advice. In study 2, after each trial we inform participants of ground truth. This does not make it more likely that they follow the advice, despite the fact that the machine is (on average) slightly more accurate than real judges. This also holds if initially the advice is mostly correct, or if it initially is mostly to predict (no) recidivism. Real judges know that their decisions affect defendants\u2019 lives. They may also be concerned about reelection or promotion. Hence a lot is at stake. In study 3 we emulate high stakes by giving participants a financial incentive. An incentive to find the ground truth, or to avoid false positive or false negatives, does not make participants more sensitive to machine advice. But an incentive to follow the advice is effective.", "venue": "", "year": 2019, "referenceCount": 60, "citationCount": 4, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1380255001", "name": "Nina Grgic-Hlaca"}, {"authorId": "145574694", "name": "C. Engel"}, {"authorId": "1958921", "name": "K. Gummadi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "7afbf3ee74f3f40395faf2efb9458cba5cfcdd84", "externalIds": {"DBLP": "journals/chb/NechushtaiL19", "MAG": "2886498471", "DOI": "10.1016/j.chb.2018.07.043"}, "url": "https://www.semanticscholar.org/paper/7afbf3ee74f3f40395faf2efb9458cba5cfcdd84", "title": "What kind of news gatekeepers do we want machines to be? Filter bubbles, fragmentation, and the normative dimensions of algorithmic recommendations", "abstract": "Abstract Machines are increasingly aiding or replacing humans in journalistic work, primarily in news distribution. We examined whether news recommendation engines contribute to filter bubbles and fragmented news audiences by asking a diverse set of real-world participants (N\u202f=\u202f168), using their personal Google accounts, to search Google News for news about Hillary Clinton and Donald Trump during the 2016 U.S. presidential campaign and report the first five stories they were recommended on each candidate. Users with different political leanings from different states were recommended very similar news, challenging the assumption that algorithms necessarily encourage echo chambers. Yet we also found a very high degree of homogeneity and concentration in the news recommendations. On average, the most recommended five news organizations comprised 69% of all recommendations. Five news organizations alone accounted for 49% of the total number of recommendations collected. Out of 14 organizations that dominated recommendations across the different searches, only three were born-digital, indicating that the news agenda constructed on Google News replicates traditional industry structures more than disrupts them. We use these findings to explore the challenges of studying machine behavior in news from a normative perspective, given the lack of agreed-upon normative standards for humans as news gatekeepers. This article suggests that because there is no one agreed-upon standard for humans as news gatekeepers, assessing the performance of machines in that role is doubly complicated.", "venue": "Comput. Hum. Behav.", "year": 2019, "referenceCount": 128, "citationCount": 68, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Political Science"], "authors": [{"authorId": "52023974", "name": "Efrat Nechushtai"}, {"authorId": "144282491", "name": "S. Lewis"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "8639fbd80ccfde2d3be43e01512bc6d809ea02aa", "externalIds": {"DBLP": "journals/pacmhci/VaranasiKD19", "MAG": "2988569850", "DOI": "10.1145/3359322"}, "url": "https://www.semanticscholar.org/paper/8639fbd80ccfde2d3be43e01512bc6d809ea02aa", "title": "How Teachers in India Reconfigure their Work Practices around a Teacher-Oriented Technology Intervention", "abstract": "The proliferation of mobile devices around the world, combined with falling costs of hardware and Internet connectivity, have resulted in an increasing number of organizations that work to introduce educational technology interventions into low-income schools in the Global South. However, to date, most prior HCI research examining such interventions has focused on interventions that target students. In this paper, we expand prior literature by examining an intervention, called Meghshala, that targets teachers in low-income schools as its primary users. Through interviews and observations with 39 participants from 12 government schools in India, we show how the introduction of a teacher-focused technology intervention causes teachers to reconfigure their work practices, including lesson preparation, in-classroom teaching practices, bureaucratic work processes, and post-teaching feedback mechanisms. We use the concept of material agency to analyze our findings with respect to teacher agency and reconfiguration, and use theories of teacher knowledge to highlight the kinds of knowledge production that teachers in our research context tend to focus on (e.g., content knowledge). Finally, we offer design opportunities for future teacher-focused technology interventions.", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2019, "referenceCount": 117, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "20611041", "name": "Rama Adithya Varanasi"}, {"authorId": "2633430", "name": "Ren\u00e9 F. Kizilcec"}, {"authorId": "2875293", "name": "Nicola Dell"}]}}, {"contexts": ["Academic studies that have analyzed journalists\u2019 use and perception of web analytics indicate that journalists recognize their utility but do not themselves fully embrace analytics as necessary to successfully produce news (Zamith 2018; Christin 2017)."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "8926e5076b1a0f48d85446c1f7084a087af3fd6a", "externalIds": {"MAG": "2969293083", "DOI": "10.1080/17512786.2019.1642132"}, "url": "https://www.semanticscholar.org/paper/8926e5076b1a0f48d85446c1f7084a087af3fd6a", "title": "The Two Faces of Janus: Web Analytics Companies and the Shifting Culture of News", "abstract": "In Greek mythology, the God of Transitions, Janus, is usually represented as having two faces: one looking at the future and the other at the past, illustrating a change from one condition to the o...", "venue": "Journalism Practice", "year": 2019, "referenceCount": 25, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["History"], "authors": [{"authorId": "1401937609", "name": "Val\u00e9rie B\u00e9lair-Gagnon"}, {"authorId": "1740991", "name": "A. Holton"}]}}, {"contexts": ["\u2026too have pushed back against the overreliance on algorithms as deficient compared to \u201cshoe-leather epistemologies\u201d (Lewis and Waters 2018), have found ways to subvert algorithmic systems (Christin 2017), or display some mixture of curiosity and skepticism (Thurman, D\u20acorr, and Kunert 2017).", "Although there are debates about which metrics are most adequate for capturing the audience\u2019s interests, the metrics themselves enter into the newsroom as objects that journalists then have to respond to \u2013 even if they ignore them (Anderson 2011; Christin 2017).", "In practice, the use of algorithms for sentencing has been more complex, often relying on inputs that reproduce inequality (O\u2019Neil 2016, pp. 24\u201327) as well as differences in how they are put into practice (Christin 2017).", "What we need in this space are strong data and analytics\u201d (quoted in Christin 2017, 6).", "Algorithms can be used to analyze data (Parasie 2015), write stories (D\u20acorr 2016), or provide newsrooms with metrics (Christin 2017)."], "isInfluential": true, "intents": ["methodology", "background"], "citingPaper": {"paperId": "918d4d2c13e0862cc61498c3886f1ab78d486307", "externalIds": {"MAG": "2935829555", "DOI": "10.1080/21670811.2019.1601577"}, "url": "https://www.semanticscholar.org/paper/918d4d2c13e0862cc61498c3886f1ab78d486307", "title": "News Algorithms, Photojournalism and the Assumption of Mechanical Objectivity in Journalism", "abstract": "Abstract This article interrogates the relationship between epistemic authority and journalistic technology through the perspective of mechanical objectivity\u2014a belief in technological systems capable of rendering a particular output in a manner that overcomes the limits of human subjectivity. By treating journalistic objectivity not as a stable referent, but as a contextual one prone to shifts in practices and understandings over time, it foregrounds how changing technologies of recording, creating, and distributing news content affect how journalistic objectivity is understood. Following this perspective, two technological practices are examined: photojournalism and algorithms. The development of photojournalism led to the prizing of news images as objective representations produced by the camera to the diminishment of human judgment. Similarly, the various outputs produced by news algorithms are accompanied by an orientation toward computational objectivity in contrast to human subjectivity. Exploring these dynamics sheds light on the ongoing relationship between news technologies and discourses of journalistic objectivity in the face of digital innovations in the production and circulation of news.", "venue": "Digital Journalism", "year": 2019, "referenceCount": 118, "citationCount": 22, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "145744244", "name": "M. Carlson"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "9562b5687c3ad0c9ff6a8b0fbef15542b5881980", "externalIds": {"MAG": "2983368686", "DOI": "10.22197/rbdpp.v5i3.290"}, "url": "https://www.semanticscholar.org/paper/9562b5687c3ad0c9ff6a8b0fbef15542b5881980", "title": "Introdu\u00e7\u00e3o \u00e0 intelig\u00eancia artificial e \u00e0 justi\u00e7a criminal na Europa", "abstract": "O presente artigo analisa a necessidade de iniciar uma discuss\u00e3o global e multidisciplinar em rela\u00e7\u00e3o \u00e0 interse\u00e7\u00e3o existente entre a intelig\u00eancia artificial e a justi\u00e7a penal, especialmente no contexto europeu. De fato, por diferentes raz\u00f5es, o direito penal \u00e9 considerado o campo no qual a estrutura computacional e a intelig\u00eancia artificial n\u00e3o podem obter uma aplica\u00e7\u00e3o direta e relevante. Ao contr\u00e1rio, constata-se uma urgente necessidade de iniciar uma reflex\u00e3o global sobre os efeitos em curto e longo prazo de tal tecnologia, que est\u00e1 remodelando todos os aspectos de nossa exist\u00eancia social, a come\u00e7ar pela justi\u00e7a. Nesse cen\u00e1rio delineado, o objetivo deste trabalho \u00e9 evidenciar os riscos mais relevantes atualmente existentes. Contudo, n\u00e3o h\u00e1 a ambi\u00e7\u00e3o de fornecer respostas, mas, ao contr\u00e1rio, expor perguntas espec\u00edficas sobre se e como a intelig\u00eancia artificial possa ser integrada nos sistemas europeus de justi\u00e7a penal. Remete-se a um estudo mais completo para qualquer tentativa de responder a tais questionamentos.", "venue": "Revista Brasileira de Direito Processual Penal", "year": 2019, "referenceCount": 51, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Political Science"], "authors": [{"authorId": "2098879133", "name": "Serena Quattrocolo"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "9dae5041ed97ab21f637fb282d758fd9cb771476", "externalIds": {"DBLP": "conf/fat/SelbstBFVV19", "MAG": "2897154134", "DOI": "10.1145/3287560.3287598"}, "url": "https://www.semanticscholar.org/paper/9dae5041ed97ab21f637fb282d758fd9cb771476", "title": "Fairness and Abstraction in Sociotechnical Systems", "abstract": "A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce \"fair\" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five \"traps\" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.", "venue": "FAT", "year": 2019, "referenceCount": 124, "citationCount": 282, "influentialCitationCount": 20, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "46432110", "name": "Andrew D. Selbst"}, {"authorId": "38818867", "name": "D. Boyd"}, {"authorId": "34597147", "name": "Sorelle A. Friedler"}, {"authorId": "1747652", "name": "Suresh Venkatasubramanian"}, {"authorId": "2896002", "name": "J. Vertesi"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "a93cfdf5f35ca5917e568ee1191f4b7c32edaeaf", "externalIds": {}, "url": "https://www.semanticscholar.org/paper/a93cfdf5f35ca5917e568ee1191f4b7c32edaeaf", "title": "Exploring Actor-Network Theory in the Investigation of Algorithms", "abstract": "This short paper, a submission for the HCI workshop \u2018Standing on the Shoulders of Giants\u2019, looks at scholarly studies of the presence of algorithmic selection in everyday life and how existing research benefits from and is connected to Actor-Network Theory (ANT). The paper then suggests that the empirical and philosophical strengths of ANT could benefit not just algorithms, but the broader field of HCI. With this exercise, this submission touches upon multiple key topics of the workshop. Most prominently, the paper addresses topics 2 and 6 by suggesting how ANT could connect to existing debates on, for example, \u2018post-userism\u2019. Also topic 5 is relevant, since the rather obscure philosophical underpinnings of ANT need to be made more accessible if we hope to make its approach useful for HCI.", "venue": "", "year": 2019, "referenceCount": 26, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "112892174", "name": "Elias Storms"}]}}, {"contexts": ["This includes practices through which algorithms are implemented (Christin, 2017), work on how\nalgorithms are reshaping observation (McQuillan, 2016) and rationality (Lowrie, 2017), while also addressing the relations between the algorithmic and the non-algorith-\nmic (Dourish, 2016)."], "isInfluential": false, "intents": ["methodology"], "citingPaper": {"paperId": "b1356473d2334ce620e691e7079859b7f1a08bc6", "externalIds": {"MAG": "2965459557", "DOI": "10.1177/2053951719863819"}, "url": "https://www.semanticscholar.org/paper/b1356473d2334ce620e691e7079859b7f1a08bc6", "title": "Algorithms as folding: Reframing the analytical focus", "abstract": "This article proposes an analytical approach to algorithms that stresses operations of folding. The aim of this approach is to broaden the common analytical focus on algorithms as biased and opaque black boxes, and to instead highlight the many relations that algorithms are interwoven with. Our proposed approach thus highlights how algorithms fold heterogeneous things: data, methods and objects with multiple ethical and political effects. We exemplify the utility of our approach by proposing three specific operations of folding\u2014proximation, universalisation and normalisation. The article develops these three operations through four empirical vignettes, drawn from different settings that deal with algorithms in relation to AIDS, Zika and stock markets. In proposing this analytical approach, we wish to highlight the many different attachments and relations that algorithms enfold. The approach thus aims to produce accounts that highlight how algorithms dynamically combine and reconfigure different social and material heterogeneities as well as the ethical, normative and political consequences of these reconfigurations.", "venue": "Big Data & Society", "year": 2019, "referenceCount": 72, "citationCount": 25, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "93799848", "name": "Francis Lee"}, {"authorId": "51340439", "name": "J. Bier"}, {"authorId": "1932041650", "name": "J. Christensen"}, {"authorId": "20333561", "name": "L. Engelmann"}, {"authorId": "74836499", "name": "Claes-Fredrik Helgesson"}, {"authorId": "2116648759", "name": "Robin Williams"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b2a784d30fc478470dc7c27c844700d7a63b5021", "externalIds": {"MAG": "2942399136", "DBLP": "conf/chi/AlkhatibB19", "DOI": "10.1145/3290605.3300760"}, "url": "https://www.semanticscholar.org/paper/b2a784d30fc478470dc7c27c844700d7a63b5021", "title": "Street-Level Algorithms: A Theory at the Gaps Between Policy and Decisions", "abstract": "Errors and biases are earning algorithms increasingly malignant reputations in society. A central challenge is that algorithms must bridge the gap between high-level policy and on-the-ground decisions, making inferences in novel situations where the policy or training data do not readily apply. In this paper, we draw on the theory of street-level bureaucracies, how human bureaucrats such as police and judges interpret policy to make on-the-ground decisions. We present by analogy a theory of street-level algorithms, the algorithms that bridge the gaps between policy and decisions about people in a socio-technical system. We argue that unlike street-level bureaucrats, who reflexively refine their decision criteria as they reason through a novel situation, street-level algorithms at best refine their criteria only after the decision is made. This loop-and-a-half delay results in illogical decisions when handling new or extenuating circumstances. This theory suggests designs for street-level algorithms that draw on historical design patterns for street-level bureaucracies, including mechanisms for self-policing and recourse in the case of error.", "venue": "CHI", "year": 2019, "referenceCount": 128, "citationCount": 58, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3153970", "name": "A. Alkhatib"}, {"authorId": "145879842", "name": "Michael S. Bernstein"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "b52a7d3349ca4a2a56b3335f87c090c3074bcb84", "externalIds": {"MAG": "2976537124", "DBLP": "journals/corr/abs-1909-11869", "ArXiv": "1909.11869", "DOI": "10.1145/3359221"}, "url": "https://www.semanticscholar.org/paper/b52a7d3349ca4a2a56b3335f87c090c3074bcb84", "title": "This Thing Called Fairness", "abstract": "The explosion in the use of software in important sociotechnical systems has renewed focus on the study of the way technical constructs reflect policies, norms, and human values. This effort requires the engagement of scholars and practitioners from many disciplines. And yet, these disciplines often conceptualize the operative values very differently while referring to them using the same vocabulary. The resulting conflation of ideas confuses discussions about values in technology at disciplinary boundaries. In the service of improving this situation, this paper examines the value of shared vocabularies, analytics, and other tools that facilitate conversations about values in light of these disciplinary specific conceptualizations, the role such tools play in furthering research and practice, outlines different conceptions of \"fairness\" deployed in discussions about computer systems, and provides an analytic tool for interdisciplinary discussions and collaborations around the concept of fairness. We use a case study of risk assessments in criminal justice applications to both motivate our effort--describing how conflation of different concepts under the banner of \"fairness\" led to unproductive confusion--and illustrate the value of the fairness analytic by demonstrating how the rigorous analysis it enables can assist in identifying key areas of theoretical, political, and practical misunderstanding or disagreement, and where desired support alignment or collaboration in the absence of consensus.", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2019, "referenceCount": 194, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Sociology", "Computer Science"], "authors": [{"authorId": "34421129", "name": "D. Mulligan"}, {"authorId": "145973578", "name": "Joshua A. Kroll"}, {"authorId": "51114342", "name": "Nitin Kohli"}, {"authorId": "9063054", "name": "Richmond Y. Wong"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "f30ead8435bed6930cbc37c0e3f289c1242965b2", "externalIds": {"DBLP": "journals/pacmhci/Grgic-HlacaEG19", "MAG": "3023055792", "DOI": "10.1145/3359280"}, "url": "https://www.semanticscholar.org/paper/f30ead8435bed6930cbc37c0e3f289c1242965b2", "title": "Human Decision Making with Machine Assistance", "abstract": "Much of political debate focuses on the concern that machines might take over. Yet in many domains it is much more plausible that the ultimate choice and responsibility remain with a human decision-maker, but that she is provided with machine advice. A quintessential illustration is the decision of a judge to bail or jail a defendant. In multiple jurisdictions in the US, judges have access to a machine prediction about a defendant's recidivism risk. In our study, we explore how receiving machine advice influences people's bail decisions. We run a vignette experiment with laypersons whom we test on a subsample of cases from the database of this prediction tool. In study 1, we ask them to predict whether defendants will recidivate before tried, and manipulate whether they have access to machine advice. We find that receiving machine advice has a small effect, which is biased in the direction of predicting no recidivism. In the field, human decision makers sometimes have a chance, after the fact, to learn whether the machine has given good advice. In study 2, after each trial we inform participants of ground truth. This does not make it more likely that they follow the advice, despite the fact that the machine is (on average) slightly more accurate than real judges. This also holds if initially the advice is mostly correct, or if it initially is mostly to predict (no) recidivism. Real judges know that their decisions affect defendants' lives. They may also be concerned about reelection or promotion. Hence a lot is at stake. In study 3 we emulate high stakes by giving participants a financial incentive. An incentive to find the ground truth, or to avoid false positive or false negatives, does not make participants more sensitive to machine advice. But an incentive to follow the advice is effective.", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2019, "referenceCount": 82, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1380255001", "name": "Nina Grgic-Hlaca"}, {"authorId": "145574694", "name": "C. Engel"}, {"authorId": "1958921", "name": "K. Gummadi"}]}}, {"contexts": ["In doing so, we build on efforts by scholars to bring Bourdieu\u2019s thought to the study of journalism (Benson, 2013; Christin, 2017; Neveu, 2009), while also extending its application to a dimension of social practice not often discussed by those using Bourdieu in communication scholarship: How\u2026", "Recent research suggests that journalists use social media to brand themselves, often with the explicit aim of advancing their careers (Molyneux and Holton, 2015) and that technology use varies according to editorial position (Christin, 2017).", "Hanitzsch (2007) and Hanusch (2009) provide key statements on the concept of culture in journalism scholarship, and Christin (2017) offers an empirical analysis of how professions buffer technological change."], "isInfluential": false, "intents": ["background"], "citingPaper": {"paperId": "085e69beae28da7348fe4d86cfb048b7a0882d24", "externalIds": {"MAG": "2756155699", "DBLP": "journals/nms/PowersV18", "DOI": "10.1177/1461444817731566"}, "url": "https://www.semanticscholar.org/paper/085e69beae28da7348fe4d86cfb048b7a0882d24", "title": "How journalists use social media in France and the United States: Analyzing technology use across journalistic fields", "abstract": "This article examines journalists\u2019 use of social media in France and the United States. Through in-depth interviews, we show that shared practical sensibilities lead journalists in both countries to use social media to accomplish routine tasks (e.g. gather information, monitor sources, and develop story ideas). At the same time, we argue that the incorporation of social media into daily practice also creates opportunities for journalists to garner peer recognition and that these opportunities vary according to the distinctive national fields in which journalists are embedded. Where American journalism incentivizes individual journalists to orient social media use toward audiences, French journalism motivates news organizations to use social media for these purposes, while leaving individual journalists to focus primarily on engaging with their peers. We position these findings in relation to debates on the uses of technologies across national settings.", "venue": "New Media Soc.", "year": 2018, "referenceCount": 50, "citationCount": 47, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Political Science", "Computer Science"], "authors": [{"authorId": "144686905", "name": "M. Powers"}, {"authorId": "1403575797", "name": "Sandra Vera-Zambrano"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citingPaper": {"paperId": "789149e1bb753adebe0b65adba882e53ec31d117", "externalIds": {"MAG": "2790949239", "DOI": "10.1007/978-3-658-21011-3_17"}, "url": "https://www.semanticscholar.org/paper/789149e1bb753adebe0b65adba882e53ec31d117", "title": "F\u00e4lle, Typen, T\u00fccken", "abstract": "In diesem Beitrag werden wir auf das Problem der zeitsensiblen Typenbildung bei der Arbeit an digitalem Material eingehen, welches wir exemplarisch anhand eines Falles aus dem Forschungsprojekt \u201eMediatisierung als Geschaftsmodell\u201c diskutieren werden. Dieses Projekt war insofern zeitsensibel angelegt, als wir die Genese und die Veranderung eines Geschaftsmodells uber einen langeren Zeitraum dokumentiert und analysiert haben. Das stellte uns vor die Frage, inwiefern Dynamik und Veranderung in der Typenbildung berucksichtigt werden konnen.", "venue": "", "year": 2018, "referenceCount": 28, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "123825813", "name": "Maria Schlechter"}, {"authorId": "71797759", "name": "H. Kirschner"}]}}]}